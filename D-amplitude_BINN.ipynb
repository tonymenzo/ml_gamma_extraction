{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12 (main, Apr  5 2022, 01:53:17) \n",
      "[Clang 12.0.0 ]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"paper\", font_scale = 1.8)\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['font.size'] = 25\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn import Module\n",
    "import FrEIA.framework as Ff\n",
    "import FrEIA.modules as Fm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Python', sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dalitz_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates Dalitz dataset so that it can be used by the pythorch syntax.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_VB(Module):\n",
    "    \"\"\"\n",
    "    Build a Bayesian linear weight layer i.e. weights are now distributions\n",
    "    \"\"\"\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "    \n",
    "    def __init__(self, in_features, out_features, resample = True):\n",
    "        super(Linear_VB, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.resample = resample                                        # Sample random weights through each forward pass\n",
    "        self.bias = Parameter(Tensor(out_features))                     # Assign as tunable parameter with dimension (1 x out_features)\n",
    "        self.mu_w = Parameter(Tensor(out_features, in_features))        # Assign as tunable parameter with dimension (out_features x in_features)\n",
    "        self.logsig2_w = Parameter(Tensor(out_features, in_features))   # Assign as tunable parameter with dimension (out_features x in_features)\n",
    "        self.random = torch.randn_like(self.logsig2_w)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward( self, input ):\n",
    "        # Sample random weights on each forward pass\n",
    "        if self.resample:\n",
    "            self.random = torch.randn_like(self.logsig2_w)\n",
    "        s2_w = self.logsig2_w.exp()\n",
    "        # Reparameterization trick\n",
    "        weight = self.mu_w + s2_w.sqrt() * self.random\n",
    "        return nn.functional.linear(input, weight, self.bias)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.mu_w.size(1))\n",
    "        self.mu_w.data.normal_(0, stdv)\n",
    "        self.logsig2_w.data.zero_().normal_(-9, 0.001)\n",
    "        self.bias.data.zero_()\n",
    "        \n",
    "    def KL( self, loguniform=False ):\n",
    "        kl = 0.5 * (self.mu_w.pow(2) + self.logsig2_w.exp() - self.logsig2_w - 1).sum()\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bayes_INN(Module):\n",
    "    def __init__(self, n_dim, training_size, input_dim = 2000, hidden_dim = 2000, output_dim = 2000, resample = True):\n",
    "        super(Bayes_INN, self).__init__()\n",
    "\n",
    "        # Number of entries in the training dataset\n",
    "        self.n_dim = n_dim\n",
    "        \n",
    "        # Loss function is dependent on the size of the training dataset\n",
    "        self.training_size = training_size\n",
    "\n",
    "        def subnet(input_dim, output_dim):\n",
    "\n",
    "            # Two types of layers -> weight layers and activation layers the latter is not Bayesian\n",
    "            self.subnet_bayes_layers = []\n",
    "            self.subnet_layers       = []\n",
    "    \n",
    "            # Define the input layer\n",
    "            linear_VB_layer = Linear_VB(input_dim, hidden_dim)#, resample = resample)\n",
    "            self.subnet_bayes_layers.append(linear_VB_layer)\n",
    "            self.subnet_layers.append(linear_VB_layer)\n",
    "            self.subnet_layers.append(nn.ReLU())\n",
    "    \n",
    "            # If you want a more complex subnet, add more hidden layers\n",
    "            linear_VB_layer = Linear_VB(hidden_dim, hidden_dim)\n",
    "            self.subnet_bayes_layers.append(linear_VB_layer)\n",
    "            self.subnet_layers.append(linear_VB_layer)\n",
    "            self.subnet_layers.append(nn.ReLU())\n",
    "    \n",
    "            # Now for the final output layer\n",
    "            linear_VB_layer = Linear_VB(hidden_dim, output_dim)#, resample = resample)\n",
    "            self.subnet_bayes_layers.append(linear_VB_layer)\n",
    "            self.subnet_layers.append(linear_VB_layer)\n",
    "            # We don't need an activation layer for the output layer\n",
    "    \n",
    "            # The full model is a sequentiual net over the defined layers above\n",
    "            return nn.Sequential(*self.subnet_layers)\n",
    "\n",
    "\n",
    "        self.inn = Ff.SequenceINN(self.n_dim)\n",
    "        for k in range(self.n_dim):\n",
    "            self.inn.append(Fm.AllInOneBlock, subnet_constructor=subnet, permute_soft=True)\n",
    "            #self.inn.append(Fm.GINCouplingBlock, subnet_constructor=subnet)\n",
    "\n",
    "    # We need the KL from the Bayesian layers\n",
    "    def KL(self):\n",
    "        kl = 0\n",
    "        for bayesian_layer in self.subnet_bayes_layers:\n",
    "            kl += bayesian_layer.KL()\n",
    "        return kl / self.training_size # Normalized to the training size? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inn_loss(z, ndim, log_jac_det):\n",
    "    l = 0.5*torch.sum(z**2, 1) - log_jac_det\n",
    "    l = l.mean() / ndim\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, scheduler=None, ndim = 5):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        # Pass through the network\n",
    "        z, log_jac_det = model(X)\n",
    "        \n",
    "        loss_inn, loss_KL, loss = 0.0, 0.0, 0.0\n",
    "        \n",
    "        # Compute the KL loss for each of the Bayesian layers in the network\n",
    "        for i in range(len(model.module_list)):\n",
    "            for j in [0,2,4]:\n",
    "                loss_KL += model.module_list[i].subnet[j].KL()\n",
    "        loss_KL = loss_KL / size\n",
    "        loss_KL = 0\n",
    "        #print(loss_KL)\n",
    "        #print(BINN.KL())\n",
    "\n",
    "        # Get the batch loss\n",
    "        loss_inn = inn_loss(z, ndim, log_jac_det)\n",
    "        \n",
    "        loss = loss_KL + loss_inn\n",
    "        \n",
    "        # Also need to include KL loss for tuning stochastic layer weights\n",
    "\n",
    "        # Reset the gradients in the optimizer (see autograd tutorial in PyTorch docs)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the network weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler != None:\n",
    "            scheduler.step(loss)\n",
    "\n",
    "        # Print the loss every 100 updates\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Total loss: {loss:>8f} INN loss: {loss_inn:>8f} KL loss: {loss_KL:>8f} learning_rate: {optimizer.param_groups[0][\"lr\"]:>8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  (8000000, 2) Validation set size:  (2000000, 2)\n",
      "Model Architecture: \n",
      "SequenceINN(\n",
      "  (module_list): ModuleList(\n",
      "    (0-1): 2 x AllInOneBlock(\n",
      "      (softplus): Softplus(beta=0.5, threshold=20)\n",
      "      (subnet): Sequential(\n",
      "        (0): Linear_VB()\n",
      "        (1): ReLU()\n",
      "        (2): Linear_VB()\n",
      "        (3): ReLU()\n",
      "        (4): Linear_VB()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 0.144088 INN loss: 0.144088 KL loss: 0.000000 learning_rate: 0.001000\n",
      "Total loss: -0.659813 INN loss: -0.659813 KL loss: 0.000000 learning_rate: 0.001000\n",
      "Total loss: -0.994354 INN loss: -0.994354 KL loss: 0.000000 learning_rate: 0.001000\n",
      "Total loss: -1.047382 INN loss: -1.047382 KL loss: 0.000000 learning_rate: 0.001000\n",
      "Total loss: -1.101462 INN loss: -1.101462 KL loss: 0.000000 learning_rate: 0.000900\n",
      "Total loss: -1.099162 INN loss: -1.099162 KL loss: 0.000000 learning_rate: 0.000729\n",
      "Total loss: -1.095482 INN loss: -1.095482 KL loss: 0.000000 learning_rate: 0.000729\n",
      "Total loss: -1.118851 INN loss: -1.118851 KL loss: 0.000000 learning_rate: 0.000590\n",
      "Total loss: -1.082161 INN loss: -1.082161 KL loss: 0.000000 learning_rate: 0.000478\n",
      "Total loss: -1.100653 INN loss: -1.100653 KL loss: 0.000000 learning_rate: 0.000430\n",
      "Total loss: -1.200483 INN loss: -1.200483 KL loss: 0.000000 learning_rate: 0.000387\n",
      "Total loss: -1.110767 INN loss: -1.110767 KL loss: 0.000000 learning_rate: 0.000349\n",
      "Total loss: -1.127628 INN loss: -1.127628 KL loss: 0.000000 learning_rate: 0.000282\n",
      "Total loss: -1.130722 INN loss: -1.130722 KL loss: 0.000000 learning_rate: 0.000229\n",
      "Total loss: -1.109917 INN loss: -1.109917 KL loss: 0.000000 learning_rate: 0.000185\n",
      "Total loss: -1.092895 INN loss: -1.092895 KL loss: 0.000000 learning_rate: 0.000150\n",
      "Total loss: -1.152381 INN loss: -1.152381 KL loss: 0.000000 learning_rate: 0.000122\n",
      "Total loss: -1.099181 INN loss: -1.099181 KL loss: 0.000000 learning_rate: 0.000098\n",
      "Total loss: -1.167910 INN loss: -1.167910 KL loss: 0.000000 learning_rate: 0.000080\n",
      "Total loss: -1.142329 INN loss: -1.142329 KL loss: 0.000000 learning_rate: 0.000065\n",
      "Total loss: -1.152237 INN loss: -1.152237 KL loss: 0.000000 learning_rate: 0.000052\n",
      "Total loss: -1.102593 INN loss: -1.102593 KL loss: 0.000000 learning_rate: 0.000042\n",
      "Total loss: -1.094066 INN loss: -1.094066 KL loss: 0.000000 learning_rate: 0.000034\n",
      "Total loss: -1.150319 INN loss: -1.150319 KL loss: 0.000000 learning_rate: 0.000028\n",
      "Total loss: -1.127940 INN loss: -1.127940 KL loss: 0.000000 learning_rate: 0.000023\n",
      "Total loss: -1.129540 INN loss: -1.129540 KL loss: 0.000000 learning_rate: 0.000018\n",
      "Total loss: -1.141119 INN loss: -1.141119 KL loss: 0.000000 learning_rate: 0.000015\n",
      "Total loss: -1.136017 INN loss: -1.136017 KL loss: 0.000000 learning_rate: 0.000012\n",
      "Total loss: -1.126946 INN loss: -1.126946 KL loss: 0.000000 learning_rate: 0.000011\n",
      "Total loss: -1.154611 INN loss: -1.154611 KL loss: 0.000000 learning_rate: 0.000009\n",
      "Total loss: -1.155578 INN loss: -1.155578 KL loss: 0.000000 learning_rate: 0.000007\n",
      "Total loss: -1.175585 INN loss: -1.175585 KL loss: 0.000000 learning_rate: 0.000006\n",
      "Total loss: -1.129111 INN loss: -1.129111 KL loss: 0.000000 learning_rate: 0.000005\n",
      "Total loss: -1.092640 INN loss: -1.092640 KL loss: 0.000000 learning_rate: 0.000004\n",
      "Total loss: -1.140317 INN loss: -1.140317 KL loss: 0.000000 learning_rate: 0.000003\n",
      "Total loss: -1.149693 INN loss: -1.149693 KL loss: 0.000000 learning_rate: 0.000003\n",
      "Total loss: -1.178030 INN loss: -1.178030 KL loss: 0.000000 learning_rate: 0.000002\n",
      "Total loss: -1.170594 INN loss: -1.170594 KL loss: 0.000000 learning_rate: 0.000002\n",
      "Total loss: -1.123947 INN loss: -1.123947 KL loss: 0.000000 learning_rate: 0.000001\n",
      "Total loss: -1.111905 INN loss: -1.111905 KL loss: 0.000000 learning_rate: 0.000001\n",
      "Total loss: -1.098726 INN loss: -1.098726 KL loss: 0.000000 learning_rate: 0.000001\n",
      "Total loss: -1.142567 INN loss: -1.142567 KL loss: 0.000000 learning_rate: 0.000001\n",
      "Total loss: -1.079822 INN loss: -1.079822 KL loss: 0.000000 learning_rate: 0.000001\n",
      "Total loss: -1.122543 INN loss: -1.122543 KL loss: 0.000000 learning_rate: 0.000001\n",
      "Total loss: -1.139037 INN loss: -1.139037 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152116 INN loss: -1.152116 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148760 INN loss: -1.148760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133826 INN loss: -1.133826 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118132 INN loss: -1.118132 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108950 INN loss: -1.108950 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132574 INN loss: -1.132574 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095096 INN loss: -1.095096 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139276 INN loss: -1.139276 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183970 INN loss: -1.183970 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112429 INN loss: -1.112429 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136275 INN loss: -1.136275 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166625 INN loss: -1.166625 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128164 INN loss: -1.128164 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145326 INN loss: -1.145326 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139028 INN loss: -1.139028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138526 INN loss: -1.138526 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108781 INN loss: -1.108781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124933 INN loss: -1.124933 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130176 INN loss: -1.130176 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159065 INN loss: -1.159065 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122180 INN loss: -1.122180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115077 INN loss: -1.115077 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126108 INN loss: -1.126108 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.060905 INN loss: -1.060905 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093921 INN loss: -1.093921 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127756 INN loss: -1.127756 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147976 INN loss: -1.147976 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096100 INN loss: -1.096100 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149689 INN loss: -1.149689 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148395 INN loss: -1.148395 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094164 INN loss: -1.094164 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158382 INN loss: -1.158382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097174 INN loss: -1.097174 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171857 INN loss: -1.171857 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119822 INN loss: -1.119822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142631 INN loss: -1.142631 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144452 INN loss: -1.144452 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148216 INN loss: -1.148216 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124648 INN loss: -1.124648 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128328 INN loss: -1.128328 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112537 INN loss: -1.112537 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146468 INN loss: -1.146468 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119028 INN loss: -1.119028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162041 INN loss: -1.162041 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121676 INN loss: -1.121676 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.070012 INN loss: -1.070012 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120343 INN loss: -1.120343 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164758 INN loss: -1.164758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138521 INN loss: -1.138521 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154656 INN loss: -1.154656 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125436 INN loss: -1.125436 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121723 INN loss: -1.121723 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146243 INN loss: -1.146243 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107156 INN loss: -1.107156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131582 INN loss: -1.131582 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098835 INN loss: -1.098835 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139764 INN loss: -1.139764 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162712 INN loss: -1.162712 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188709 INN loss: -1.188709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158845 INN loss: -1.158845 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145098 INN loss: -1.145098 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140909 INN loss: -1.140909 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089513 INN loss: -1.089513 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117760 INN loss: -1.117760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155971 INN loss: -1.155971 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126613 INN loss: -1.126613 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164494 INN loss: -1.164494 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167713 INN loss: -1.167713 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141844 INN loss: -1.141844 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123148 INN loss: -1.123148 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160585 INN loss: -1.160585 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132852 INN loss: -1.132852 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117396 INN loss: -1.117396 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113320 INN loss: -1.113320 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098927 INN loss: -1.098927 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133661 INN loss: -1.133661 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145236 INN loss: -1.145236 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135118 INN loss: -1.135118 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168330 INN loss: -1.168330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128312 INN loss: -1.128312 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123001 INN loss: -1.123001 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107983 INN loss: -1.107983 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099212 INN loss: -1.099212 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149646 INN loss: -1.149646 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154683 INN loss: -1.154683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118609 INN loss: -1.118609 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104295 INN loss: -1.104295 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108855 INN loss: -1.108855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127930 INN loss: -1.127930 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156404 INN loss: -1.156404 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129316 INN loss: -1.129316 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136082 INN loss: -1.136082 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173812 INN loss: -1.173812 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112485 INN loss: -1.112485 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160152 INN loss: -1.160152 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134845 INN loss: -1.134845 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087544 INN loss: -1.087544 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173492 INN loss: -1.173492 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143277 INN loss: -1.143277 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098322 INN loss: -1.098322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165419 INN loss: -1.165419 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107829 INN loss: -1.107829 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117311 INN loss: -1.117311 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132069 INN loss: -1.132069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089689 INN loss: -1.089689 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154946 INN loss: -1.154946 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174276 INN loss: -1.174276 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107560 INN loss: -1.107560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127090 INN loss: -1.127090 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154133 INN loss: -1.154133 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132455 INN loss: -1.132455 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130187 INN loss: -1.130187 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109301 INN loss: -1.109301 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088254 INN loss: -1.088254 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129367 INN loss: -1.129367 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▏                                                           | 1/50 [02:36<2:08:02, 156.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.113629 INN loss: -1.113629 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127314 INN loss: -1.127314 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116505 INN loss: -1.116505 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109666 INN loss: -1.109666 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149641 INN loss: -1.149641 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114667 INN loss: -1.114667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092467 INN loss: -1.092467 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136652 INN loss: -1.136652 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182651 INN loss: -1.182651 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154541 INN loss: -1.154541 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080443 INN loss: -1.080443 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109848 INN loss: -1.109848 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146019 INN loss: -1.146019 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160243 INN loss: -1.160243 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.070604 INN loss: -1.070604 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162498 INN loss: -1.162498 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155992 INN loss: -1.155992 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180681 INN loss: -1.180681 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188789 INN loss: -1.188789 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098328 INN loss: -1.098328 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140381 INN loss: -1.140381 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158149 INN loss: -1.158149 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129369 INN loss: -1.129369 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122018 INN loss: -1.122018 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135987 INN loss: -1.135987 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151419 INN loss: -1.151419 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160676 INN loss: -1.160676 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183968 INN loss: -1.183968 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132327 INN loss: -1.132327 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149533 INN loss: -1.149533 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102513 INN loss: -1.102513 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089070 INN loss: -1.089070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076026 INN loss: -1.076026 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135366 INN loss: -1.135366 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132893 INN loss: -1.132893 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128780 INN loss: -1.128780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106928 INN loss: -1.106928 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116763 INN loss: -1.116763 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159539 INN loss: -1.159539 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160120 INN loss: -1.160120 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158452 INN loss: -1.158452 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132476 INN loss: -1.132476 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144875 INN loss: -1.144875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.071127 INN loss: -1.071127 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143731 INN loss: -1.143731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076417 INN loss: -1.076417 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166582 INN loss: -1.166582 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136286 INN loss: -1.136286 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127776 INN loss: -1.127776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152801 INN loss: -1.152801 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135875 INN loss: -1.135875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169923 INN loss: -1.169923 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096117 INN loss: -1.096117 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084578 INN loss: -1.084578 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171495 INN loss: -1.171495 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124592 INN loss: -1.124592 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.057911 INN loss: -1.057911 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125549 INN loss: -1.125549 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162102 INN loss: -1.162102 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.064926 INN loss: -1.064926 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131754 INN loss: -1.131754 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165651 INN loss: -1.165651 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136096 INN loss: -1.136096 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133541 INN loss: -1.133541 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130939 INN loss: -1.130939 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146679 INN loss: -1.146679 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132200 INN loss: -1.132200 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132297 INN loss: -1.132297 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123872 INN loss: -1.123872 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127401 INN loss: -1.127401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102827 INN loss: -1.102827 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135127 INN loss: -1.135127 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109864 INN loss: -1.109864 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132957 INN loss: -1.132957 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137752 INN loss: -1.137752 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127434 INN loss: -1.127434 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110209 INN loss: -1.110209 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122379 INN loss: -1.122379 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.086730 INN loss: -1.086730 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179000 INN loss: -1.179000 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121573 INN loss: -1.121573 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145848 INN loss: -1.145848 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148278 INN loss: -1.148278 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122911 INN loss: -1.122911 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114747 INN loss: -1.114747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097342 INN loss: -1.097342 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154024 INN loss: -1.154024 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158693 INN loss: -1.158693 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136886 INN loss: -1.136886 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189073 INN loss: -1.189073 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107132 INN loss: -1.107132 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138860 INN loss: -1.138860 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092051 INN loss: -1.092051 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105212 INN loss: -1.105212 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138861 INN loss: -1.138861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103617 INN loss: -1.103617 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083933 INN loss: -1.083933 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164614 INN loss: -1.164614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132485 INN loss: -1.132485 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158813 INN loss: -1.158813 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144247 INN loss: -1.144247 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124218 INN loss: -1.124218 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137624 INN loss: -1.137624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128750 INN loss: -1.128750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105259 INN loss: -1.105259 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117593 INN loss: -1.117593 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127980 INN loss: -1.127980 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141906 INN loss: -1.141906 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104270 INN loss: -1.104270 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135785 INN loss: -1.135785 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159687 INN loss: -1.159687 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114012 INN loss: -1.114012 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098921 INN loss: -1.098921 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103066 INN loss: -1.103066 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124887 INN loss: -1.124887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157292 INN loss: -1.157292 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133586 INN loss: -1.133586 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135224 INN loss: -1.135224 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102717 INN loss: -1.102717 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178255 INN loss: -1.178255 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113743 INN loss: -1.113743 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109543 INN loss: -1.109543 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146936 INN loss: -1.146936 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108318 INN loss: -1.108318 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168617 INN loss: -1.168617 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118921 INN loss: -1.118921 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135711 INN loss: -1.135711 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122986 INN loss: -1.122986 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112061 INN loss: -1.112061 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115716 INN loss: -1.115716 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119019 INN loss: -1.119019 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133143 INN loss: -1.133143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131285 INN loss: -1.131285 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129335 INN loss: -1.129335 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103798 INN loss: -1.103798 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134210 INN loss: -1.134210 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102296 INN loss: -1.102296 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135542 INN loss: -1.135542 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152748 INN loss: -1.152748 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138737 INN loss: -1.138737 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107184 INN loss: -1.107184 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110091 INN loss: -1.110091 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128461 INN loss: -1.128461 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164635 INN loss: -1.164635 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143642 INN loss: -1.143642 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149083 INN loss: -1.149083 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145900 INN loss: -1.145900 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182940 INN loss: -1.182940 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182086 INN loss: -1.182086 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134539 INN loss: -1.134539 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116893 INN loss: -1.116893 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119390 INN loss: -1.119390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133946 INN loss: -1.133946 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112839 INN loss: -1.112839 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134215 INN loss: -1.134215 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138870 INN loss: -1.138870 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123527 INN loss: -1.123527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099858 INN loss: -1.099858 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126486 INN loss: -1.126486 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101710 INN loss: -1.101710 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▍                                                          | 2/50 [04:59<1:59:02, 148.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.129763 INN loss: -1.129763 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172547 INN loss: -1.172547 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129425 INN loss: -1.129425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121994 INN loss: -1.121994 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136657 INN loss: -1.136657 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143454 INN loss: -1.143454 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128563 INN loss: -1.128563 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126937 INN loss: -1.126937 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168151 INN loss: -1.168151 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124022 INN loss: -1.124022 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153741 INN loss: -1.153741 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126625 INN loss: -1.126625 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160732 INN loss: -1.160732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108714 INN loss: -1.108714 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130834 INN loss: -1.130834 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135636 INN loss: -1.135636 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132491 INN loss: -1.132491 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134741 INN loss: -1.134741 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121266 INN loss: -1.121266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154203 INN loss: -1.154203 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162977 INN loss: -1.162977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120656 INN loss: -1.120656 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126006 INN loss: -1.126006 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121172 INN loss: -1.121172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126915 INN loss: -1.126915 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132378 INN loss: -1.132378 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131102 INN loss: -1.131102 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149962 INN loss: -1.149962 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144171 INN loss: -1.144171 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141815 INN loss: -1.141815 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124349 INN loss: -1.124349 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108839 INN loss: -1.108839 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131725 INN loss: -1.131725 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125771 INN loss: -1.125771 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122685 INN loss: -1.122685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116091 INN loss: -1.116091 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123145 INN loss: -1.123145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167650 INN loss: -1.167650 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139156 INN loss: -1.139156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168261 INN loss: -1.168261 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142411 INN loss: -1.142411 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129836 INN loss: -1.129836 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146003 INN loss: -1.146003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099913 INN loss: -1.099913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082750 INN loss: -1.082750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089885 INN loss: -1.089885 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101595 INN loss: -1.101595 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153842 INN loss: -1.153842 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101548 INN loss: -1.101548 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105500 INN loss: -1.105500 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125260 INN loss: -1.125260 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126673 INN loss: -1.126673 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106189 INN loss: -1.106189 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089895 INN loss: -1.089895 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166609 INN loss: -1.166609 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160066 INN loss: -1.160066 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107750 INN loss: -1.107750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106067 INN loss: -1.106067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118595 INN loss: -1.118595 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139185 INN loss: -1.139185 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170611 INN loss: -1.170611 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156043 INN loss: -1.156043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131343 INN loss: -1.131343 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148657 INN loss: -1.148657 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169127 INN loss: -1.169127 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098634 INN loss: -1.098634 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166119 INN loss: -1.166119 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155401 INN loss: -1.155401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119249 INN loss: -1.119249 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177946 INN loss: -1.177946 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097213 INN loss: -1.097213 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119509 INN loss: -1.119509 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131724 INN loss: -1.131724 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128307 INN loss: -1.128307 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109909 INN loss: -1.109909 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.079108 INN loss: -1.079108 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124122 INN loss: -1.124122 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151463 INN loss: -1.151463 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163726 INN loss: -1.163726 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159649 INN loss: -1.159649 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117354 INN loss: -1.117354 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152104 INN loss: -1.152104 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166251 INN loss: -1.166251 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104238 INN loss: -1.104238 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163054 INN loss: -1.163054 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114626 INN loss: -1.114626 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151938 INN loss: -1.151938 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156503 INN loss: -1.156503 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158469 INN loss: -1.158469 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144167 INN loss: -1.144167 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123247 INN loss: -1.123247 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115173 INN loss: -1.115173 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109967 INN loss: -1.109967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130297 INN loss: -1.130297 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126866 INN loss: -1.126866 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126665 INN loss: -1.126665 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159969 INN loss: -1.159969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128042 INN loss: -1.128042 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160042 INN loss: -1.160042 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146960 INN loss: -1.146960 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152138 INN loss: -1.152138 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123453 INN loss: -1.123453 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128944 INN loss: -1.128944 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142629 INN loss: -1.142629 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125456 INN loss: -1.125456 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151433 INN loss: -1.151433 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143676 INN loss: -1.143676 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167209 INN loss: -1.167209 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158792 INN loss: -1.158792 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085921 INN loss: -1.085921 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105346 INN loss: -1.105346 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156350 INN loss: -1.156350 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111600 INN loss: -1.111600 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132475 INN loss: -1.132475 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170538 INN loss: -1.170538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161412 INN loss: -1.161412 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177472 INN loss: -1.177472 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131869 INN loss: -1.131869 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112296 INN loss: -1.112296 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115491 INN loss: -1.115491 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183792 INN loss: -1.183792 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164842 INN loss: -1.164842 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177639 INN loss: -1.177639 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128383 INN loss: -1.128383 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113374 INN loss: -1.113374 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116696 INN loss: -1.116696 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103119 INN loss: -1.103119 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151204 INN loss: -1.151204 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161956 INN loss: -1.161956 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118875 INN loss: -1.118875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135400 INN loss: -1.135400 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133035 INN loss: -1.133035 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117738 INN loss: -1.117738 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107948 INN loss: -1.107948 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100493 INN loss: -1.100493 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166617 INN loss: -1.166617 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123882 INN loss: -1.123882 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154955 INN loss: -1.154955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100026 INN loss: -1.100026 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118747 INN loss: -1.118747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138925 INN loss: -1.138925 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121824 INN loss: -1.121824 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117921 INN loss: -1.117921 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091401 INN loss: -1.091401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126418 INN loss: -1.126418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168474 INN loss: -1.168474 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147747 INN loss: -1.147747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148473 INN loss: -1.148473 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170538 INN loss: -1.170538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141676 INN loss: -1.141676 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184784 INN loss: -1.184784 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160547 INN loss: -1.160547 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125605 INN loss: -1.125605 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177554 INN loss: -1.177554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174284 INN loss: -1.174284 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123028 INN loss: -1.123028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115523 INN loss: -1.115523 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136822 INN loss: -1.136822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145271 INN loss: -1.145271 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115517 INN loss: -1.115517 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███▍                                                     | 3/50 [1:03:05<21:50:08, 1672.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.112983 INN loss: -1.112983 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112415 INN loss: -1.112415 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165071 INN loss: -1.165071 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115806 INN loss: -1.115806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104970 INN loss: -1.104970 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147402 INN loss: -1.147402 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179481 INN loss: -1.179481 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125193 INN loss: -1.125193 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155821 INN loss: -1.155821 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114427 INN loss: -1.114427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150330 INN loss: -1.150330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130760 INN loss: -1.130760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146758 INN loss: -1.146758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152330 INN loss: -1.152330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108133 INN loss: -1.108133 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118267 INN loss: -1.118267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137063 INN loss: -1.137063 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119848 INN loss: -1.119848 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129869 INN loss: -1.129869 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113320 INN loss: -1.113320 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146869 INN loss: -1.146869 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160526 INN loss: -1.160526 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180289 INN loss: -1.180289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134496 INN loss: -1.134496 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134652 INN loss: -1.134652 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166158 INN loss: -1.166158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134136 INN loss: -1.134136 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154302 INN loss: -1.154302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145996 INN loss: -1.145996 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158533 INN loss: -1.158533 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164454 INN loss: -1.164454 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127991 INN loss: -1.127991 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112831 INN loss: -1.112831 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130267 INN loss: -1.130267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163735 INN loss: -1.163735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120215 INN loss: -1.120215 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135378 INN loss: -1.135378 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122821 INN loss: -1.122821 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132607 INN loss: -1.132607 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135604 INN loss: -1.135604 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149510 INN loss: -1.149510 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151794 INN loss: -1.151794 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135103 INN loss: -1.135103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139351 INN loss: -1.139351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172971 INN loss: -1.172971 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168297 INN loss: -1.168297 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096129 INN loss: -1.096129 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.068700 INN loss: -1.068700 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082716 INN loss: -1.082716 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.193993 INN loss: -1.193993 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162789 INN loss: -1.162789 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093930 INN loss: -1.093930 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152963 INN loss: -1.152963 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164264 INN loss: -1.164264 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100368 INN loss: -1.100368 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132910 INN loss: -1.132910 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132256 INN loss: -1.132256 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142602 INN loss: -1.142602 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179978 INN loss: -1.179978 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096842 INN loss: -1.096842 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109328 INN loss: -1.109328 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138030 INN loss: -1.138030 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089130 INN loss: -1.089130 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149797 INN loss: -1.149797 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102946 INN loss: -1.102946 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135927 INN loss: -1.135927 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149833 INN loss: -1.149833 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124097 INN loss: -1.124097 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164049 INN loss: -1.164049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156397 INN loss: -1.156397 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140137 INN loss: -1.140137 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142137 INN loss: -1.142137 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088902 INN loss: -1.088902 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152674 INN loss: -1.152674 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118225 INN loss: -1.118225 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112960 INN loss: -1.112960 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129864 INN loss: -1.129864 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143120 INN loss: -1.143120 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097839 INN loss: -1.097839 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173322 INN loss: -1.173322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109240 INN loss: -1.109240 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156773 INN loss: -1.156773 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142382 INN loss: -1.142382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154138 INN loss: -1.154138 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102527 INN loss: -1.102527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104520 INN loss: -1.104520 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138138 INN loss: -1.138138 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118070 INN loss: -1.118070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106028 INN loss: -1.106028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131111 INN loss: -1.131111 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171899 INN loss: -1.171899 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132363 INN loss: -1.132363 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114772 INN loss: -1.114772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116757 INN loss: -1.116757 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159499 INN loss: -1.159499 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129897 INN loss: -1.129897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131017 INN loss: -1.131017 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161264 INN loss: -1.161264 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127873 INN loss: -1.127873 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124400 INN loss: -1.124400 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126514 INN loss: -1.126514 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092151 INN loss: -1.092151 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115214 INN loss: -1.115214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153723 INN loss: -1.153723 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112798 INN loss: -1.112798 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183120 INN loss: -1.183120 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154175 INN loss: -1.154175 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159710 INN loss: -1.159710 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143142 INN loss: -1.143142 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119765 INN loss: -1.119765 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121402 INN loss: -1.121402 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.194174 INN loss: -1.194174 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143652 INN loss: -1.143652 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115651 INN loss: -1.115651 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174371 INN loss: -1.174371 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127064 INN loss: -1.127064 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121728 INN loss: -1.121728 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173807 INN loss: -1.173807 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157690 INN loss: -1.157690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145523 INN loss: -1.145523 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125043 INN loss: -1.125043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126254 INN loss: -1.126254 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121822 INN loss: -1.121822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115508 INN loss: -1.115508 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140536 INN loss: -1.140536 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106222 INN loss: -1.106222 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138107 INN loss: -1.138107 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124518 INN loss: -1.124518 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116285 INN loss: -1.116285 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121851 INN loss: -1.121851 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178791 INN loss: -1.178791 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116758 INN loss: -1.116758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180965 INN loss: -1.180965 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173277 INN loss: -1.173277 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130020 INN loss: -1.130020 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144582 INN loss: -1.144582 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111392 INN loss: -1.111392 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110337 INN loss: -1.110337 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102510 INN loss: -1.102510 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114059 INN loss: -1.114059 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151214 INN loss: -1.151214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148139 INN loss: -1.148139 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113632 INN loss: -1.113632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.201481 INN loss: -1.201481 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101859 INN loss: -1.101859 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154572 INN loss: -1.154572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136468 INN loss: -1.136468 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136113 INN loss: -1.136113 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173215 INN loss: -1.173215 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112433 INN loss: -1.112433 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153534 INN loss: -1.153534 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130181 INN loss: -1.130181 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168655 INN loss: -1.168655 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115144 INN loss: -1.115144 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151821 INN loss: -1.151821 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101678 INN loss: -1.101678 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190214 INN loss: -1.190214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133964 INN loss: -1.133964 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183774 INN loss: -1.183774 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126291 INN loss: -1.126291 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|████▌                                                    | 4/50 [3:25:31<56:02:40, 4386.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.135931 INN loss: -1.135931 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.062947 INN loss: -1.062947 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170216 INN loss: -1.170216 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104859 INN loss: -1.104859 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136040 INN loss: -1.136040 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120207 INN loss: -1.120207 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133709 INN loss: -1.133709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132160 INN loss: -1.132160 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110609 INN loss: -1.110609 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150637 INN loss: -1.150637 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121361 INN loss: -1.121361 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137344 INN loss: -1.137344 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103833 INN loss: -1.103833 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145155 INN loss: -1.145155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102262 INN loss: -1.102262 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.055781 INN loss: -1.055781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128828 INN loss: -1.128828 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160360 INN loss: -1.160360 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151875 INN loss: -1.151875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126487 INN loss: -1.126487 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165552 INN loss: -1.165552 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142304 INN loss: -1.142304 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155787 INN loss: -1.155787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152554 INN loss: -1.152554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162771 INN loss: -1.162771 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133843 INN loss: -1.133843 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124002 INN loss: -1.124002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103994 INN loss: -1.103994 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169566 INN loss: -1.169566 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159138 INN loss: -1.159138 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139069 INN loss: -1.139069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161000 INN loss: -1.161000 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161680 INN loss: -1.161680 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111511 INN loss: -1.111511 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110548 INN loss: -1.110548 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151019 INN loss: -1.151019 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141593 INN loss: -1.141593 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131691 INN loss: -1.131691 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.206296 INN loss: -1.206296 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129150 INN loss: -1.129150 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102913 INN loss: -1.102913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125646 INN loss: -1.125646 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132323 INN loss: -1.132323 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113255 INN loss: -1.113255 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113342 INN loss: -1.113342 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093653 INN loss: -1.093653 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101325 INN loss: -1.101325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178691 INN loss: -1.178691 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130576 INN loss: -1.130576 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119936 INN loss: -1.119936 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137304 INN loss: -1.137304 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111503 INN loss: -1.111503 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163425 INN loss: -1.163425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103094 INN loss: -1.103094 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112829 INN loss: -1.112829 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111557 INN loss: -1.111557 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139097 INN loss: -1.139097 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154964 INN loss: -1.154964 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168010 INN loss: -1.168010 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164955 INN loss: -1.164955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110122 INN loss: -1.110122 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147256 INN loss: -1.147256 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107585 INN loss: -1.107585 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143090 INN loss: -1.143090 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165873 INN loss: -1.165873 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136063 INN loss: -1.136063 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111999 INN loss: -1.111999 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152869 INN loss: -1.152869 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102849 INN loss: -1.102849 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134448 INN loss: -1.134448 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152482 INN loss: -1.152482 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123146 INN loss: -1.123146 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132758 INN loss: -1.132758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123082 INN loss: -1.123082 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098719 INN loss: -1.098719 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097288 INN loss: -1.097288 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108338 INN loss: -1.108338 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112043 INN loss: -1.112043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156148 INN loss: -1.156148 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160605 INN loss: -1.160605 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159174 INN loss: -1.159174 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130757 INN loss: -1.130757 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146783 INN loss: -1.146783 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128588 INN loss: -1.128588 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158888 INN loss: -1.158888 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159459 INN loss: -1.159459 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108985 INN loss: -1.108985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145368 INN loss: -1.145368 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108699 INN loss: -1.108699 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147096 INN loss: -1.147096 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126344 INN loss: -1.126344 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119233 INN loss: -1.119233 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120286 INN loss: -1.120286 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097611 INN loss: -1.097611 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122388 INN loss: -1.122388 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152972 INN loss: -1.152972 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157387 INN loss: -1.157387 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167779 INN loss: -1.167779 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114347 INN loss: -1.114347 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155814 INN loss: -1.155814 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111155 INN loss: -1.111155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132555 INN loss: -1.132555 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157358 INN loss: -1.157358 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166077 INN loss: -1.166077 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139165 INN loss: -1.139165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128102 INN loss: -1.128102 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158301 INN loss: -1.158301 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114572 INN loss: -1.114572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141341 INN loss: -1.141341 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156941 INN loss: -1.156941 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164498 INN loss: -1.164498 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138425 INN loss: -1.138425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154803 INN loss: -1.154803 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139628 INN loss: -1.139628 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101282 INN loss: -1.101282 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149689 INN loss: -1.149689 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157264 INN loss: -1.157264 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117502 INN loss: -1.117502 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151560 INN loss: -1.151560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128390 INN loss: -1.128390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113172 INN loss: -1.113172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159751 INN loss: -1.159751 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096574 INN loss: -1.096574 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103261 INN loss: -1.103261 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094898 INN loss: -1.094898 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138304 INN loss: -1.138304 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107172 INN loss: -1.107172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152833 INN loss: -1.152833 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134197 INN loss: -1.134197 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095486 INN loss: -1.095486 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156243 INN loss: -1.156243 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091689 INN loss: -1.091689 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105476 INN loss: -1.105476 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085045 INN loss: -1.085045 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148100 INN loss: -1.148100 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081707 INN loss: -1.081707 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137697 INN loss: -1.137697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149235 INN loss: -1.149235 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122640 INN loss: -1.122640 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146896 INN loss: -1.146896 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168497 INN loss: -1.168497 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144857 INN loss: -1.144857 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.203589 INN loss: -1.203589 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120378 INN loss: -1.120378 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177436 INN loss: -1.177436 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146772 INN loss: -1.146772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085656 INN loss: -1.085656 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165583 INN loss: -1.165583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128103 INN loss: -1.128103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127120 INN loss: -1.127120 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140422 INN loss: -1.140422 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137528 INN loss: -1.137528 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146345 INN loss: -1.146345 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148623 INN loss: -1.148623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158257 INN loss: -1.158257 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111614 INN loss: -1.111614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127339 INN loss: -1.127339 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157460 INN loss: -1.157460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154901 INN loss: -1.154901 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143025 INN loss: -1.143025 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████▋                                                   | 5/50 [3:27:47<35:40:03, 2853.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.140640 INN loss: -1.140640 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110045 INN loss: -1.110045 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152180 INN loss: -1.152180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109518 INN loss: -1.109518 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128384 INN loss: -1.128384 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107267 INN loss: -1.107267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126180 INN loss: -1.126180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148946 INN loss: -1.148946 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100079 INN loss: -1.100079 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.200029 INN loss: -1.200029 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100299 INN loss: -1.100299 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149154 INN loss: -1.149154 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160400 INN loss: -1.160400 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104696 INN loss: -1.104696 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151386 INN loss: -1.151386 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092382 INN loss: -1.092382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122101 INN loss: -1.122101 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149635 INN loss: -1.149635 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155139 INN loss: -1.155139 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120431 INN loss: -1.120431 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145496 INN loss: -1.145496 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120268 INN loss: -1.120268 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121859 INN loss: -1.121859 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161155 INN loss: -1.161155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120476 INN loss: -1.120476 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135190 INN loss: -1.135190 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111795 INN loss: -1.111795 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098077 INN loss: -1.098077 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165068 INN loss: -1.165068 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104224 INN loss: -1.104224 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133981 INN loss: -1.133981 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.072602 INN loss: -1.072602 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136577 INN loss: -1.136577 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130122 INN loss: -1.130122 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107414 INN loss: -1.107414 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161961 INN loss: -1.161961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.221116 INN loss: -1.221116 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129158 INN loss: -1.129158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100649 INN loss: -1.100649 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140359 INN loss: -1.140359 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100817 INN loss: -1.100817 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139490 INN loss: -1.139490 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142125 INN loss: -1.142125 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133719 INN loss: -1.133719 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161103 INN loss: -1.161103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151283 INN loss: -1.151283 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124318 INN loss: -1.124318 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156855 INN loss: -1.156855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129591 INN loss: -1.129591 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153426 INN loss: -1.153426 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139191 INN loss: -1.139191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129636 INN loss: -1.129636 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138032 INN loss: -1.138032 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149377 INN loss: -1.149377 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185821 INN loss: -1.185821 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122485 INN loss: -1.122485 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100423 INN loss: -1.100423 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138134 INN loss: -1.138134 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096345 INN loss: -1.096345 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169449 INN loss: -1.169449 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130580 INN loss: -1.130580 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148607 INN loss: -1.148607 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135139 INN loss: -1.135139 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144161 INN loss: -1.144161 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124166 INN loss: -1.124166 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136710 INN loss: -1.136710 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114598 INN loss: -1.114598 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130232 INN loss: -1.130232 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128056 INN loss: -1.128056 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172800 INN loss: -1.172800 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169954 INN loss: -1.169954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116234 INN loss: -1.116234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126839 INN loss: -1.126839 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093460 INN loss: -1.093460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133644 INN loss: -1.133644 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159040 INN loss: -1.159040 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178725 INN loss: -1.178725 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171875 INN loss: -1.171875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169075 INN loss: -1.169075 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128473 INN loss: -1.128473 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134185 INN loss: -1.134185 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152172 INN loss: -1.152172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080899 INN loss: -1.080899 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156152 INN loss: -1.156152 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151827 INN loss: -1.151827 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096198 INN loss: -1.096198 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134293 INN loss: -1.134293 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.197562 INN loss: -1.197562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136886 INN loss: -1.136886 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171874 INN loss: -1.171874 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127368 INN loss: -1.127368 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135939 INN loss: -1.135939 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119441 INN loss: -1.119441 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185301 INN loss: -1.185301 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133929 INN loss: -1.133929 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150001 INN loss: -1.150001 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158808 INN loss: -1.158808 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162698 INN loss: -1.162698 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133362 INN loss: -1.133362 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163213 INN loss: -1.163213 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135533 INN loss: -1.135533 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129806 INN loss: -1.129806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172970 INN loss: -1.172970 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144028 INN loss: -1.144028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161939 INN loss: -1.161939 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157568 INN loss: -1.157568 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149446 INN loss: -1.149446 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097871 INN loss: -1.097871 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107847 INN loss: -1.107847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103177 INN loss: -1.103177 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154740 INN loss: -1.154740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143753 INN loss: -1.143753 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166299 INN loss: -1.166299 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140316 INN loss: -1.140316 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138851 INN loss: -1.138851 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127895 INN loss: -1.127895 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142771 INN loss: -1.142771 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172035 INN loss: -1.172035 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131758 INN loss: -1.131758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175646 INN loss: -1.175646 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167610 INN loss: -1.167610 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166167 INN loss: -1.166167 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144895 INN loss: -1.144895 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126949 INN loss: -1.126949 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118130 INN loss: -1.118130 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160677 INN loss: -1.160677 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179546 INN loss: -1.179546 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111367 INN loss: -1.111367 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130724 INN loss: -1.130724 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185732 INN loss: -1.185732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156264 INN loss: -1.156264 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108232 INN loss: -1.108232 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154079 INN loss: -1.154079 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125621 INN loss: -1.125621 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114139 INN loss: -1.114139 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108475 INN loss: -1.108475 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152803 INN loss: -1.152803 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148410 INN loss: -1.148410 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136914 INN loss: -1.136914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118166 INN loss: -1.118166 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163686 INN loss: -1.163686 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132384 INN loss: -1.132384 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124161 INN loss: -1.124161 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130259 INN loss: -1.130259 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142390 INN loss: -1.142390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110906 INN loss: -1.110906 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120116 INN loss: -1.120116 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.053908 INN loss: -1.053908 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111363 INN loss: -1.111363 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096302 INN loss: -1.096302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104835 INN loss: -1.104835 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176299 INN loss: -1.176299 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111408 INN loss: -1.111408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153452 INN loss: -1.153452 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134328 INN loss: -1.134328 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168793 INN loss: -1.168793 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150222 INN loss: -1.150222 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176503 INN loss: -1.176503 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141000 INN loss: -1.141000 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120647 INN loss: -1.120647 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████▊                                                  | 6/50 [3:29:28<23:26:14, 1917.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.150535 INN loss: -1.150535 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146159 INN loss: -1.146159 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115935 INN loss: -1.115935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123658 INN loss: -1.123658 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080589 INN loss: -1.080589 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118145 INN loss: -1.118145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121377 INN loss: -1.121377 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099759 INN loss: -1.099759 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134524 INN loss: -1.134524 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168497 INN loss: -1.168497 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133780 INN loss: -1.133780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130876 INN loss: -1.130876 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165145 INN loss: -1.165145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098916 INN loss: -1.098916 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132282 INN loss: -1.132282 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119335 INN loss: -1.119335 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111454 INN loss: -1.111454 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138606 INN loss: -1.138606 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181254 INN loss: -1.181254 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110557 INN loss: -1.110557 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133220 INN loss: -1.133220 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099218 INN loss: -1.099218 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.204246 INN loss: -1.204246 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132175 INN loss: -1.132175 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132173 INN loss: -1.132173 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145656 INN loss: -1.145656 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081391 INN loss: -1.081391 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127683 INN loss: -1.127683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124054 INN loss: -1.124054 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102258 INN loss: -1.102258 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160970 INN loss: -1.160970 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153021 INN loss: -1.153021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165924 INN loss: -1.165924 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089936 INN loss: -1.089936 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095179 INN loss: -1.095179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163740 INN loss: -1.163740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147664 INN loss: -1.147664 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136845 INN loss: -1.136845 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104847 INN loss: -1.104847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122109 INN loss: -1.122109 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146569 INN loss: -1.146569 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116634 INN loss: -1.116634 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125523 INN loss: -1.125523 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157396 INN loss: -1.157396 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127217 INN loss: -1.127217 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163293 INN loss: -1.163293 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127640 INN loss: -1.127640 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179735 INN loss: -1.179735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138731 INN loss: -1.138731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135594 INN loss: -1.135594 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130267 INN loss: -1.130267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133238 INN loss: -1.133238 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121127 INN loss: -1.121127 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150743 INN loss: -1.150743 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095199 INN loss: -1.095199 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122395 INN loss: -1.122395 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114284 INN loss: -1.114284 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105019 INN loss: -1.105019 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138360 INN loss: -1.138360 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148341 INN loss: -1.148341 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144900 INN loss: -1.144900 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144601 INN loss: -1.144601 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114308 INN loss: -1.114308 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110204 INN loss: -1.110204 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114421 INN loss: -1.114421 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111752 INN loss: -1.111752 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142750 INN loss: -1.142750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115955 INN loss: -1.115955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161063 INN loss: -1.161063 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126509 INN loss: -1.126509 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117272 INN loss: -1.117272 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184393 INN loss: -1.184393 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126726 INN loss: -1.126726 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163408 INN loss: -1.163408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133643 INN loss: -1.133643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134776 INN loss: -1.134776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153528 INN loss: -1.153528 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130269 INN loss: -1.130269 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163552 INN loss: -1.163552 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131942 INN loss: -1.131942 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110281 INN loss: -1.110281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136517 INN loss: -1.136517 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136679 INN loss: -1.136679 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159870 INN loss: -1.159870 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155883 INN loss: -1.155883 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157535 INN loss: -1.157535 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124366 INN loss: -1.124366 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147236 INN loss: -1.147236 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162320 INN loss: -1.162320 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175367 INN loss: -1.175367 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132382 INN loss: -1.132382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156116 INN loss: -1.156116 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156351 INN loss: -1.156351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177781 INN loss: -1.177781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145514 INN loss: -1.145514 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114627 INN loss: -1.114627 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175385 INN loss: -1.175385 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095059 INN loss: -1.095059 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137130 INN loss: -1.137130 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141380 INN loss: -1.141380 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098872 INN loss: -1.098872 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107289 INN loss: -1.107289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118535 INN loss: -1.118535 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175862 INN loss: -1.175862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118195 INN loss: -1.118195 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160089 INN loss: -1.160089 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109043 INN loss: -1.109043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149562 INN loss: -1.149562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167321 INN loss: -1.167321 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125988 INN loss: -1.125988 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132727 INN loss: -1.132727 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156805 INN loss: -1.156805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102315 INN loss: -1.102315 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166070 INN loss: -1.166070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117607 INN loss: -1.117607 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114070 INN loss: -1.114070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122325 INN loss: -1.122325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135234 INN loss: -1.135234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107242 INN loss: -1.107242 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089591 INN loss: -1.089591 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187299 INN loss: -1.187299 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126238 INN loss: -1.126238 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144357 INN loss: -1.144357 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140434 INN loss: -1.140434 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134776 INN loss: -1.134776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096074 INN loss: -1.096074 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140537 INN loss: -1.140537 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116495 INN loss: -1.116495 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162017 INN loss: -1.162017 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170224 INN loss: -1.170224 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134239 INN loss: -1.134239 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113017 INN loss: -1.113017 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116976 INN loss: -1.116976 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098891 INN loss: -1.098891 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162705 INN loss: -1.162705 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139344 INN loss: -1.139344 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136190 INN loss: -1.136190 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132254 INN loss: -1.132254 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121801 INN loss: -1.121801 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147779 INN loss: -1.147779 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180674 INN loss: -1.180674 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147553 INN loss: -1.147553 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127463 INN loss: -1.127463 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153519 INN loss: -1.153519 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128782 INN loss: -1.128782 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128917 INN loss: -1.128917 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106157 INN loss: -1.106157 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123694 INN loss: -1.123694 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.061582 INN loss: -1.061582 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111350 INN loss: -1.111350 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146013 INN loss: -1.146013 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147230 INN loss: -1.147230 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150661 INN loss: -1.150661 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121697 INN loss: -1.121697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158780 INN loss: -1.158780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132139 INN loss: -1.132139 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162045 INN loss: -1.162045 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149115 INN loss: -1.149115 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130958 INN loss: -1.130958 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095749 INN loss: -1.095749 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████▉                                                 | 7/50 [3:31:06<15:47:52, 1322.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.124214 INN loss: -1.124214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095955 INN loss: -1.095955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168688 INN loss: -1.168688 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096100 INN loss: -1.096100 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115458 INN loss: -1.115458 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107220 INN loss: -1.107220 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148722 INN loss: -1.148722 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152256 INN loss: -1.152256 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125891 INN loss: -1.125891 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109837 INN loss: -1.109837 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177379 INN loss: -1.177379 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138601 INN loss: -1.138601 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154541 INN loss: -1.154541 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156969 INN loss: -1.156969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170149 INN loss: -1.170149 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152591 INN loss: -1.152591 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156003 INN loss: -1.156003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187187 INN loss: -1.187187 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123128 INN loss: -1.123128 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.061121 INN loss: -1.061121 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151843 INN loss: -1.151843 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152623 INN loss: -1.152623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101562 INN loss: -1.101562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106717 INN loss: -1.106717 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126895 INN loss: -1.126895 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121993 INN loss: -1.121993 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120214 INN loss: -1.120214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127021 INN loss: -1.127021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146390 INN loss: -1.146390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095069 INN loss: -1.095069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110950 INN loss: -1.110950 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135544 INN loss: -1.135544 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157486 INN loss: -1.157486 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169622 INN loss: -1.169622 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179401 INN loss: -1.179401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114900 INN loss: -1.114900 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110266 INN loss: -1.110266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153255 INN loss: -1.153255 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164215 INN loss: -1.164215 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109818 INN loss: -1.109818 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121341 INN loss: -1.121341 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108127 INN loss: -1.108127 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162216 INN loss: -1.162216 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132184 INN loss: -1.132184 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134040 INN loss: -1.134040 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161130 INN loss: -1.161130 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095282 INN loss: -1.095282 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141286 INN loss: -1.141286 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136709 INN loss: -1.136709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093088 INN loss: -1.093088 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156749 INN loss: -1.156749 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142008 INN loss: -1.142008 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130614 INN loss: -1.130614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149090 INN loss: -1.149090 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156218 INN loss: -1.156218 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102643 INN loss: -1.102643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129396 INN loss: -1.129396 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180080 INN loss: -1.180080 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159022 INN loss: -1.159022 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.192982 INN loss: -1.192982 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125500 INN loss: -1.125500 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157958 INN loss: -1.157958 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140302 INN loss: -1.140302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142193 INN loss: -1.142193 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161459 INN loss: -1.161459 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.195269 INN loss: -1.195269 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123581 INN loss: -1.123581 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143227 INN loss: -1.143227 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139783 INN loss: -1.139783 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114358 INN loss: -1.114358 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139379 INN loss: -1.139379 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129567 INN loss: -1.129567 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109496 INN loss: -1.109496 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.073727 INN loss: -1.073727 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118066 INN loss: -1.118066 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111909 INN loss: -1.111909 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129693 INN loss: -1.129693 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129757 INN loss: -1.129757 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130431 INN loss: -1.130431 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148669 INN loss: -1.148669 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137589 INN loss: -1.137589 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118851 INN loss: -1.118851 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160849 INN loss: -1.160849 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157207 INN loss: -1.157207 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115335 INN loss: -1.115335 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088583 INN loss: -1.088583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152049 INN loss: -1.152049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117055 INN loss: -1.117055 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154853 INN loss: -1.154853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084548 INN loss: -1.084548 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177329 INN loss: -1.177329 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105744 INN loss: -1.105744 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113396 INN loss: -1.113396 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120233 INN loss: -1.120233 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150177 INN loss: -1.150177 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143128 INN loss: -1.143128 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124871 INN loss: -1.124871 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132103 INN loss: -1.132103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157947 INN loss: -1.157947 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165795 INN loss: -1.165795 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.061207 INN loss: -1.061207 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147812 INN loss: -1.147812 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164112 INN loss: -1.164112 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130237 INN loss: -1.130237 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123617 INN loss: -1.123617 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188385 INN loss: -1.188385 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114844 INN loss: -1.114844 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136435 INN loss: -1.136435 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130526 INN loss: -1.130526 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135592 INN loss: -1.135592 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121058 INN loss: -1.121058 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120794 INN loss: -1.120794 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136667 INN loss: -1.136667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150844 INN loss: -1.150844 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110126 INN loss: -1.110126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130173 INN loss: -1.130173 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155527 INN loss: -1.155527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140157 INN loss: -1.140157 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124752 INN loss: -1.124752 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156430 INN loss: -1.156430 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132683 INN loss: -1.132683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169422 INN loss: -1.169422 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135289 INN loss: -1.135289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128237 INN loss: -1.128237 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138718 INN loss: -1.138718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131827 INN loss: -1.131827 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133451 INN loss: -1.133451 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177893 INN loss: -1.177893 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169875 INN loss: -1.169875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145262 INN loss: -1.145262 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158771 INN loss: -1.158771 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105356 INN loss: -1.105356 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162144 INN loss: -1.162144 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151330 INN loss: -1.151330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128885 INN loss: -1.128885 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175703 INN loss: -1.175703 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166205 INN loss: -1.166205 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128695 INN loss: -1.128695 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110380 INN loss: -1.110380 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168165 INN loss: -1.168165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117750 INN loss: -1.117750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137831 INN loss: -1.137831 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160506 INN loss: -1.160506 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106467 INN loss: -1.106467 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113764 INN loss: -1.113764 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087268 INN loss: -1.087268 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098226 INN loss: -1.098226 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139509 INN loss: -1.139509 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124268 INN loss: -1.124268 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120979 INN loss: -1.120979 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165468 INN loss: -1.165468 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128516 INN loss: -1.128516 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090183 INN loss: -1.090183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107984 INN loss: -1.107984 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144037 INN loss: -1.144037 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165736 INN loss: -1.165736 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145174 INN loss: -1.145174 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171802 INN loss: -1.171802 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150683 INN loss: -1.150683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157213 INN loss: -1.157213 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████▎                                                | 8/50 [3:35:04<11:24:10, 977.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.101491 INN loss: -1.101491 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130812 INN loss: -1.130812 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089200 INN loss: -1.089200 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157317 INN loss: -1.157317 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105517 INN loss: -1.105517 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099196 INN loss: -1.099196 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137061 INN loss: -1.137061 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101036 INN loss: -1.101036 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091969 INN loss: -1.091969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121720 INN loss: -1.121720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140309 INN loss: -1.140309 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116481 INN loss: -1.116481 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092442 INN loss: -1.092442 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151002 INN loss: -1.151002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154714 INN loss: -1.154714 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178164 INN loss: -1.178164 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101574 INN loss: -1.101574 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177746 INN loss: -1.177746 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110720 INN loss: -1.110720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110356 INN loss: -1.110356 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136881 INN loss: -1.136881 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172549 INN loss: -1.172549 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127410 INN loss: -1.127410 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170780 INN loss: -1.170780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138668 INN loss: -1.138668 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136373 INN loss: -1.136373 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125143 INN loss: -1.125143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128653 INN loss: -1.128653 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174874 INN loss: -1.174874 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131551 INN loss: -1.131551 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178752 INN loss: -1.178752 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122992 INN loss: -1.122992 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128073 INN loss: -1.128073 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182352 INN loss: -1.182352 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141643 INN loss: -1.141643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160248 INN loss: -1.160248 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140227 INN loss: -1.140227 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146427 INN loss: -1.146427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146313 INN loss: -1.146313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144323 INN loss: -1.144323 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115443 INN loss: -1.115443 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143134 INN loss: -1.143134 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123206 INN loss: -1.123206 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157792 INN loss: -1.157792 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099257 INN loss: -1.099257 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100039 INN loss: -1.100039 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125807 INN loss: -1.125807 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147786 INN loss: -1.147786 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085312 INN loss: -1.085312 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148220 INN loss: -1.148220 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146372 INN loss: -1.146372 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087037 INN loss: -1.087037 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179178 INN loss: -1.179178 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114616 INN loss: -1.114616 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150933 INN loss: -1.150933 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.193854 INN loss: -1.193854 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121311 INN loss: -1.121311 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115263 INN loss: -1.115263 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147218 INN loss: -1.147218 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171914 INN loss: -1.171914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147184 INN loss: -1.147184 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139407 INN loss: -1.139407 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181332 INN loss: -1.181332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141852 INN loss: -1.141852 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109908 INN loss: -1.109908 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136281 INN loss: -1.136281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112269 INN loss: -1.112269 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117220 INN loss: -1.117220 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154170 INN loss: -1.154170 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112853 INN loss: -1.112853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156745 INN loss: -1.156745 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144970 INN loss: -1.144970 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132488 INN loss: -1.132488 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101916 INN loss: -1.101916 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126421 INN loss: -1.126421 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115834 INN loss: -1.115834 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135720 INN loss: -1.135720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148568 INN loss: -1.148568 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084582 INN loss: -1.084582 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130151 INN loss: -1.130151 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157878 INN loss: -1.157878 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.079554 INN loss: -1.079554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076188 INN loss: -1.076188 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103987 INN loss: -1.103987 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110440 INN loss: -1.110440 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133012 INN loss: -1.133012 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105281 INN loss: -1.105281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117297 INN loss: -1.117297 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104914 INN loss: -1.104914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142216 INN loss: -1.142216 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156070 INN loss: -1.156070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177957 INN loss: -1.177957 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168617 INN loss: -1.168617 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163568 INN loss: -1.163568 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106188 INN loss: -1.106188 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113682 INN loss: -1.113682 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134788 INN loss: -1.134788 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094268 INN loss: -1.094268 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152667 INN loss: -1.152667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138793 INN loss: -1.138793 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132658 INN loss: -1.132658 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136351 INN loss: -1.136351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118180 INN loss: -1.118180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113223 INN loss: -1.113223 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154382 INN loss: -1.154382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.197028 INN loss: -1.197028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134643 INN loss: -1.134643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120756 INN loss: -1.120756 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152761 INN loss: -1.152761 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139833 INN loss: -1.139833 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186362 INN loss: -1.186362 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179553 INN loss: -1.179553 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117852 INN loss: -1.117852 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140466 INN loss: -1.140466 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136108 INN loss: -1.136108 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118742 INN loss: -1.118742 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126647 INN loss: -1.126647 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136938 INN loss: -1.136938 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146110 INN loss: -1.146110 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124376 INN loss: -1.124376 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132347 INN loss: -1.132347 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.086542 INN loss: -1.086542 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122616 INN loss: -1.122616 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163622 INN loss: -1.163622 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148544 INN loss: -1.148544 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142735 INN loss: -1.142735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132581 INN loss: -1.132581 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142549 INN loss: -1.142549 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130131 INN loss: -1.130131 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152313 INN loss: -1.152313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123330 INN loss: -1.123330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164732 INN loss: -1.164732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162323 INN loss: -1.162323 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143506 INN loss: -1.143506 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167517 INN loss: -1.167517 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114767 INN loss: -1.114767 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155805 INN loss: -1.155805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154811 INN loss: -1.154811 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120593 INN loss: -1.120593 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138183 INN loss: -1.138183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137128 INN loss: -1.137128 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131043 INN loss: -1.131043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134651 INN loss: -1.134651 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126059 INN loss: -1.126059 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148002 INN loss: -1.148002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121171 INN loss: -1.121171 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105749 INN loss: -1.105749 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150118 INN loss: -1.150118 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108941 INN loss: -1.108941 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143212 INN loss: -1.143212 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130621 INN loss: -1.130621 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109806 INN loss: -1.109806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138110 INN loss: -1.138110 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172606 INN loss: -1.172606 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156747 INN loss: -1.156747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139101 INN loss: -1.139101 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136652 INN loss: -1.136652 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170629 INN loss: -1.170629 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134373 INN loss: -1.134373 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159428 INN loss: -1.159428 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████▌                                                | 9/50 [3:36:37<7:58:53, 700.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.136236 INN loss: -1.136236 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128464 INN loss: -1.128464 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.077549 INN loss: -1.077549 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134100 INN loss: -1.134100 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119265 INN loss: -1.119265 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146740 INN loss: -1.146740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114442 INN loss: -1.114442 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136954 INN loss: -1.136954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117861 INN loss: -1.117861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127450 INN loss: -1.127450 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155648 INN loss: -1.155648 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145690 INN loss: -1.145690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104865 INN loss: -1.104865 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154286 INN loss: -1.154286 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083294 INN loss: -1.083294 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153643 INN loss: -1.153643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142368 INN loss: -1.142368 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102680 INN loss: -1.102680 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116882 INN loss: -1.116882 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165025 INN loss: -1.165025 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107207 INN loss: -1.107207 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170188 INN loss: -1.170188 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163963 INN loss: -1.163963 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135175 INN loss: -1.135175 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166789 INN loss: -1.166789 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108046 INN loss: -1.108046 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124698 INN loss: -1.124698 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146208 INN loss: -1.146208 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124331 INN loss: -1.124331 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160184 INN loss: -1.160184 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124929 INN loss: -1.124929 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123488 INN loss: -1.123488 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159991 INN loss: -1.159991 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152985 INN loss: -1.152985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.063690 INN loss: -1.063690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117839 INN loss: -1.117839 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166753 INN loss: -1.166753 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107350 INN loss: -1.107350 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138149 INN loss: -1.138149 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124817 INN loss: -1.124817 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131183 INN loss: -1.131183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155242 INN loss: -1.155242 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.193146 INN loss: -1.193146 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156173 INN loss: -1.156173 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153827 INN loss: -1.153827 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136469 INN loss: -1.136469 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130668 INN loss: -1.130668 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154402 INN loss: -1.154402 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169895 INN loss: -1.169895 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120853 INN loss: -1.120853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143528 INN loss: -1.143528 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154378 INN loss: -1.154378 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107534 INN loss: -1.107534 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122809 INN loss: -1.122809 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118081 INN loss: -1.118081 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139878 INN loss: -1.139878 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153357 INN loss: -1.153357 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142731 INN loss: -1.142731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089329 INN loss: -1.089329 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116255 INN loss: -1.116255 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177213 INN loss: -1.177213 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115941 INN loss: -1.115941 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178572 INN loss: -1.178572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094690 INN loss: -1.094690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124280 INN loss: -1.124280 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136416 INN loss: -1.136416 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114951 INN loss: -1.114951 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111685 INN loss: -1.111685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180441 INN loss: -1.180441 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122440 INN loss: -1.122440 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132713 INN loss: -1.132713 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149531 INN loss: -1.149531 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173692 INN loss: -1.173692 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165718 INN loss: -1.165718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152450 INN loss: -1.152450 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114103 INN loss: -1.114103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106001 INN loss: -1.106001 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123034 INN loss: -1.123034 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130656 INN loss: -1.130656 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138018 INN loss: -1.138018 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161989 INN loss: -1.161989 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129522 INN loss: -1.129522 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162098 INN loss: -1.162098 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145989 INN loss: -1.145989 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145964 INN loss: -1.145964 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160131 INN loss: -1.160131 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179696 INN loss: -1.179696 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154642 INN loss: -1.154642 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177656 INN loss: -1.177656 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122596 INN loss: -1.122596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180517 INN loss: -1.180517 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100871 INN loss: -1.100871 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130497 INN loss: -1.130497 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158812 INN loss: -1.158812 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109081 INN loss: -1.109081 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113282 INN loss: -1.113282 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121745 INN loss: -1.121745 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141553 INN loss: -1.141553 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156076 INN loss: -1.156076 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082099 INN loss: -1.082099 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099589 INN loss: -1.099589 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113141 INN loss: -1.113141 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103583 INN loss: -1.103583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142731 INN loss: -1.142731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156554 INN loss: -1.156554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132786 INN loss: -1.132786 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106705 INN loss: -1.106705 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167292 INN loss: -1.167292 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133065 INN loss: -1.133065 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162949 INN loss: -1.162949 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128404 INN loss: -1.128404 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160222 INN loss: -1.160222 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162639 INN loss: -1.162639 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114170 INN loss: -1.114170 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114804 INN loss: -1.114804 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132048 INN loss: -1.132048 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115100 INN loss: -1.115100 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098020 INN loss: -1.098020 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130772 INN loss: -1.130772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145946 INN loss: -1.145946 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128337 INN loss: -1.128337 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130965 INN loss: -1.130965 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124599 INN loss: -1.124599 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123587 INN loss: -1.123587 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116886 INN loss: -1.116886 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096046 INN loss: -1.096046 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151641 INN loss: -1.151641 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147261 INN loss: -1.147261 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.071564 INN loss: -1.071564 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111141 INN loss: -1.111141 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154820 INN loss: -1.154820 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139637 INN loss: -1.139637 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103649 INN loss: -1.103649 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172418 INN loss: -1.172418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124298 INN loss: -1.124298 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163210 INN loss: -1.163210 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133027 INN loss: -1.133027 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166247 INN loss: -1.166247 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124266 INN loss: -1.124266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132470 INN loss: -1.132470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144873 INN loss: -1.144873 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183538 INN loss: -1.183538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154190 INN loss: -1.154190 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115354 INN loss: -1.115354 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153277 INN loss: -1.153277 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106533 INN loss: -1.106533 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149853 INN loss: -1.149853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148028 INN loss: -1.148028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146525 INN loss: -1.146525 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156801 INN loss: -1.156801 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098687 INN loss: -1.098687 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163618 INN loss: -1.163618 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142496 INN loss: -1.142496 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121632 INN loss: -1.121632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139099 INN loss: -1.139099 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151638 INN loss: -1.151638 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149063 INN loss: -1.149063 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.071258 INN loss: -1.071258 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142652 INN loss: -1.142652 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133360 INN loss: -1.133360 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████▌                                              | 10/50 [3:38:13<5:42:50, 514.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.094639 INN loss: -1.094639 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151922 INN loss: -1.151922 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115103 INN loss: -1.115103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108470 INN loss: -1.108470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115472 INN loss: -1.115472 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146435 INN loss: -1.146435 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144114 INN loss: -1.144114 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.078470 INN loss: -1.078470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157402 INN loss: -1.157402 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157355 INN loss: -1.157355 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110453 INN loss: -1.110453 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133651 INN loss: -1.133651 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121046 INN loss: -1.121046 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161065 INN loss: -1.161065 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145667 INN loss: -1.145667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.072779 INN loss: -1.072779 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159973 INN loss: -1.159973 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126750 INN loss: -1.126750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151870 INN loss: -1.151870 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160276 INN loss: -1.160276 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152596 INN loss: -1.152596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120078 INN loss: -1.120078 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165606 INN loss: -1.165606 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105856 INN loss: -1.105856 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135123 INN loss: -1.135123 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179477 INN loss: -1.179477 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171954 INN loss: -1.171954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148076 INN loss: -1.148076 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182714 INN loss: -1.182714 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158584 INN loss: -1.158584 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189932 INN loss: -1.189932 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153129 INN loss: -1.153129 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116955 INN loss: -1.116955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153917 INN loss: -1.153917 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145474 INN loss: -1.145474 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132237 INN loss: -1.132237 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114550 INN loss: -1.114550 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172796 INN loss: -1.172796 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108133 INN loss: -1.108133 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121322 INN loss: -1.121322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.051445 INN loss: -1.051445 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157679 INN loss: -1.157679 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139917 INN loss: -1.139917 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123277 INN loss: -1.123277 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164452 INN loss: -1.164452 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131636 INN loss: -1.131636 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131057 INN loss: -1.131057 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126680 INN loss: -1.126680 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123360 INN loss: -1.123360 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136733 INN loss: -1.136733 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083720 INN loss: -1.083720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116623 INN loss: -1.116623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121053 INN loss: -1.121053 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131353 INN loss: -1.131353 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152352 INN loss: -1.152352 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146835 INN loss: -1.146835 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140913 INN loss: -1.140913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113659 INN loss: -1.113659 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170335 INN loss: -1.170335 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160188 INN loss: -1.160188 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177057 INN loss: -1.177057 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154311 INN loss: -1.154311 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127562 INN loss: -1.127562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139235 INN loss: -1.139235 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122869 INN loss: -1.122869 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153738 INN loss: -1.153738 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156336 INN loss: -1.156336 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142560 INN loss: -1.142560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124910 INN loss: -1.124910 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139727 INN loss: -1.139727 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163837 INN loss: -1.163837 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181983 INN loss: -1.181983 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135889 INN loss: -1.135889 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149764 INN loss: -1.149764 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108927 INN loss: -1.108927 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159466 INN loss: -1.159466 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097083 INN loss: -1.097083 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129361 INN loss: -1.129361 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116258 INN loss: -1.116258 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129172 INN loss: -1.129172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178473 INN loss: -1.178473 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168853 INN loss: -1.168853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149935 INN loss: -1.149935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156022 INN loss: -1.156022 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128159 INN loss: -1.128159 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137799 INN loss: -1.137799 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157601 INN loss: -1.157601 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121662 INN loss: -1.121662 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118783 INN loss: -1.118783 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189118 INN loss: -1.189118 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131198 INN loss: -1.131198 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119287 INN loss: -1.119287 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112319 INN loss: -1.112319 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162798 INN loss: -1.162798 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122800 INN loss: -1.122800 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151375 INN loss: -1.151375 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150218 INN loss: -1.150218 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128417 INN loss: -1.128417 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122030 INN loss: -1.122030 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174146 INN loss: -1.174146 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118846 INN loss: -1.118846 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111172 INN loss: -1.111172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161143 INN loss: -1.161143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177289 INN loss: -1.177289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182007 INN loss: -1.182007 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122718 INN loss: -1.122718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162895 INN loss: -1.162895 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126434 INN loss: -1.126434 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127161 INN loss: -1.127161 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143362 INN loss: -1.143362 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115675 INN loss: -1.115675 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088981 INN loss: -1.088981 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133417 INN loss: -1.133417 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129827 INN loss: -1.129827 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.078059 INN loss: -1.078059 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134637 INN loss: -1.134637 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175477 INN loss: -1.175477 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140358 INN loss: -1.140358 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104090 INN loss: -1.104090 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172563 INN loss: -1.172563 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128324 INN loss: -1.128324 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114469 INN loss: -1.114469 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150337 INN loss: -1.150337 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134067 INN loss: -1.134067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139003 INN loss: -1.139003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156917 INN loss: -1.156917 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138428 INN loss: -1.138428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136627 INN loss: -1.136627 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181700 INN loss: -1.181700 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142723 INN loss: -1.142723 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122880 INN loss: -1.122880 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145317 INN loss: -1.145317 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148709 INN loss: -1.148709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152862 INN loss: -1.152862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136180 INN loss: -1.136180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154791 INN loss: -1.154791 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098966 INN loss: -1.098966 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132273 INN loss: -1.132273 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123059 INN loss: -1.123059 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154437 INN loss: -1.154437 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144977 INN loss: -1.144977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161371 INN loss: -1.161371 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145301 INN loss: -1.145301 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090513 INN loss: -1.090513 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134038 INN loss: -1.134038 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139623 INN loss: -1.139623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130527 INN loss: -1.130527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125638 INN loss: -1.125638 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158507 INN loss: -1.158507 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152145 INN loss: -1.152145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162291 INN loss: -1.162291 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082126 INN loss: -1.082126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141318 INN loss: -1.141318 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160659 INN loss: -1.160659 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156827 INN loss: -1.156827 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109376 INN loss: -1.109376 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120042 INN loss: -1.120042 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147004 INN loss: -1.147004 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138579 INN loss: -1.138579 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125318 INN loss: -1.125318 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████▊                                             | 11/50 [3:39:43<4:09:48, 384.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.179374 INN loss: -1.179374 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152563 INN loss: -1.152563 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116653 INN loss: -1.116653 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130381 INN loss: -1.130381 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170301 INN loss: -1.170301 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163924 INN loss: -1.163924 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141886 INN loss: -1.141886 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126501 INN loss: -1.126501 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145631 INN loss: -1.145631 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.207391 INN loss: -1.207391 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174739 INN loss: -1.174739 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155004 INN loss: -1.155004 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103425 INN loss: -1.103425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127948 INN loss: -1.127948 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186700 INN loss: -1.186700 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145178 INN loss: -1.145178 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141266 INN loss: -1.141266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101809 INN loss: -1.101809 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168872 INN loss: -1.168872 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106015 INN loss: -1.106015 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153159 INN loss: -1.153159 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118600 INN loss: -1.118600 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132723 INN loss: -1.132723 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155076 INN loss: -1.155076 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145355 INN loss: -1.145355 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128973 INN loss: -1.128973 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098041 INN loss: -1.098041 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132370 INN loss: -1.132370 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185061 INN loss: -1.185061 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.196697 INN loss: -1.196697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.073686 INN loss: -1.073686 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081111 INN loss: -1.081111 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116879 INN loss: -1.116879 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126105 INN loss: -1.126105 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141824 INN loss: -1.141824 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149281 INN loss: -1.149281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129717 INN loss: -1.129717 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144418 INN loss: -1.144418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185690 INN loss: -1.185690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131423 INN loss: -1.131423 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142019 INN loss: -1.142019 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178518 INN loss: -1.178518 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153181 INN loss: -1.153181 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164319 INN loss: -1.164319 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137049 INN loss: -1.137049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138523 INN loss: -1.138523 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165319 INN loss: -1.165319 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152660 INN loss: -1.152660 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138268 INN loss: -1.138268 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148894 INN loss: -1.148894 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137364 INN loss: -1.137364 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122969 INN loss: -1.122969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144921 INN loss: -1.144921 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143585 INN loss: -1.143585 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183607 INN loss: -1.183607 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139843 INN loss: -1.139843 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144516 INN loss: -1.144516 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139970 INN loss: -1.139970 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100710 INN loss: -1.100710 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154043 INN loss: -1.154043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110840 INN loss: -1.110840 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142465 INN loss: -1.142465 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102667 INN loss: -1.102667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116811 INN loss: -1.116811 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131550 INN loss: -1.131550 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172597 INN loss: -1.172597 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135982 INN loss: -1.135982 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153899 INN loss: -1.153899 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155292 INN loss: -1.155292 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166736 INN loss: -1.166736 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130720 INN loss: -1.130720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162394 INN loss: -1.162394 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147813 INN loss: -1.147813 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137173 INN loss: -1.137173 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105427 INN loss: -1.105427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094113 INN loss: -1.094113 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124157 INN loss: -1.124157 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159870 INN loss: -1.159870 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161198 INN loss: -1.161198 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099731 INN loss: -1.099731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110311 INN loss: -1.110311 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161161 INN loss: -1.161161 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122622 INN loss: -1.122622 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153752 INN loss: -1.153752 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138261 INN loss: -1.138261 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133218 INN loss: -1.133218 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095220 INN loss: -1.095220 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122266 INN loss: -1.122266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128014 INN loss: -1.128014 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101164 INN loss: -1.101164 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183574 INN loss: -1.183574 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114579 INN loss: -1.114579 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176749 INN loss: -1.176749 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.209114 INN loss: -1.209114 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141461 INN loss: -1.141461 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112086 INN loss: -1.112086 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111446 INN loss: -1.111446 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.195516 INN loss: -1.195516 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128314 INN loss: -1.128314 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151953 INN loss: -1.151953 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092483 INN loss: -1.092483 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101748 INN loss: -1.101748 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133346 INN loss: -1.133346 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164687 INN loss: -1.164687 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119158 INN loss: -1.119158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168901 INN loss: -1.168901 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121780 INN loss: -1.121780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169633 INN loss: -1.169633 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127969 INN loss: -1.127969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154380 INN loss: -1.154380 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144031 INN loss: -1.144031 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126964 INN loss: -1.126964 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136067 INN loss: -1.136067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138143 INN loss: -1.138143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117990 INN loss: -1.117990 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.195228 INN loss: -1.195228 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150678 INN loss: -1.150678 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.198873 INN loss: -1.198873 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128109 INN loss: -1.128109 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157742 INN loss: -1.157742 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123246 INN loss: -1.123246 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.192328 INN loss: -1.192328 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081275 INN loss: -1.081275 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135127 INN loss: -1.135127 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190139 INN loss: -1.190139 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128882 INN loss: -1.128882 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157487 INN loss: -1.157487 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128284 INN loss: -1.128284 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125838 INN loss: -1.125838 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138756 INN loss: -1.138756 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136017 INN loss: -1.136017 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103309 INN loss: -1.103309 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137666 INN loss: -1.137666 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118711 INN loss: -1.118711 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161872 INN loss: -1.161872 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147069 INN loss: -1.147069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141707 INN loss: -1.141707 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128492 INN loss: -1.128492 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125931 INN loss: -1.125931 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188061 INN loss: -1.188061 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121544 INN loss: -1.121544 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189457 INN loss: -1.189457 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131095 INN loss: -1.131095 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143952 INN loss: -1.143952 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149243 INN loss: -1.149243 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149346 INN loss: -1.149346 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132103 INN loss: -1.132103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134445 INN loss: -1.134445 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127493 INN loss: -1.127493 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168860 INN loss: -1.168860 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164556 INN loss: -1.164556 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134230 INN loss: -1.134230 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126363 INN loss: -1.126363 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146595 INN loss: -1.146595 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171006 INN loss: -1.171006 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171140 INN loss: -1.171140 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117499 INN loss: -1.117499 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155206 INN loss: -1.155206 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133742 INN loss: -1.133742 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099468 INN loss: -1.099468 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████▉                                            | 12/50 [3:41:18<3:07:38, 296.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.123788 INN loss: -1.123788 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160603 INN loss: -1.160603 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090775 INN loss: -1.090775 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135547 INN loss: -1.135547 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135615 INN loss: -1.135615 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166427 INN loss: -1.166427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106878 INN loss: -1.106878 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137871 INN loss: -1.137871 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158616 INN loss: -1.158616 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113777 INN loss: -1.113777 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121953 INN loss: -1.121953 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125604 INN loss: -1.125604 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136655 INN loss: -1.136655 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147202 INN loss: -1.147202 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142397 INN loss: -1.142397 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147090 INN loss: -1.147090 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101631 INN loss: -1.101631 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115709 INN loss: -1.115709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137969 INN loss: -1.137969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.073185 INN loss: -1.073185 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168947 INN loss: -1.168947 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144135 INN loss: -1.144135 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141487 INN loss: -1.141487 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102832 INN loss: -1.102832 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090790 INN loss: -1.090790 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187196 INN loss: -1.187196 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105853 INN loss: -1.105853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155280 INN loss: -1.155280 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088099 INN loss: -1.088099 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.047677 INN loss: -1.047677 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.077572 INN loss: -1.077572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159691 INN loss: -1.159691 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164593 INN loss: -1.164593 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120080 INN loss: -1.120080 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147373 INN loss: -1.147373 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141554 INN loss: -1.141554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150085 INN loss: -1.150085 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119541 INN loss: -1.119541 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152484 INN loss: -1.152484 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144491 INN loss: -1.144491 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163155 INN loss: -1.163155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088859 INN loss: -1.088859 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156072 INN loss: -1.156072 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.074893 INN loss: -1.074893 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166557 INN loss: -1.166557 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140935 INN loss: -1.140935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150311 INN loss: -1.150311 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142710 INN loss: -1.142710 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117879 INN loss: -1.117879 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141656 INN loss: -1.141656 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113058 INN loss: -1.113058 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105428 INN loss: -1.105428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109909 INN loss: -1.109909 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149262 INN loss: -1.149262 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.192004 INN loss: -1.192004 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164048 INN loss: -1.164048 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098855 INN loss: -1.098855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128142 INN loss: -1.128142 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152099 INN loss: -1.152099 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115930 INN loss: -1.115930 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107143 INN loss: -1.107143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147181 INN loss: -1.147181 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.086792 INN loss: -1.086792 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100562 INN loss: -1.100562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133379 INN loss: -1.133379 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142907 INN loss: -1.142907 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174010 INN loss: -1.174010 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157100 INN loss: -1.157100 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141673 INN loss: -1.141673 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143163 INN loss: -1.143163 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124688 INN loss: -1.124688 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128178 INN loss: -1.128178 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156198 INN loss: -1.156198 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139862 INN loss: -1.139862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167078 INN loss: -1.167078 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183156 INN loss: -1.183156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114702 INN loss: -1.114702 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094674 INN loss: -1.094674 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185351 INN loss: -1.185351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131471 INN loss: -1.131471 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127609 INN loss: -1.127609 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133449 INN loss: -1.133449 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150974 INN loss: -1.150974 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137344 INN loss: -1.137344 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129554 INN loss: -1.129554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151885 INN loss: -1.151885 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116394 INN loss: -1.116394 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167513 INN loss: -1.167513 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112758 INN loss: -1.112758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129397 INN loss: -1.129397 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148177 INN loss: -1.148177 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116554 INN loss: -1.116554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116545 INN loss: -1.116545 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131673 INN loss: -1.131673 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119013 INN loss: -1.119013 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088058 INN loss: -1.088058 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118572 INN loss: -1.118572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131981 INN loss: -1.131981 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110289 INN loss: -1.110289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108328 INN loss: -1.108328 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.078256 INN loss: -1.078256 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107269 INN loss: -1.107269 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110230 INN loss: -1.110230 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162576 INN loss: -1.162576 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109496 INN loss: -1.109496 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147407 INN loss: -1.147407 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141327 INN loss: -1.141327 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126532 INN loss: -1.126532 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139180 INN loss: -1.139180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174249 INN loss: -1.174249 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111048 INN loss: -1.111048 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153961 INN loss: -1.153961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171729 INN loss: -1.171729 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136001 INN loss: -1.136001 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098325 INN loss: -1.098325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144523 INN loss: -1.144523 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114701 INN loss: -1.114701 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149495 INN loss: -1.149495 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097518 INN loss: -1.097518 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169291 INN loss: -1.169291 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130772 INN loss: -1.130772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183474 INN loss: -1.183474 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142577 INN loss: -1.142577 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148164 INN loss: -1.148164 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140825 INN loss: -1.140825 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133657 INN loss: -1.133657 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132945 INN loss: -1.132945 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152205 INN loss: -1.152205 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120584 INN loss: -1.120584 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130672 INN loss: -1.130672 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168732 INN loss: -1.168732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138355 INN loss: -1.138355 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115967 INN loss: -1.115967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130766 INN loss: -1.130766 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173801 INN loss: -1.173801 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143793 INN loss: -1.143793 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168720 INN loss: -1.168720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093428 INN loss: -1.093428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121398 INN loss: -1.121398 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151641 INN loss: -1.151641 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129397 INN loss: -1.129397 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108587 INN loss: -1.108587 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151902 INN loss: -1.151902 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160308 INN loss: -1.160308 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131772 INN loss: -1.131772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127407 INN loss: -1.127407 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143287 INN loss: -1.143287 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156930 INN loss: -1.156930 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162915 INN loss: -1.162915 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171143 INN loss: -1.171143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130528 INN loss: -1.130528 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132670 INN loss: -1.132670 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138967 INN loss: -1.138967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161932 INN loss: -1.161932 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161962 INN loss: -1.161962 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159657 INN loss: -1.159657 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139635 INN loss: -1.139635 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166295 INN loss: -1.166295 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138256 INN loss: -1.138256 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156973 INN loss: -1.156973 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████████                                           | 13/50 [3:42:52<2:24:57, 235.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.147757 INN loss: -1.147757 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110939 INN loss: -1.110939 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124476 INN loss: -1.124476 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094039 INN loss: -1.094039 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080267 INN loss: -1.080267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147102 INN loss: -1.147102 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121936 INN loss: -1.121936 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142978 INN loss: -1.142978 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163683 INN loss: -1.163683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138862 INN loss: -1.138862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099679 INN loss: -1.099679 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164658 INN loss: -1.164658 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151818 INN loss: -1.151818 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095861 INN loss: -1.095861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123183 INN loss: -1.123183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122381 INN loss: -1.122381 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177243 INN loss: -1.177243 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153948 INN loss: -1.153948 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099209 INN loss: -1.099209 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125069 INN loss: -1.125069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101372 INN loss: -1.101372 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144213 INN loss: -1.144213 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170942 INN loss: -1.170942 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128614 INN loss: -1.128614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166681 INN loss: -1.166681 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132662 INN loss: -1.132662 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.192705 INN loss: -1.192705 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140450 INN loss: -1.140450 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142972 INN loss: -1.142972 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143536 INN loss: -1.143536 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093101 INN loss: -1.093101 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107433 INN loss: -1.107433 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121697 INN loss: -1.121697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122518 INN loss: -1.122518 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103658 INN loss: -1.103658 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147761 INN loss: -1.147761 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165134 INN loss: -1.165134 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128406 INN loss: -1.128406 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155228 INN loss: -1.155228 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141223 INN loss: -1.141223 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176477 INN loss: -1.176477 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125449 INN loss: -1.125449 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157943 INN loss: -1.157943 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165093 INN loss: -1.165093 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185288 INN loss: -1.185288 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160334 INN loss: -1.160334 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104043 INN loss: -1.104043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140780 INN loss: -1.140780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127074 INN loss: -1.127074 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095134 INN loss: -1.095134 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124175 INN loss: -1.124175 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.062471 INN loss: -1.062471 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092322 INN loss: -1.092322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146542 INN loss: -1.146542 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127785 INN loss: -1.127785 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083536 INN loss: -1.083536 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123110 INN loss: -1.123110 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080794 INN loss: -1.080794 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126778 INN loss: -1.126778 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140264 INN loss: -1.140264 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151670 INN loss: -1.151670 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133053 INN loss: -1.133053 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.039034 INN loss: -1.039034 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127009 INN loss: -1.127009 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124482 INN loss: -1.124482 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112414 INN loss: -1.112414 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106939 INN loss: -1.106939 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129855 INN loss: -1.129855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125360 INN loss: -1.125360 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152313 INN loss: -1.152313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102054 INN loss: -1.102054 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132432 INN loss: -1.132432 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.067932 INN loss: -1.067932 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099662 INN loss: -1.099662 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155905 INN loss: -1.155905 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149610 INN loss: -1.149610 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124724 INN loss: -1.124724 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132224 INN loss: -1.132224 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184623 INN loss: -1.184623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095042 INN loss: -1.095042 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113033 INN loss: -1.113033 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152619 INN loss: -1.152619 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138703 INN loss: -1.138703 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083694 INN loss: -1.083694 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166700 INN loss: -1.166700 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180706 INN loss: -1.180706 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123719 INN loss: -1.123719 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129648 INN loss: -1.129648 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117866 INN loss: -1.117866 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093532 INN loss: -1.093532 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136297 INN loss: -1.136297 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132089 INN loss: -1.132089 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153408 INN loss: -1.153408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166067 INN loss: -1.166067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178748 INN loss: -1.178748 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142302 INN loss: -1.142302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148936 INN loss: -1.148936 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103585 INN loss: -1.103585 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092126 INN loss: -1.092126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154983 INN loss: -1.154983 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094908 INN loss: -1.094908 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.073889 INN loss: -1.073889 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172361 INN loss: -1.172361 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139052 INN loss: -1.139052 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096527 INN loss: -1.096527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162800 INN loss: -1.162800 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138658 INN loss: -1.138658 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145775 INN loss: -1.145775 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125797 INN loss: -1.125797 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144360 INN loss: -1.144360 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151647 INN loss: -1.151647 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165049 INN loss: -1.165049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162560 INN loss: -1.162560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104318 INN loss: -1.104318 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132781 INN loss: -1.132781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142197 INN loss: -1.142197 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139418 INN loss: -1.139418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133608 INN loss: -1.133608 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097956 INN loss: -1.097956 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148003 INN loss: -1.148003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109806 INN loss: -1.109806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124710 INN loss: -1.124710 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168570 INN loss: -1.168570 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116322 INN loss: -1.116322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145710 INN loss: -1.145710 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148241 INN loss: -1.148241 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087572 INN loss: -1.087572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177758 INN loss: -1.177758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109114 INN loss: -1.109114 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136646 INN loss: -1.136646 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150721 INN loss: -1.150721 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121403 INN loss: -1.121403 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143908 INN loss: -1.143908 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110915 INN loss: -1.110915 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129219 INN loss: -1.129219 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165850 INN loss: -1.165850 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096354 INN loss: -1.096354 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103784 INN loss: -1.103784 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125669 INN loss: -1.125669 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182879 INN loss: -1.182879 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084622 INN loss: -1.084622 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088776 INN loss: -1.088776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165830 INN loss: -1.165830 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165943 INN loss: -1.165943 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150253 INN loss: -1.150253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127470 INN loss: -1.127470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132963 INN loss: -1.132963 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123273 INN loss: -1.123273 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169832 INN loss: -1.169832 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159473 INN loss: -1.159473 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139815 INN loss: -1.139815 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085280 INN loss: -1.085280 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155022 INN loss: -1.155022 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145674 INN loss: -1.145674 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128086 INN loss: -1.128086 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112718 INN loss: -1.112718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126798 INN loss: -1.126798 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137805 INN loss: -1.137805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144310 INN loss: -1.144310 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128137 INN loss: -1.128137 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|████████████████▏                                         | 14/50 [3:44:26<1:55:32, 192.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.172694 INN loss: -1.172694 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163805 INN loss: -1.163805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099692 INN loss: -1.099692 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141495 INN loss: -1.141495 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119047 INN loss: -1.119047 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133466 INN loss: -1.133466 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097425 INN loss: -1.097425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153008 INN loss: -1.153008 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100756 INN loss: -1.100756 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.205914 INN loss: -1.205914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108295 INN loss: -1.108295 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114806 INN loss: -1.114806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132088 INN loss: -1.132088 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139783 INN loss: -1.139783 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176977 INN loss: -1.176977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129747 INN loss: -1.129747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138552 INN loss: -1.138552 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158007 INN loss: -1.158007 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169166 INN loss: -1.169166 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140467 INN loss: -1.140467 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142891 INN loss: -1.142891 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.075011 INN loss: -1.075011 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109366 INN loss: -1.109366 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138891 INN loss: -1.138891 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135473 INN loss: -1.135473 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110413 INN loss: -1.110413 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134796 INN loss: -1.134796 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158925 INN loss: -1.158925 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132894 INN loss: -1.132894 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099752 INN loss: -1.099752 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129709 INN loss: -1.129709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096868 INN loss: -1.096868 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157228 INN loss: -1.157228 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157202 INN loss: -1.157202 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128705 INN loss: -1.128705 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162835 INN loss: -1.162835 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116189 INN loss: -1.116189 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146244 INN loss: -1.146244 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136056 INN loss: -1.136056 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126477 INN loss: -1.126477 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114794 INN loss: -1.114794 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.199407 INN loss: -1.199407 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099361 INN loss: -1.099361 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167574 INN loss: -1.167574 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168781 INN loss: -1.168781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155599 INN loss: -1.155599 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130423 INN loss: -1.130423 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145294 INN loss: -1.145294 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144429 INN loss: -1.144429 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142065 INN loss: -1.142065 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147712 INN loss: -1.147712 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131954 INN loss: -1.131954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085912 INN loss: -1.085912 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144708 INN loss: -1.144708 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109564 INN loss: -1.109564 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139888 INN loss: -1.139888 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143632 INN loss: -1.143632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123332 INN loss: -1.123332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127046 INN loss: -1.127046 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182954 INN loss: -1.182954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167578 INN loss: -1.167578 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154231 INN loss: -1.154231 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128540 INN loss: -1.128540 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143932 INN loss: -1.143932 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123233 INN loss: -1.123233 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137914 INN loss: -1.137914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137456 INN loss: -1.137456 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174726 INN loss: -1.174726 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151680 INN loss: -1.151680 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165585 INN loss: -1.165585 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094776 INN loss: -1.094776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174489 INN loss: -1.174489 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169405 INN loss: -1.169405 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136574 INN loss: -1.136574 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148427 INN loss: -1.148427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129428 INN loss: -1.129428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125069 INN loss: -1.125069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092210 INN loss: -1.092210 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139100 INN loss: -1.139100 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111556 INN loss: -1.111556 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150596 INN loss: -1.150596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141753 INN loss: -1.141753 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118027 INN loss: -1.118027 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150335 INN loss: -1.150335 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162340 INN loss: -1.162340 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108423 INN loss: -1.108423 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143138 INN loss: -1.143138 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163507 INN loss: -1.163507 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163111 INN loss: -1.163111 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144064 INN loss: -1.144064 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101942 INN loss: -1.101942 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168034 INN loss: -1.168034 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165172 INN loss: -1.165172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140524 INN loss: -1.140524 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137186 INN loss: -1.137186 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141328 INN loss: -1.141328 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126258 INN loss: -1.126258 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137485 INN loss: -1.137485 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130574 INN loss: -1.130574 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170350 INN loss: -1.170350 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155682 INN loss: -1.155682 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099771 INN loss: -1.099771 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095525 INN loss: -1.095525 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172192 INN loss: -1.172192 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171934 INN loss: -1.171934 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135588 INN loss: -1.135588 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128901 INN loss: -1.128901 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147426 INN loss: -1.147426 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138011 INN loss: -1.138011 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.071087 INN loss: -1.071087 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141316 INN loss: -1.141316 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143116 INN loss: -1.143116 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110905 INN loss: -1.110905 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093860 INN loss: -1.093860 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115968 INN loss: -1.115968 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141985 INN loss: -1.141985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157144 INN loss: -1.157144 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163581 INN loss: -1.163581 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163090 INN loss: -1.163090 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113561 INN loss: -1.113561 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128749 INN loss: -1.128749 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090401 INN loss: -1.090401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153459 INN loss: -1.153459 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131083 INN loss: -1.131083 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159126 INN loss: -1.159126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131402 INN loss: -1.131402 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116576 INN loss: -1.116576 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141068 INN loss: -1.141068 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100158 INN loss: -1.100158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140963 INN loss: -1.140963 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120122 INN loss: -1.120122 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112809 INN loss: -1.112809 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162878 INN loss: -1.162878 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104229 INN loss: -1.104229 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163547 INN loss: -1.163547 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137212 INN loss: -1.137212 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137864 INN loss: -1.137864 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111883 INN loss: -1.111883 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129238 INN loss: -1.129238 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160893 INN loss: -1.160893 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117443 INN loss: -1.117443 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143538 INN loss: -1.143538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122329 INN loss: -1.122329 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138178 INN loss: -1.138178 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112610 INN loss: -1.112610 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118423 INN loss: -1.118423 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137957 INN loss: -1.137957 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154331 INN loss: -1.154331 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144824 INN loss: -1.144824 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135401 INN loss: -1.135401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154785 INN loss: -1.154785 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156199 INN loss: -1.156199 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112518 INN loss: -1.112518 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161286 INN loss: -1.161286 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143911 INN loss: -1.143911 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117621 INN loss: -1.117621 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.064137 INN loss: -1.064137 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128946 INN loss: -1.128946 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149864 INN loss: -1.149864 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138747 INN loss: -1.138747 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████▍                                        | 15/50 [3:45:58<1:34:40, 162.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.151726 INN loss: -1.151726 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132266 INN loss: -1.132266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171904 INN loss: -1.171904 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139307 INN loss: -1.139307 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167046 INN loss: -1.167046 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149006 INN loss: -1.149006 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116675 INN loss: -1.116675 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109618 INN loss: -1.109618 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.079837 INN loss: -1.079837 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170837 INN loss: -1.170837 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172614 INN loss: -1.172614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092996 INN loss: -1.092996 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144442 INN loss: -1.144442 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106458 INN loss: -1.106458 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148693 INN loss: -1.148693 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118739 INN loss: -1.118739 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135775 INN loss: -1.135775 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140152 INN loss: -1.140152 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145274 INN loss: -1.145274 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139677 INN loss: -1.139677 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113806 INN loss: -1.113806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132957 INN loss: -1.132957 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125138 INN loss: -1.125138 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143535 INN loss: -1.143535 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156852 INN loss: -1.156852 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115250 INN loss: -1.115250 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085643 INN loss: -1.085643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131494 INN loss: -1.131494 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188683 INN loss: -1.188683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126583 INN loss: -1.126583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107069 INN loss: -1.107069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115430 INN loss: -1.115430 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.068481 INN loss: -1.068481 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120606 INN loss: -1.120606 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169923 INN loss: -1.169923 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124552 INN loss: -1.124552 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119289 INN loss: -1.119289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144092 INN loss: -1.144092 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152107 INN loss: -1.152107 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149138 INN loss: -1.149138 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135153 INN loss: -1.135153 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100152 INN loss: -1.100152 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119756 INN loss: -1.119756 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144764 INN loss: -1.144764 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175300 INN loss: -1.175300 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131130 INN loss: -1.131130 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111171 INN loss: -1.111171 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150453 INN loss: -1.150453 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124489 INN loss: -1.124489 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119248 INN loss: -1.119248 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153548 INN loss: -1.153548 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137559 INN loss: -1.137559 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113835 INN loss: -1.113835 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119745 INN loss: -1.119745 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135330 INN loss: -1.135330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177698 INN loss: -1.177698 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117610 INN loss: -1.117610 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167195 INN loss: -1.167195 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127967 INN loss: -1.127967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114073 INN loss: -1.114073 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143966 INN loss: -1.143966 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178019 INN loss: -1.178019 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136327 INN loss: -1.136327 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.078954 INN loss: -1.078954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177636 INN loss: -1.177636 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110924 INN loss: -1.110924 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137608 INN loss: -1.137608 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182444 INN loss: -1.182444 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147215 INN loss: -1.147215 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100101 INN loss: -1.100101 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.209466 INN loss: -1.209466 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143130 INN loss: -1.143130 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112245 INN loss: -1.112245 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124076 INN loss: -1.124076 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104052 INN loss: -1.104052 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130793 INN loss: -1.130793 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121574 INN loss: -1.121574 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144532 INN loss: -1.144532 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122430 INN loss: -1.122430 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130660 INN loss: -1.130660 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134608 INN loss: -1.134608 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103568 INN loss: -1.103568 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150114 INN loss: -1.150114 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138096 INN loss: -1.138096 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136102 INN loss: -1.136102 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102046 INN loss: -1.102046 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161904 INN loss: -1.161904 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137907 INN loss: -1.137907 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156183 INN loss: -1.156183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130222 INN loss: -1.130222 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127299 INN loss: -1.127299 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.191998 INN loss: -1.191998 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132013 INN loss: -1.132013 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094887 INN loss: -1.094887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.193297 INN loss: -1.193297 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180887 INN loss: -1.180887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126310 INN loss: -1.126310 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179658 INN loss: -1.179658 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138155 INN loss: -1.138155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113455 INN loss: -1.113455 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124694 INN loss: -1.124694 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182077 INN loss: -1.182077 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080713 INN loss: -1.080713 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098735 INN loss: -1.098735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119823 INN loss: -1.119823 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098105 INN loss: -1.098105 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130136 INN loss: -1.130136 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159484 INN loss: -1.159484 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129285 INN loss: -1.129285 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110011 INN loss: -1.110011 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161431 INN loss: -1.161431 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153892 INN loss: -1.153892 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140221 INN loss: -1.140221 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146911 INN loss: -1.146911 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111947 INN loss: -1.111947 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140021 INN loss: -1.140021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174703 INN loss: -1.174703 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128244 INN loss: -1.128244 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115839 INN loss: -1.115839 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146521 INN loss: -1.146521 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110252 INN loss: -1.110252 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158917 INN loss: -1.158917 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107519 INN loss: -1.107519 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143310 INN loss: -1.143310 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184390 INN loss: -1.184390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110134 INN loss: -1.110134 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131002 INN loss: -1.131002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120314 INN loss: -1.120314 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138101 INN loss: -1.138101 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111338 INN loss: -1.111338 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.197253 INN loss: -1.197253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149300 INN loss: -1.149300 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152201 INN loss: -1.152201 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127828 INN loss: -1.127828 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181706 INN loss: -1.181706 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116096 INN loss: -1.116096 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102439 INN loss: -1.102439 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153237 INN loss: -1.153237 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134922 INN loss: -1.134922 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134101 INN loss: -1.134101 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129597 INN loss: -1.129597 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149403 INN loss: -1.149403 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108669 INN loss: -1.108669 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147338 INN loss: -1.147338 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165546 INN loss: -1.165546 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131607 INN loss: -1.131607 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134184 INN loss: -1.134184 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.199235 INN loss: -1.199235 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173007 INN loss: -1.173007 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158291 INN loss: -1.158291 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154599 INN loss: -1.154599 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108551 INN loss: -1.108551 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100312 INN loss: -1.100312 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142990 INN loss: -1.142990 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111093 INN loss: -1.111093 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129832 INN loss: -1.129832 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144667 INN loss: -1.144667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137373 INN loss: -1.137373 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098357 INN loss: -1.098357 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124955 INN loss: -1.124955 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████▌                                       | 16/50 [3:47:34<1:20:39, 142.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.120996 INN loss: -1.120996 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.191369 INN loss: -1.191369 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137040 INN loss: -1.137040 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127805 INN loss: -1.127805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142655 INN loss: -1.142655 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116683 INN loss: -1.116683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134810 INN loss: -1.134810 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076316 INN loss: -1.076316 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138204 INN loss: -1.138204 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120741 INN loss: -1.120741 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144000 INN loss: -1.144000 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101104 INN loss: -1.101104 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176631 INN loss: -1.176631 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114153 INN loss: -1.114153 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148470 INN loss: -1.148470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184068 INN loss: -1.184068 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132443 INN loss: -1.132443 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.072436 INN loss: -1.072436 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190326 INN loss: -1.190326 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144475 INN loss: -1.144475 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129048 INN loss: -1.129048 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141975 INN loss: -1.141975 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094253 INN loss: -1.094253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154205 INN loss: -1.154205 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131230 INN loss: -1.131230 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130447 INN loss: -1.130447 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.073340 INN loss: -1.073340 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163203 INN loss: -1.163203 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149038 INN loss: -1.149038 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144430 INN loss: -1.144430 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170508 INN loss: -1.170508 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090155 INN loss: -1.090155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138612 INN loss: -1.138612 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127843 INN loss: -1.127843 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163629 INN loss: -1.163629 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142549 INN loss: -1.142549 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131327 INN loss: -1.131327 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120624 INN loss: -1.120624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161805 INN loss: -1.161805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091143 INN loss: -1.091143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139914 INN loss: -1.139914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148432 INN loss: -1.148432 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136920 INN loss: -1.136920 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128184 INN loss: -1.128184 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100141 INN loss: -1.100141 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140165 INN loss: -1.140165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105307 INN loss: -1.105307 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088349 INN loss: -1.088349 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110571 INN loss: -1.110571 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135891 INN loss: -1.135891 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138248 INN loss: -1.138248 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144715 INN loss: -1.144715 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133940 INN loss: -1.133940 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096161 INN loss: -1.096161 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141232 INN loss: -1.141232 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168272 INN loss: -1.168272 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146018 INN loss: -1.146018 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181958 INN loss: -1.181958 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172186 INN loss: -1.172186 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142142 INN loss: -1.142142 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109982 INN loss: -1.109982 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174551 INN loss: -1.174551 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134740 INN loss: -1.134740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124093 INN loss: -1.124093 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139245 INN loss: -1.139245 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111843 INN loss: -1.111843 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162724 INN loss: -1.162724 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140913 INN loss: -1.140913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126519 INN loss: -1.126519 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162916 INN loss: -1.162916 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129345 INN loss: -1.129345 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125556 INN loss: -1.125556 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132768 INN loss: -1.132768 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188067 INN loss: -1.188067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103023 INN loss: -1.103023 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127325 INN loss: -1.127325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129935 INN loss: -1.129935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145303 INN loss: -1.145303 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135418 INN loss: -1.135418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100400 INN loss: -1.100400 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176585 INN loss: -1.176585 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159929 INN loss: -1.159929 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.198488 INN loss: -1.198488 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156624 INN loss: -1.156624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106104 INN loss: -1.106104 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133116 INN loss: -1.133116 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133580 INN loss: -1.133580 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118708 INN loss: -1.118708 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147501 INN loss: -1.147501 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154362 INN loss: -1.154362 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142339 INN loss: -1.142339 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107093 INN loss: -1.107093 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140290 INN loss: -1.140290 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170828 INN loss: -1.170828 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132718 INN loss: -1.132718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134695 INN loss: -1.134695 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.074252 INN loss: -1.074252 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153749 INN loss: -1.153749 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151504 INN loss: -1.151504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117723 INN loss: -1.117723 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134884 INN loss: -1.134884 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173699 INN loss: -1.173699 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097693 INN loss: -1.097693 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122502 INN loss: -1.122502 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118647 INN loss: -1.118647 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088884 INN loss: -1.088884 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186457 INN loss: -1.186457 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132721 INN loss: -1.132721 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108447 INN loss: -1.108447 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112232 INN loss: -1.112232 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132685 INN loss: -1.132685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157190 INN loss: -1.157190 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177981 INN loss: -1.177981 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167512 INN loss: -1.167512 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138927 INN loss: -1.138927 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151221 INN loss: -1.151221 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124600 INN loss: -1.124600 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159957 INN loss: -1.159957 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171432 INN loss: -1.171432 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108998 INN loss: -1.108998 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174497 INN loss: -1.174497 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147865 INN loss: -1.147865 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124503 INN loss: -1.124503 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155371 INN loss: -1.155371 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124474 INN loss: -1.124474 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128157 INN loss: -1.128157 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165805 INN loss: -1.165805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116983 INN loss: -1.116983 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146557 INN loss: -1.146557 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083696 INN loss: -1.083696 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162683 INN loss: -1.162683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178404 INN loss: -1.178404 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158654 INN loss: -1.158654 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165450 INN loss: -1.165450 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125399 INN loss: -1.125399 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135287 INN loss: -1.135287 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146447 INN loss: -1.146447 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.075103 INN loss: -1.075103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164588 INN loss: -1.164588 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.219676 INN loss: -1.219676 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102724 INN loss: -1.102724 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175743 INN loss: -1.175743 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186066 INN loss: -1.186066 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134443 INN loss: -1.134443 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164043 INN loss: -1.164043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105971 INN loss: -1.105971 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130194 INN loss: -1.130194 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155183 INN loss: -1.155183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146063 INN loss: -1.146063 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101525 INN loss: -1.101525 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099590 INN loss: -1.099590 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150552 INN loss: -1.150552 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148742 INN loss: -1.148742 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156930 INN loss: -1.156930 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093022 INN loss: -1.093022 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094232 INN loss: -1.094232 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143355 INN loss: -1.143355 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175922 INN loss: -1.175922 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119902 INN loss: -1.119902 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.073920 INN loss: -1.073920 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████▋                                      | 17/50 [3:49:10<1:10:30, 128.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.125405 INN loss: -1.125405 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129467 INN loss: -1.129467 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128521 INN loss: -1.128521 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096543 INN loss: -1.096543 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158500 INN loss: -1.158500 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141317 INN loss: -1.141317 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.065570 INN loss: -1.065570 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149138 INN loss: -1.149138 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158448 INN loss: -1.158448 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151977 INN loss: -1.151977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167866 INN loss: -1.167866 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128928 INN loss: -1.128928 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107489 INN loss: -1.107489 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139695 INN loss: -1.139695 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143481 INN loss: -1.143481 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167001 INN loss: -1.167001 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159868 INN loss: -1.159868 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145955 INN loss: -1.145955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176905 INN loss: -1.176905 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100502 INN loss: -1.100502 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142004 INN loss: -1.142004 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164195 INN loss: -1.164195 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142179 INN loss: -1.142179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115480 INN loss: -1.115480 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.207773 INN loss: -1.207773 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149749 INN loss: -1.149749 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142374 INN loss: -1.142374 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156663 INN loss: -1.156663 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133847 INN loss: -1.133847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.073287 INN loss: -1.073287 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119264 INN loss: -1.119264 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137667 INN loss: -1.137667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128180 INN loss: -1.128180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134578 INN loss: -1.134578 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167806 INN loss: -1.167806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163850 INN loss: -1.163850 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119281 INN loss: -1.119281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162000 INN loss: -1.162000 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173091 INN loss: -1.173091 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124302 INN loss: -1.124302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096998 INN loss: -1.096998 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118823 INN loss: -1.118823 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154862 INN loss: -1.154862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123744 INN loss: -1.123744 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185983 INN loss: -1.185983 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128979 INN loss: -1.128979 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082861 INN loss: -1.082861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169915 INN loss: -1.169915 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128056 INN loss: -1.128056 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152857 INN loss: -1.152857 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136953 INN loss: -1.136953 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154160 INN loss: -1.154160 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159865 INN loss: -1.159865 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130920 INN loss: -1.130920 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145417 INN loss: -1.145417 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144772 INN loss: -1.144772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158322 INN loss: -1.158322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182362 INN loss: -1.182362 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129972 INN loss: -1.129972 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141789 INN loss: -1.141789 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134404 INN loss: -1.134404 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129385 INN loss: -1.129385 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114429 INN loss: -1.114429 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162372 INN loss: -1.162372 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166105 INN loss: -1.166105 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180632 INN loss: -1.180632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116417 INN loss: -1.116417 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158734 INN loss: -1.158734 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120922 INN loss: -1.120922 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157158 INN loss: -1.157158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122248 INN loss: -1.122248 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099223 INN loss: -1.099223 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098568 INN loss: -1.098568 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121862 INN loss: -1.121862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117797 INN loss: -1.117797 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155541 INN loss: -1.155541 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080948 INN loss: -1.080948 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147498 INN loss: -1.147498 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165083 INN loss: -1.165083 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147973 INN loss: -1.147973 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136835 INN loss: -1.136835 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139691 INN loss: -1.139691 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115083 INN loss: -1.115083 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115039 INN loss: -1.115039 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140287 INN loss: -1.140287 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087843 INN loss: -1.087843 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116998 INN loss: -1.116998 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094214 INN loss: -1.094214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169527 INN loss: -1.169527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107185 INN loss: -1.107185 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169040 INN loss: -1.169040 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104173 INN loss: -1.104173 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117506 INN loss: -1.117506 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139042 INN loss: -1.139042 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.066672 INN loss: -1.066672 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110460 INN loss: -1.110460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141765 INN loss: -1.141765 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.202646 INN loss: -1.202646 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149348 INN loss: -1.149348 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164711 INN loss: -1.164711 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143904 INN loss: -1.143904 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178488 INN loss: -1.178488 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135119 INN loss: -1.135119 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102499 INN loss: -1.102499 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106739 INN loss: -1.106739 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127034 INN loss: -1.127034 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114605 INN loss: -1.114605 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117554 INN loss: -1.117554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107884 INN loss: -1.107884 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095814 INN loss: -1.095814 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096056 INN loss: -1.096056 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165443 INN loss: -1.165443 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146830 INN loss: -1.146830 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122545 INN loss: -1.122545 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126923 INN loss: -1.126923 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155849 INN loss: -1.155849 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128132 INN loss: -1.128132 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120959 INN loss: -1.120959 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124102 INN loss: -1.124102 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119395 INN loss: -1.119395 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161432 INN loss: -1.161432 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133186 INN loss: -1.133186 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123232 INN loss: -1.123232 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155511 INN loss: -1.155511 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164153 INN loss: -1.164153 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176673 INN loss: -1.176673 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151780 INN loss: -1.151780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143228 INN loss: -1.143228 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.072062 INN loss: -1.072062 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155224 INN loss: -1.155224 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183867 INN loss: -1.183867 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154435 INN loss: -1.154435 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118886 INN loss: -1.118886 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085303 INN loss: -1.085303 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126634 INN loss: -1.126634 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126509 INN loss: -1.126509 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146893 INN loss: -1.146893 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127375 INN loss: -1.127375 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.193398 INN loss: -1.193398 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126908 INN loss: -1.126908 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082727 INN loss: -1.082727 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108887 INN loss: -1.108887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087780 INN loss: -1.087780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142652 INN loss: -1.142652 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160659 INN loss: -1.160659 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111839 INN loss: -1.111839 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098245 INN loss: -1.098245 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116570 INN loss: -1.116570 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087135 INN loss: -1.087135 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160131 INN loss: -1.160131 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114931 INN loss: -1.114931 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114944 INN loss: -1.114944 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124464 INN loss: -1.124464 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112709 INN loss: -1.112709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135991 INN loss: -1.135991 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114213 INN loss: -1.114213 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164746 INN loss: -1.164746 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121666 INN loss: -1.121666 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145293 INN loss: -1.145293 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131146 INN loss: -1.131146 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████▉                                     | 18/50 [3:50:46<1:03:16, 118.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.119556 INN loss: -1.119556 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110792 INN loss: -1.110792 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178974 INN loss: -1.178974 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124698 INN loss: -1.124698 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139023 INN loss: -1.139023 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174505 INN loss: -1.174505 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169454 INN loss: -1.169454 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120649 INN loss: -1.120649 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146722 INN loss: -1.146722 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132997 INN loss: -1.132997 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107775 INN loss: -1.107775 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120137 INN loss: -1.120137 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164845 INN loss: -1.164845 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155110 INN loss: -1.155110 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136120 INN loss: -1.136120 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140487 INN loss: -1.140487 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168452 INN loss: -1.168452 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164260 INN loss: -1.164260 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167842 INN loss: -1.167842 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143004 INN loss: -1.143004 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155070 INN loss: -1.155070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147027 INN loss: -1.147027 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180370 INN loss: -1.180370 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146534 INN loss: -1.146534 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157334 INN loss: -1.157334 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169978 INN loss: -1.169978 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158098 INN loss: -1.158098 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170156 INN loss: -1.170156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115533 INN loss: -1.115533 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119965 INN loss: -1.119965 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160661 INN loss: -1.160661 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102972 INN loss: -1.102972 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100824 INN loss: -1.100824 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139154 INN loss: -1.139154 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.075843 INN loss: -1.075843 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155547 INN loss: -1.155547 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132518 INN loss: -1.132518 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157237 INN loss: -1.157237 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133004 INN loss: -1.133004 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133596 INN loss: -1.133596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167858 INN loss: -1.167858 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157435 INN loss: -1.157435 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088599 INN loss: -1.088599 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128139 INN loss: -1.128139 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127535 INN loss: -1.127535 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124992 INN loss: -1.124992 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128149 INN loss: -1.128149 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122254 INN loss: -1.122254 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154881 INN loss: -1.154881 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144969 INN loss: -1.144969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125214 INN loss: -1.125214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097378 INN loss: -1.097378 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155447 INN loss: -1.155447 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129009 INN loss: -1.129009 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126336 INN loss: -1.126336 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167810 INN loss: -1.167810 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151666 INN loss: -1.151666 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150776 INN loss: -1.150776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178742 INN loss: -1.178742 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124352 INN loss: -1.124352 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111117 INN loss: -1.111117 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111234 INN loss: -1.111234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127842 INN loss: -1.127842 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127280 INN loss: -1.127280 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136483 INN loss: -1.136483 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156325 INN loss: -1.156325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151337 INN loss: -1.151337 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160576 INN loss: -1.160576 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104644 INN loss: -1.104644 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147464 INN loss: -1.147464 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119906 INN loss: -1.119906 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140906 INN loss: -1.140906 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166051 INN loss: -1.166051 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133301 INN loss: -1.133301 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136632 INN loss: -1.136632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164211 INN loss: -1.164211 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174157 INN loss: -1.174157 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114067 INN loss: -1.114067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139391 INN loss: -1.139391 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157041 INN loss: -1.157041 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160912 INN loss: -1.160912 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158610 INN loss: -1.158610 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144258 INN loss: -1.144258 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156103 INN loss: -1.156103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146386 INN loss: -1.146386 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129510 INN loss: -1.129510 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123306 INN loss: -1.123306 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166961 INN loss: -1.166961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126620 INN loss: -1.126620 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131758 INN loss: -1.131758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093177 INN loss: -1.093177 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098419 INN loss: -1.098419 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097133 INN loss: -1.097133 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115828 INN loss: -1.115828 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141117 INN loss: -1.141117 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150213 INN loss: -1.150213 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130501 INN loss: -1.130501 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166071 INN loss: -1.166071 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171000 INN loss: -1.171000 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.196998 INN loss: -1.196998 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130824 INN loss: -1.130824 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093086 INN loss: -1.093086 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167238 INN loss: -1.167238 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167137 INN loss: -1.167137 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107403 INN loss: -1.107403 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135891 INN loss: -1.135891 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117279 INN loss: -1.117279 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161387 INN loss: -1.161387 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134056 INN loss: -1.134056 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130372 INN loss: -1.130372 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164169 INN loss: -1.164169 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114021 INN loss: -1.114021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161143 INN loss: -1.161143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090862 INN loss: -1.090862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172338 INN loss: -1.172338 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150332 INN loss: -1.150332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142253 INN loss: -1.142253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141897 INN loss: -1.141897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137049 INN loss: -1.137049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121459 INN loss: -1.121459 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123670 INN loss: -1.123670 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175710 INN loss: -1.175710 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164659 INN loss: -1.164659 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143848 INN loss: -1.143848 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124352 INN loss: -1.124352 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122671 INN loss: -1.122671 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149592 INN loss: -1.149592 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137206 INN loss: -1.137206 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102666 INN loss: -1.102666 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134357 INN loss: -1.134357 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080738 INN loss: -1.080738 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156731 INN loss: -1.156731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117911 INN loss: -1.117911 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153617 INN loss: -1.153617 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149545 INN loss: -1.149545 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116289 INN loss: -1.116289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155845 INN loss: -1.155845 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177575 INN loss: -1.177575 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099423 INN loss: -1.099423 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117502 INN loss: -1.117502 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185179 INN loss: -1.185179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126480 INN loss: -1.126480 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186949 INN loss: -1.186949 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129759 INN loss: -1.129759 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094339 INN loss: -1.094339 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183670 INN loss: -1.183670 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124293 INN loss: -1.124293 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177151 INN loss: -1.177151 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119236 INN loss: -1.119236 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137718 INN loss: -1.137718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151587 INN loss: -1.151587 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126562 INN loss: -1.126562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169450 INN loss: -1.169450 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125282 INN loss: -1.125282 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146569 INN loss: -1.146569 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136049 INN loss: -1.136049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081045 INN loss: -1.081045 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118319 INN loss: -1.118319 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108040 INN loss: -1.108040 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173999 INN loss: -1.173999 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████████████▊                                     | 19/50 [3:52:21<57:40, 111.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.124876 INN loss: -1.124876 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146819 INN loss: -1.146819 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125336 INN loss: -1.125336 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126677 INN loss: -1.126677 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156665 INN loss: -1.156665 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161409 INN loss: -1.161409 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138975 INN loss: -1.138975 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107225 INN loss: -1.107225 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132235 INN loss: -1.132235 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124220 INN loss: -1.124220 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124978 INN loss: -1.124978 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138937 INN loss: -1.138937 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131622 INN loss: -1.131622 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084652 INN loss: -1.084652 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159161 INN loss: -1.159161 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163146 INN loss: -1.163146 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163295 INN loss: -1.163295 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091897 INN loss: -1.091897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103752 INN loss: -1.103752 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149007 INN loss: -1.149007 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114457 INN loss: -1.114457 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133765 INN loss: -1.133765 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113324 INN loss: -1.113324 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146790 INN loss: -1.146790 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172767 INN loss: -1.172767 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181117 INN loss: -1.181117 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167476 INN loss: -1.167476 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121665 INN loss: -1.121665 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172458 INN loss: -1.172458 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097089 INN loss: -1.097089 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184554 INN loss: -1.184554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153276 INN loss: -1.153276 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149013 INN loss: -1.149013 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165180 INN loss: -1.165180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127746 INN loss: -1.127746 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136940 INN loss: -1.136940 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163718 INN loss: -1.163718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140758 INN loss: -1.140758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152480 INN loss: -1.152480 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157216 INN loss: -1.157216 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139270 INN loss: -1.139270 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125499 INN loss: -1.125499 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167957 INN loss: -1.167957 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112227 INN loss: -1.112227 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121376 INN loss: -1.121376 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113428 INN loss: -1.113428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109004 INN loss: -1.109004 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094770 INN loss: -1.094770 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117330 INN loss: -1.117330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141649 INN loss: -1.141649 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102722 INN loss: -1.102722 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146864 INN loss: -1.146864 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142768 INN loss: -1.142768 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168919 INN loss: -1.168919 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145732 INN loss: -1.145732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101481 INN loss: -1.101481 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152535 INN loss: -1.152535 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141868 INN loss: -1.141868 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183234 INN loss: -1.183234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122096 INN loss: -1.122096 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136550 INN loss: -1.136550 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115772 INN loss: -1.115772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156910 INN loss: -1.156910 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171364 INN loss: -1.171364 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173855 INN loss: -1.173855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085188 INN loss: -1.085188 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134251 INN loss: -1.134251 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175932 INN loss: -1.175932 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084095 INN loss: -1.084095 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142209 INN loss: -1.142209 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120334 INN loss: -1.120334 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167848 INN loss: -1.167848 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098869 INN loss: -1.098869 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099394 INN loss: -1.099394 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103251 INN loss: -1.103251 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142226 INN loss: -1.142226 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099520 INN loss: -1.099520 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143355 INN loss: -1.143355 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136978 INN loss: -1.136978 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112073 INN loss: -1.112073 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130176 INN loss: -1.130176 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138027 INN loss: -1.138027 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136360 INN loss: -1.136360 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094175 INN loss: -1.094175 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128609 INN loss: -1.128609 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168940 INN loss: -1.168940 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144125 INN loss: -1.144125 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106766 INN loss: -1.106766 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155251 INN loss: -1.155251 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158919 INN loss: -1.158919 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114467 INN loss: -1.114467 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123658 INN loss: -1.123658 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134950 INN loss: -1.134950 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122728 INN loss: -1.122728 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127983 INN loss: -1.127983 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176683 INN loss: -1.176683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131794 INN loss: -1.131794 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118284 INN loss: -1.118284 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112722 INN loss: -1.112722 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147097 INN loss: -1.147097 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116903 INN loss: -1.116903 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166693 INN loss: -1.166693 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142056 INN loss: -1.142056 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117155 INN loss: -1.117155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127728 INN loss: -1.127728 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092688 INN loss: -1.092688 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132787 INN loss: -1.132787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161016 INN loss: -1.161016 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149146 INN loss: -1.149146 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150768 INN loss: -1.150768 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099406 INN loss: -1.099406 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148573 INN loss: -1.148573 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119208 INN loss: -1.119208 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093504 INN loss: -1.093504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120350 INN loss: -1.120350 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115040 INN loss: -1.115040 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125646 INN loss: -1.125646 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122937 INN loss: -1.122937 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166592 INN loss: -1.166592 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112494 INN loss: -1.112494 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181928 INN loss: -1.181928 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098766 INN loss: -1.098766 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161740 INN loss: -1.161740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145645 INN loss: -1.145645 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166129 INN loss: -1.166129 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128133 INN loss: -1.128133 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185187 INN loss: -1.185187 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145795 INN loss: -1.145795 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.061023 INN loss: -1.061023 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138901 INN loss: -1.138901 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146397 INN loss: -1.146397 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157442 INN loss: -1.157442 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132861 INN loss: -1.132861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185197 INN loss: -1.185197 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175179 INN loss: -1.175179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119533 INN loss: -1.119533 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155269 INN loss: -1.155269 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158092 INN loss: -1.158092 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133438 INN loss: -1.133438 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131208 INN loss: -1.131208 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.055601 INN loss: -1.055601 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.205945 INN loss: -1.205945 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107520 INN loss: -1.107520 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127577 INN loss: -1.127577 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091498 INN loss: -1.091498 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168121 INN loss: -1.168121 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134408 INN loss: -1.134408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135451 INN loss: -1.135451 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096212 INN loss: -1.096212 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149756 INN loss: -1.149756 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168386 INN loss: -1.168386 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148702 INN loss: -1.148702 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158374 INN loss: -1.158374 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161374 INN loss: -1.161374 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.212635 INN loss: -1.212635 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131816 INN loss: -1.131816 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127850 INN loss: -1.127850 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131505 INN loss: -1.131505 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127060 INN loss: -1.127060 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118604 INN loss: -1.118604 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████                                    | 20/50 [3:53:55<53:08, 106.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.130651 INN loss: -1.130651 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144008 INN loss: -1.144008 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101422 INN loss: -1.101422 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103779 INN loss: -1.103779 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131219 INN loss: -1.131219 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174112 INN loss: -1.174112 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153152 INN loss: -1.153152 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139512 INN loss: -1.139512 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172918 INN loss: -1.172918 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110818 INN loss: -1.110818 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143137 INN loss: -1.143137 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164910 INN loss: -1.164910 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113776 INN loss: -1.113776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140825 INN loss: -1.140825 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138704 INN loss: -1.138704 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116754 INN loss: -1.116754 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125149 INN loss: -1.125149 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108624 INN loss: -1.108624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140147 INN loss: -1.140147 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137879 INN loss: -1.137879 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137508 INN loss: -1.137508 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178285 INN loss: -1.178285 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143560 INN loss: -1.143560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161313 INN loss: -1.161313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113984 INN loss: -1.113984 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182498 INN loss: -1.182498 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139445 INN loss: -1.139445 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152810 INN loss: -1.152810 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114343 INN loss: -1.114343 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125509 INN loss: -1.125509 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144572 INN loss: -1.144572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148287 INN loss: -1.148287 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164230 INN loss: -1.164230 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127359 INN loss: -1.127359 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141976 INN loss: -1.141976 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118778 INN loss: -1.118778 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120099 INN loss: -1.120099 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124949 INN loss: -1.124949 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144810 INN loss: -1.144810 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118719 INN loss: -1.118719 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143401 INN loss: -1.143401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151903 INN loss: -1.151903 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106261 INN loss: -1.106261 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120124 INN loss: -1.120124 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144693 INN loss: -1.144693 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135169 INN loss: -1.135169 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108732 INN loss: -1.108732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165309 INN loss: -1.165309 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091874 INN loss: -1.091874 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125345 INN loss: -1.125345 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134432 INN loss: -1.134432 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127293 INN loss: -1.127293 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187647 INN loss: -1.187647 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084651 INN loss: -1.084651 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140354 INN loss: -1.140354 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141728 INN loss: -1.141728 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108397 INN loss: -1.108397 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172123 INN loss: -1.172123 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143335 INN loss: -1.143335 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156392 INN loss: -1.156392 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136770 INN loss: -1.136770 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114347 INN loss: -1.114347 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176281 INN loss: -1.176281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152685 INN loss: -1.152685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139693 INN loss: -1.139693 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167077 INN loss: -1.167077 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141678 INN loss: -1.141678 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093240 INN loss: -1.093240 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120001 INN loss: -1.120001 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164600 INN loss: -1.164600 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163117 INN loss: -1.163117 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095890 INN loss: -1.095890 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153979 INN loss: -1.153979 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112612 INN loss: -1.112612 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163811 INN loss: -1.163811 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101829 INN loss: -1.101829 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159793 INN loss: -1.159793 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173619 INN loss: -1.173619 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129648 INN loss: -1.129648 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168279 INN loss: -1.168279 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181362 INN loss: -1.181362 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097036 INN loss: -1.097036 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158499 INN loss: -1.158499 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157035 INN loss: -1.157035 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149805 INN loss: -1.149805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143972 INN loss: -1.143972 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144905 INN loss: -1.144905 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128264 INN loss: -1.128264 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157265 INN loss: -1.157265 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162336 INN loss: -1.162336 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186018 INN loss: -1.186018 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138172 INN loss: -1.138172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137316 INN loss: -1.137316 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120739 INN loss: -1.120739 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125614 INN loss: -1.125614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143452 INN loss: -1.143452 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101004 INN loss: -1.101004 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111688 INN loss: -1.111688 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133127 INN loss: -1.133127 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122566 INN loss: -1.122566 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131157 INN loss: -1.131157 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153067 INN loss: -1.153067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123712 INN loss: -1.123712 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091261 INN loss: -1.091261 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144701 INN loss: -1.144701 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120896 INN loss: -1.120896 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162530 INN loss: -1.162530 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102921 INN loss: -1.102921 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173179 INN loss: -1.173179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154521 INN loss: -1.154521 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153636 INN loss: -1.153636 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139682 INN loss: -1.139682 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112009 INN loss: -1.112009 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088115 INN loss: -1.088115 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150915 INN loss: -1.150915 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124532 INN loss: -1.124532 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163416 INN loss: -1.163416 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168409 INN loss: -1.168409 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139223 INN loss: -1.139223 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105779 INN loss: -1.105779 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.191924 INN loss: -1.191924 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124294 INN loss: -1.124294 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149849 INN loss: -1.149849 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103012 INN loss: -1.103012 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121751 INN loss: -1.121751 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130344 INN loss: -1.130344 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125302 INN loss: -1.125302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081583 INN loss: -1.081583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177905 INN loss: -1.177905 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169912 INN loss: -1.169912 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146087 INN loss: -1.146087 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128677 INN loss: -1.128677 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137623 INN loss: -1.137623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123402 INN loss: -1.123402 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137697 INN loss: -1.137697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114254 INN loss: -1.114254 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155818 INN loss: -1.155818 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117572 INN loss: -1.117572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125067 INN loss: -1.125067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180121 INN loss: -1.180121 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100125 INN loss: -1.100125 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114040 INN loss: -1.114040 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137862 INN loss: -1.137862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134717 INN loss: -1.134717 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165189 INN loss: -1.165189 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106650 INN loss: -1.106650 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179526 INN loss: -1.179526 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100101 INN loss: -1.100101 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109466 INN loss: -1.109466 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098560 INN loss: -1.098560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139191 INN loss: -1.139191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138387 INN loss: -1.138387 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105066 INN loss: -1.105066 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174645 INN loss: -1.174645 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156376 INN loss: -1.156376 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114060 INN loss: -1.114060 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141775 INN loss: -1.141775 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159092 INN loss: -1.159092 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141868 INN loss: -1.141868 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144605 INN loss: -1.144605 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████▏                                  | 21/50 [3:55:32<49:56, 103.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.144350 INN loss: -1.144350 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130033 INN loss: -1.130033 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165564 INN loss: -1.165564 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150694 INN loss: -1.150694 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105142 INN loss: -1.105142 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138001 INN loss: -1.138001 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147475 INN loss: -1.147475 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168071 INN loss: -1.168071 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127376 INN loss: -1.127376 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138734 INN loss: -1.138734 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128383 INN loss: -1.128383 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098820 INN loss: -1.098820 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102376 INN loss: -1.102376 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157315 INN loss: -1.157315 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135322 INN loss: -1.135322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142601 INN loss: -1.142601 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115777 INN loss: -1.115777 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087923 INN loss: -1.087923 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121681 INN loss: -1.121681 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188652 INN loss: -1.188652 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124744 INN loss: -1.124744 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133584 INN loss: -1.133584 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117633 INN loss: -1.117633 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173012 INN loss: -1.173012 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106493 INN loss: -1.106493 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084853 INN loss: -1.084853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148891 INN loss: -1.148891 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173714 INN loss: -1.173714 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131473 INN loss: -1.131473 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188897 INN loss: -1.188897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147852 INN loss: -1.147852 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153728 INN loss: -1.153728 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135199 INN loss: -1.135199 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131176 INN loss: -1.131176 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126771 INN loss: -1.126771 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135661 INN loss: -1.135661 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081961 INN loss: -1.081961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138679 INN loss: -1.138679 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109408 INN loss: -1.109408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102876 INN loss: -1.102876 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160692 INN loss: -1.160692 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158440 INN loss: -1.158440 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131230 INN loss: -1.131230 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118822 INN loss: -1.118822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132950 INN loss: -1.132950 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.202696 INN loss: -1.202696 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.086958 INN loss: -1.086958 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186281 INN loss: -1.186281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135449 INN loss: -1.135449 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166741 INN loss: -1.166741 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121439 INN loss: -1.121439 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169228 INN loss: -1.169228 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085212 INN loss: -1.085212 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120245 INN loss: -1.120245 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134596 INN loss: -1.134596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141885 INN loss: -1.141885 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101146 INN loss: -1.101146 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155032 INN loss: -1.155032 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176186 INN loss: -1.176186 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114174 INN loss: -1.114174 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110870 INN loss: -1.110870 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140030 INN loss: -1.140030 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126512 INN loss: -1.126512 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140948 INN loss: -1.140948 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104782 INN loss: -1.104782 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157455 INN loss: -1.157455 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141098 INN loss: -1.141098 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148937 INN loss: -1.148937 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104090 INN loss: -1.104090 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.086914 INN loss: -1.086914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152898 INN loss: -1.152898 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094895 INN loss: -1.094895 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114076 INN loss: -1.114076 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153214 INN loss: -1.153214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171187 INN loss: -1.171187 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170293 INN loss: -1.170293 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133847 INN loss: -1.133847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134991 INN loss: -1.134991 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152044 INN loss: -1.152044 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146239 INN loss: -1.146239 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150107 INN loss: -1.150107 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106619 INN loss: -1.106619 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145066 INN loss: -1.145066 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165279 INN loss: -1.165279 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165509 INN loss: -1.165509 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130085 INN loss: -1.130085 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138504 INN loss: -1.138504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163786 INN loss: -1.163786 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146296 INN loss: -1.146296 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173427 INN loss: -1.173427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106514 INN loss: -1.106514 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.065169 INN loss: -1.065169 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124667 INN loss: -1.124667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.193735 INN loss: -1.193735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140935 INN loss: -1.140935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134632 INN loss: -1.134632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161479 INN loss: -1.161479 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131220 INN loss: -1.131220 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128041 INN loss: -1.128041 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148624 INN loss: -1.148624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133218 INN loss: -1.133218 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151188 INN loss: -1.151188 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116582 INN loss: -1.116582 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095501 INN loss: -1.095501 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190072 INN loss: -1.190072 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163382 INN loss: -1.163382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153330 INN loss: -1.153330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160390 INN loss: -1.160390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141288 INN loss: -1.141288 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120904 INN loss: -1.120904 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148177 INN loss: -1.148177 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126105 INN loss: -1.126105 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121488 INN loss: -1.121488 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107944 INN loss: -1.107944 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179572 INN loss: -1.179572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144273 INN loss: -1.144273 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156912 INN loss: -1.156912 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153575 INN loss: -1.153575 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178846 INN loss: -1.178846 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109250 INN loss: -1.109250 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122970 INN loss: -1.122970 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155964 INN loss: -1.155964 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158318 INN loss: -1.158318 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128047 INN loss: -1.128047 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089542 INN loss: -1.089542 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153779 INN loss: -1.153779 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171087 INN loss: -1.171087 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119566 INN loss: -1.119566 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152183 INN loss: -1.152183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114263 INN loss: -1.114263 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130962 INN loss: -1.130962 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091889 INN loss: -1.091889 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120048 INN loss: -1.120048 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159010 INN loss: -1.159010 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095361 INN loss: -1.095361 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141710 INN loss: -1.141710 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139674 INN loss: -1.139674 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132554 INN loss: -1.132554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156613 INN loss: -1.156613 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152170 INN loss: -1.152170 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132598 INN loss: -1.132598 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171535 INN loss: -1.171535 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149775 INN loss: -1.149775 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143273 INN loss: -1.143273 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156781 INN loss: -1.156781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082625 INN loss: -1.082625 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172373 INN loss: -1.172373 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.194220 INN loss: -1.194220 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106287 INN loss: -1.106287 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114995 INN loss: -1.114995 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124588 INN loss: -1.124588 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142324 INN loss: -1.142324 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138284 INN loss: -1.138284 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177665 INN loss: -1.177665 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156974 INN loss: -1.156974 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117068 INN loss: -1.117068 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094489 INN loss: -1.094489 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129998 INN loss: -1.129998 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158636 INN loss: -1.158636 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146391 INN loss: -1.146391 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▍                                 | 22/50 [3:57:12<47:46, 102.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.149706 INN loss: -1.149706 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133947 INN loss: -1.133947 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183645 INN loss: -1.183645 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081483 INN loss: -1.081483 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148885 INN loss: -1.148885 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175155 INN loss: -1.175155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144087 INN loss: -1.144087 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102735 INN loss: -1.102735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143396 INN loss: -1.143396 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176077 INN loss: -1.176077 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150106 INN loss: -1.150106 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133628 INN loss: -1.133628 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125638 INN loss: -1.125638 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147428 INN loss: -1.147428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153799 INN loss: -1.153799 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155248 INN loss: -1.155248 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103654 INN loss: -1.103654 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118643 INN loss: -1.118643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089129 INN loss: -1.089129 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175126 INN loss: -1.175126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098913 INN loss: -1.098913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145861 INN loss: -1.145861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145348 INN loss: -1.145348 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142386 INN loss: -1.142386 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146438 INN loss: -1.146438 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138481 INN loss: -1.138481 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170740 INN loss: -1.170740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130713 INN loss: -1.130713 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146961 INN loss: -1.146961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.198349 INN loss: -1.198349 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133779 INN loss: -1.133779 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140502 INN loss: -1.140502 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139932 INN loss: -1.139932 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184001 INN loss: -1.184001 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142255 INN loss: -1.142255 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150291 INN loss: -1.150291 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110047 INN loss: -1.110047 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103835 INN loss: -1.103835 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108357 INN loss: -1.108357 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168280 INN loss: -1.168280 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117992 INN loss: -1.117992 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120797 INN loss: -1.120797 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130070 INN loss: -1.130070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135630 INN loss: -1.135630 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131312 INN loss: -1.131312 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141349 INN loss: -1.141349 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133633 INN loss: -1.133633 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159036 INN loss: -1.159036 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128707 INN loss: -1.128707 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104935 INN loss: -1.104935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176070 INN loss: -1.176070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121351 INN loss: -1.121351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105537 INN loss: -1.105537 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104347 INN loss: -1.104347 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131988 INN loss: -1.131988 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116248 INN loss: -1.116248 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125132 INN loss: -1.125132 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105614 INN loss: -1.105614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125091 INN loss: -1.125091 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157313 INN loss: -1.157313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130147 INN loss: -1.130147 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094596 INN loss: -1.094596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139836 INN loss: -1.139836 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119436 INN loss: -1.119436 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116572 INN loss: -1.116572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130405 INN loss: -1.130405 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113153 INN loss: -1.113153 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149680 INN loss: -1.149680 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125819 INN loss: -1.125819 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118189 INN loss: -1.118189 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140289 INN loss: -1.140289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168567 INN loss: -1.168567 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125616 INN loss: -1.125616 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122506 INN loss: -1.122506 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147891 INN loss: -1.147891 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129575 INN loss: -1.129575 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098799 INN loss: -1.098799 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149729 INN loss: -1.149729 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131325 INN loss: -1.131325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175066 INN loss: -1.175066 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118090 INN loss: -1.118090 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.195343 INN loss: -1.195343 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099268 INN loss: -1.099268 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110569 INN loss: -1.110569 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168164 INN loss: -1.168164 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115866 INN loss: -1.115866 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141800 INN loss: -1.141800 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177568 INN loss: -1.177568 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118781 INN loss: -1.118781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123765 INN loss: -1.123765 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117234 INN loss: -1.117234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133878 INN loss: -1.133878 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126117 INN loss: -1.126117 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120681 INN loss: -1.120681 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173281 INN loss: -1.173281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183395 INN loss: -1.183395 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131619 INN loss: -1.131619 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161032 INN loss: -1.161032 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161051 INN loss: -1.161051 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099460 INN loss: -1.099460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132431 INN loss: -1.132431 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130520 INN loss: -1.130520 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154507 INN loss: -1.154507 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121205 INN loss: -1.121205 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126200 INN loss: -1.126200 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121183 INN loss: -1.121183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135485 INN loss: -1.135485 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125300 INN loss: -1.125300 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131085 INN loss: -1.131085 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150417 INN loss: -1.150417 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157332 INN loss: -1.157332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184371 INN loss: -1.184371 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114302 INN loss: -1.114302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136519 INN loss: -1.136519 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155607 INN loss: -1.155607 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141196 INN loss: -1.141196 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101984 INN loss: -1.101984 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134206 INN loss: -1.134206 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153569 INN loss: -1.153569 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150393 INN loss: -1.150393 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122947 INN loss: -1.122947 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105067 INN loss: -1.105067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158859 INN loss: -1.158859 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171006 INN loss: -1.171006 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181384 INN loss: -1.181384 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134798 INN loss: -1.134798 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103800 INN loss: -1.103800 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146520 INN loss: -1.146520 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.065809 INN loss: -1.065809 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087898 INN loss: -1.087898 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169587 INN loss: -1.169587 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139771 INN loss: -1.139771 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164244 INN loss: -1.164244 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089720 INN loss: -1.089720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146760 INN loss: -1.146760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146165 INN loss: -1.146165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141002 INN loss: -1.141002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168959 INN loss: -1.168959 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157007 INN loss: -1.157007 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132632 INN loss: -1.132632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092070 INN loss: -1.092070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092702 INN loss: -1.092702 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145913 INN loss: -1.145913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142476 INN loss: -1.142476 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147180 INN loss: -1.147180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169945 INN loss: -1.169945 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119502 INN loss: -1.119502 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125661 INN loss: -1.125661 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148561 INN loss: -1.148561 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109721 INN loss: -1.109721 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126231 INN loss: -1.126231 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178027 INN loss: -1.178027 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107334 INN loss: -1.107334 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159867 INN loss: -1.159867 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154033 INN loss: -1.154033 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125179 INN loss: -1.125179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183856 INN loss: -1.183856 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123981 INN loss: -1.123981 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159719 INN loss: -1.159719 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106490 INN loss: -1.106490 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|███████████████████████████▌                                | 23/50 [3:58:56<46:17, 102.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.135262 INN loss: -1.135262 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155468 INN loss: -1.155468 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091007 INN loss: -1.091007 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140272 INN loss: -1.140272 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124761 INN loss: -1.124761 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108443 INN loss: -1.108443 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126200 INN loss: -1.126200 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106002 INN loss: -1.106002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163052 INN loss: -1.163052 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117802 INN loss: -1.117802 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138872 INN loss: -1.138872 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110130 INN loss: -1.110130 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101173 INN loss: -1.101173 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109848 INN loss: -1.109848 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150919 INN loss: -1.150919 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180430 INN loss: -1.180430 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120290 INN loss: -1.120290 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165382 INN loss: -1.165382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141232 INN loss: -1.141232 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.061601 INN loss: -1.061601 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119277 INN loss: -1.119277 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117845 INN loss: -1.117845 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149520 INN loss: -1.149520 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179219 INN loss: -1.179219 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162496 INN loss: -1.162496 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164811 INN loss: -1.164811 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130235 INN loss: -1.130235 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127477 INN loss: -1.127477 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160633 INN loss: -1.160633 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115663 INN loss: -1.115663 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161511 INN loss: -1.161511 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121199 INN loss: -1.121199 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155717 INN loss: -1.155717 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144332 INN loss: -1.144332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138332 INN loss: -1.138332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101498 INN loss: -1.101498 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150191 INN loss: -1.150191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113319 INN loss: -1.113319 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159313 INN loss: -1.159313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076165 INN loss: -1.076165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121304 INN loss: -1.121304 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100685 INN loss: -1.100685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127620 INN loss: -1.127620 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160795 INN loss: -1.160795 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108593 INN loss: -1.108593 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146111 INN loss: -1.146111 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148025 INN loss: -1.148025 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136867 INN loss: -1.136867 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107644 INN loss: -1.107644 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.062323 INN loss: -1.062323 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105266 INN loss: -1.105266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139830 INN loss: -1.139830 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116869 INN loss: -1.116869 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119582 INN loss: -1.119582 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162415 INN loss: -1.162415 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164404 INN loss: -1.164404 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162819 INN loss: -1.162819 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138672 INN loss: -1.138672 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117941 INN loss: -1.117941 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127901 INN loss: -1.127901 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114702 INN loss: -1.114702 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155504 INN loss: -1.155504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106364 INN loss: -1.106364 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171668 INN loss: -1.171668 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154165 INN loss: -1.154165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151826 INN loss: -1.151826 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154180 INN loss: -1.154180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136312 INN loss: -1.136312 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119214 INN loss: -1.119214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131380 INN loss: -1.131380 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128402 INN loss: -1.128402 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162865 INN loss: -1.162865 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166522 INN loss: -1.166522 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121880 INN loss: -1.121880 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134354 INN loss: -1.134354 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102579 INN loss: -1.102579 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129046 INN loss: -1.129046 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131824 INN loss: -1.131824 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110858 INN loss: -1.110858 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155388 INN loss: -1.155388 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135641 INN loss: -1.135641 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129885 INN loss: -1.129885 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162947 INN loss: -1.162947 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095377 INN loss: -1.095377 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.194244 INN loss: -1.194244 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165438 INN loss: -1.165438 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180546 INN loss: -1.180546 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097868 INN loss: -1.097868 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111307 INN loss: -1.111307 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153486 INN loss: -1.153486 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155460 INN loss: -1.155460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083322 INN loss: -1.083322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170933 INN loss: -1.170933 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136949 INN loss: -1.136949 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138770 INN loss: -1.138770 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106726 INN loss: -1.106726 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100769 INN loss: -1.100769 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124775 INN loss: -1.124775 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089747 INN loss: -1.089747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137594 INN loss: -1.137594 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094501 INN loss: -1.094501 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160796 INN loss: -1.160796 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146792 INN loss: -1.146792 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151242 INN loss: -1.151242 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120336 INN loss: -1.120336 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103540 INN loss: -1.103540 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182539 INN loss: -1.182539 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162621 INN loss: -1.162621 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164901 INN loss: -1.164901 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159077 INN loss: -1.159077 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142603 INN loss: -1.142603 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161266 INN loss: -1.161266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127991 INN loss: -1.127991 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162041 INN loss: -1.162041 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150210 INN loss: -1.150210 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131694 INN loss: -1.131694 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119852 INN loss: -1.119852 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.195378 INN loss: -1.195378 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163331 INN loss: -1.163331 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131728 INN loss: -1.131728 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133890 INN loss: -1.133890 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184973 INN loss: -1.184973 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157386 INN loss: -1.157386 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161164 INN loss: -1.161164 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188938 INN loss: -1.188938 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145527 INN loss: -1.145527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117207 INN loss: -1.117207 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140483 INN loss: -1.140483 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134107 INN loss: -1.134107 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160514 INN loss: -1.160514 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126592 INN loss: -1.126592 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146519 INN loss: -1.146519 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134055 INN loss: -1.134055 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161644 INN loss: -1.161644 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129737 INN loss: -1.129737 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133554 INN loss: -1.133554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156743 INN loss: -1.156743 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170174 INN loss: -1.170174 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096411 INN loss: -1.096411 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105942 INN loss: -1.105942 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156060 INN loss: -1.156060 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145182 INN loss: -1.145182 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130977 INN loss: -1.130977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144818 INN loss: -1.144818 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158639 INN loss: -1.158639 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116103 INN loss: -1.116103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111064 INN loss: -1.111064 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103898 INN loss: -1.103898 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143930 INN loss: -1.143930 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178329 INN loss: -1.178329 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110315 INN loss: -1.110315 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155011 INN loss: -1.155011 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096512 INN loss: -1.096512 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132209 INN loss: -1.132209 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099562 INN loss: -1.099562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124768 INN loss: -1.124768 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144143 INN loss: -1.144143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125434 INN loss: -1.125434 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139518 INN loss: -1.139518 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106609 INN loss: -1.106609 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████████████████████████████▊                               | 24/50 [4:00:37<44:21, 102.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.160808 INN loss: -1.160808 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126716 INN loss: -1.126716 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126224 INN loss: -1.126224 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113805 INN loss: -1.113805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090543 INN loss: -1.090543 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126351 INN loss: -1.126351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161953 INN loss: -1.161953 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159388 INN loss: -1.159388 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.193074 INN loss: -1.193074 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.204990 INN loss: -1.204990 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148097 INN loss: -1.148097 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108911 INN loss: -1.108911 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108182 INN loss: -1.108182 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.192885 INN loss: -1.192885 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103371 INN loss: -1.103371 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118940 INN loss: -1.118940 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132657 INN loss: -1.132657 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173215 INN loss: -1.173215 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136683 INN loss: -1.136683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166903 INN loss: -1.166903 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187928 INN loss: -1.187928 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101644 INN loss: -1.101644 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184147 INN loss: -1.184147 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142593 INN loss: -1.142593 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142290 INN loss: -1.142290 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128289 INN loss: -1.128289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165712 INN loss: -1.165712 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.067613 INN loss: -1.067613 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.191900 INN loss: -1.191900 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121934 INN loss: -1.121934 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136259 INN loss: -1.136259 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126036 INN loss: -1.126036 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160052 INN loss: -1.160052 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165089 INN loss: -1.165089 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149997 INN loss: -1.149997 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141530 INN loss: -1.141530 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169463 INN loss: -1.169463 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107471 INN loss: -1.107471 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142310 INN loss: -1.142310 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092862 INN loss: -1.092862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129945 INN loss: -1.129945 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143039 INN loss: -1.143039 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168286 INN loss: -1.168286 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152081 INN loss: -1.152081 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157332 INN loss: -1.157332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142812 INN loss: -1.142812 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.054845 INN loss: -1.054845 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160325 INN loss: -1.160325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124290 INN loss: -1.124290 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133249 INN loss: -1.133249 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179966 INN loss: -1.179966 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154801 INN loss: -1.154801 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.077825 INN loss: -1.077825 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129454 INN loss: -1.129454 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148994 INN loss: -1.148994 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167414 INN loss: -1.167414 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115472 INN loss: -1.115472 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115713 INN loss: -1.115713 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099272 INN loss: -1.099272 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089821 INN loss: -1.089821 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167054 INN loss: -1.167054 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116849 INN loss: -1.116849 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139907 INN loss: -1.139907 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096504 INN loss: -1.096504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151145 INN loss: -1.151145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166474 INN loss: -1.166474 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095139 INN loss: -1.095139 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156531 INN loss: -1.156531 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118906 INN loss: -1.118906 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121487 INN loss: -1.121487 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103309 INN loss: -1.103309 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137624 INN loss: -1.137624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103285 INN loss: -1.103285 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145286 INN loss: -1.145286 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129242 INN loss: -1.129242 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131291 INN loss: -1.131291 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.065134 INN loss: -1.065134 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096980 INN loss: -1.096980 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120563 INN loss: -1.120563 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118291 INN loss: -1.118291 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126624 INN loss: -1.126624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103151 INN loss: -1.103151 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095515 INN loss: -1.095515 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163716 INN loss: -1.163716 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113903 INN loss: -1.113903 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146911 INN loss: -1.146911 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149097 INN loss: -1.149097 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.198186 INN loss: -1.198186 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126666 INN loss: -1.126666 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104923 INN loss: -1.104923 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173589 INN loss: -1.173589 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132785 INN loss: -1.132785 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169886 INN loss: -1.169886 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155041 INN loss: -1.155041 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141934 INN loss: -1.141934 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137294 INN loss: -1.137294 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096437 INN loss: -1.096437 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157069 INN loss: -1.157069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097727 INN loss: -1.097727 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141352 INN loss: -1.141352 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115345 INN loss: -1.115345 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148772 INN loss: -1.148772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144865 INN loss: -1.144865 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114980 INN loss: -1.114980 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155662 INN loss: -1.155662 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143895 INN loss: -1.143895 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151235 INN loss: -1.151235 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.197095 INN loss: -1.197095 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140796 INN loss: -1.140796 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151869 INN loss: -1.151869 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161770 INN loss: -1.161770 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082876 INN loss: -1.082876 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131841 INN loss: -1.131841 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162690 INN loss: -1.162690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116423 INN loss: -1.116423 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136704 INN loss: -1.136704 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143065 INN loss: -1.143065 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091426 INN loss: -1.091426 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184064 INN loss: -1.184064 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153705 INN loss: -1.153705 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095887 INN loss: -1.095887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121108 INN loss: -1.121108 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123732 INN loss: -1.123732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189778 INN loss: -1.189778 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137773 INN loss: -1.137773 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160087 INN loss: -1.160087 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186861 INN loss: -1.186861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120554 INN loss: -1.120554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146705 INN loss: -1.146705 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181059 INN loss: -1.181059 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157527 INN loss: -1.157527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102639 INN loss: -1.102639 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130180 INN loss: -1.130180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137558 INN loss: -1.137558 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099162 INN loss: -1.099162 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147171 INN loss: -1.147171 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168112 INN loss: -1.168112 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095135 INN loss: -1.095135 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084565 INN loss: -1.084565 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146608 INN loss: -1.146608 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119268 INN loss: -1.119268 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146952 INN loss: -1.146952 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163327 INN loss: -1.163327 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153892 INN loss: -1.153892 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155571 INN loss: -1.155571 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146441 INN loss: -1.146441 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146814 INN loss: -1.146814 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151103 INN loss: -1.151103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096072 INN loss: -1.096072 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144809 INN loss: -1.144809 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184854 INN loss: -1.184854 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151271 INN loss: -1.151271 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144465 INN loss: -1.144465 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145820 INN loss: -1.145820 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118990 INN loss: -1.118990 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145640 INN loss: -1.145640 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165565 INN loss: -1.165565 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176418 INN loss: -1.176418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128453 INN loss: -1.128453 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095781 INN loss: -1.095781 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████                              | 25/50 [4:02:19<42:36, 102.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.075731 INN loss: -1.075731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105533 INN loss: -1.105533 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120919 INN loss: -1.120919 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114208 INN loss: -1.114208 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141648 INN loss: -1.141648 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125202 INN loss: -1.125202 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151463 INN loss: -1.151463 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151242 INN loss: -1.151242 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137282 INN loss: -1.137282 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119618 INN loss: -1.119618 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087367 INN loss: -1.087367 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123169 INN loss: -1.123169 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134744 INN loss: -1.134744 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158398 INN loss: -1.158398 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108786 INN loss: -1.108786 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140260 INN loss: -1.140260 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132800 INN loss: -1.132800 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117839 INN loss: -1.117839 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158778 INN loss: -1.158778 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147582 INN loss: -1.147582 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190895 INN loss: -1.190895 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126799 INN loss: -1.126799 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151116 INN loss: -1.151116 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143618 INN loss: -1.143618 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.197153 INN loss: -1.197153 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119209 INN loss: -1.119209 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173155 INN loss: -1.173155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143094 INN loss: -1.143094 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.066192 INN loss: -1.066192 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144979 INN loss: -1.144979 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150383 INN loss: -1.150383 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143359 INN loss: -1.143359 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142570 INN loss: -1.142570 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173589 INN loss: -1.173589 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120516 INN loss: -1.120516 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154202 INN loss: -1.154202 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118005 INN loss: -1.118005 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146276 INN loss: -1.146276 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.198234 INN loss: -1.198234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109840 INN loss: -1.109840 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128930 INN loss: -1.128930 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114988 INN loss: -1.114988 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131102 INN loss: -1.131102 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176157 INN loss: -1.176157 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123855 INN loss: -1.123855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104296 INN loss: -1.104296 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182265 INN loss: -1.182265 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139959 INN loss: -1.139959 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139071 INN loss: -1.139071 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106129 INN loss: -1.106129 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140665 INN loss: -1.140665 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126650 INN loss: -1.126650 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156740 INN loss: -1.156740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141930 INN loss: -1.141930 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133604 INN loss: -1.133604 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130976 INN loss: -1.130976 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133087 INN loss: -1.133087 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099030 INN loss: -1.099030 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107705 INN loss: -1.107705 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167917 INN loss: -1.167917 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126000 INN loss: -1.126000 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112873 INN loss: -1.112873 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094058 INN loss: -1.094058 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146133 INN loss: -1.146133 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138760 INN loss: -1.138760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110101 INN loss: -1.110101 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149981 INN loss: -1.149981 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146296 INN loss: -1.146296 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127958 INN loss: -1.127958 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128663 INN loss: -1.128663 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146156 INN loss: -1.146156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092108 INN loss: -1.092108 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125254 INN loss: -1.125254 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151968 INN loss: -1.151968 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148265 INN loss: -1.148265 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172183 INN loss: -1.172183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150978 INN loss: -1.150978 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177030 INN loss: -1.177030 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131740 INN loss: -1.131740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132949 INN loss: -1.132949 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124428 INN loss: -1.124428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155296 INN loss: -1.155296 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130214 INN loss: -1.130214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162447 INN loss: -1.162447 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105544 INN loss: -1.105544 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095920 INN loss: -1.095920 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178132 INN loss: -1.178132 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119183 INN loss: -1.119183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156553 INN loss: -1.156553 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139321 INN loss: -1.139321 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156511 INN loss: -1.156511 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144563 INN loss: -1.144563 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134247 INN loss: -1.134247 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106546 INN loss: -1.106546 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152840 INN loss: -1.152840 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180248 INN loss: -1.180248 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137625 INN loss: -1.137625 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138993 INN loss: -1.138993 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125993 INN loss: -1.125993 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172088 INN loss: -1.172088 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124864 INN loss: -1.124864 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093600 INN loss: -1.093600 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170142 INN loss: -1.170142 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.078485 INN loss: -1.078485 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150676 INN loss: -1.150676 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156476 INN loss: -1.156476 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167664 INN loss: -1.167664 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092460 INN loss: -1.092460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105068 INN loss: -1.105068 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153357 INN loss: -1.153357 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.075110 INN loss: -1.075110 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127069 INN loss: -1.127069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158706 INN loss: -1.158706 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161667 INN loss: -1.161667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102998 INN loss: -1.102998 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159751 INN loss: -1.159751 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163865 INN loss: -1.163865 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169267 INN loss: -1.169267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164880 INN loss: -1.164880 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149407 INN loss: -1.149407 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128265 INN loss: -1.128265 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145809 INN loss: -1.145809 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148458 INN loss: -1.148458 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138394 INN loss: -1.138394 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165922 INN loss: -1.165922 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125620 INN loss: -1.125620 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150696 INN loss: -1.150696 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114126 INN loss: -1.114126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109132 INN loss: -1.109132 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084780 INN loss: -1.084780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125384 INN loss: -1.125384 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085387 INN loss: -1.085387 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129684 INN loss: -1.129684 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143446 INN loss: -1.143446 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139147 INN loss: -1.139147 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136161 INN loss: -1.136161 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159868 INN loss: -1.159868 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.064740 INN loss: -1.064740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128260 INN loss: -1.128260 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117213 INN loss: -1.117213 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115984 INN loss: -1.115984 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116707 INN loss: -1.116707 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134982 INN loss: -1.134982 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085389 INN loss: -1.085389 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157543 INN loss: -1.157543 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094624 INN loss: -1.094624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182849 INN loss: -1.182849 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179687 INN loss: -1.179687 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124949 INN loss: -1.124949 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107855 INN loss: -1.107855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095346 INN loss: -1.095346 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171563 INN loss: -1.171563 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153529 INN loss: -1.153529 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114851 INN loss: -1.114851 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160266 INN loss: -1.160266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089984 INN loss: -1.089984 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132981 INN loss: -1.132981 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146572 INN loss: -1.146572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117203 INN loss: -1.117203 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133993 INN loss: -1.133993 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████▏                            | 26/50 [4:04:02<41:02, 102.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.120398 INN loss: -1.120398 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135804 INN loss: -1.135804 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167289 INN loss: -1.167289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112284 INN loss: -1.112284 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160643 INN loss: -1.160643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108680 INN loss: -1.108680 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149508 INN loss: -1.149508 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094828 INN loss: -1.094828 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165916 INN loss: -1.165916 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158204 INN loss: -1.158204 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143656 INN loss: -1.143656 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166003 INN loss: -1.166003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124890 INN loss: -1.124890 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.078623 INN loss: -1.078623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113201 INN loss: -1.113201 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119838 INN loss: -1.119838 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147319 INN loss: -1.147319 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178800 INN loss: -1.178800 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165188 INN loss: -1.165188 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138154 INN loss: -1.138154 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122617 INN loss: -1.122617 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145227 INN loss: -1.145227 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118742 INN loss: -1.118742 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119272 INN loss: -1.119272 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123973 INN loss: -1.123973 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121781 INN loss: -1.121781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143712 INN loss: -1.143712 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109532 INN loss: -1.109532 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114919 INN loss: -1.114919 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113645 INN loss: -1.113645 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150103 INN loss: -1.150103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096656 INN loss: -1.096656 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181269 INN loss: -1.181269 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160297 INN loss: -1.160297 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148401 INN loss: -1.148401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114919 INN loss: -1.114919 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105995 INN loss: -1.105995 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184478 INN loss: -1.184478 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145363 INN loss: -1.145363 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137709 INN loss: -1.137709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128431 INN loss: -1.128431 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149440 INN loss: -1.149440 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114722 INN loss: -1.114722 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098545 INN loss: -1.098545 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123828 INN loss: -1.123828 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165619 INN loss: -1.165619 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114162 INN loss: -1.114162 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091112 INN loss: -1.091112 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162748 INN loss: -1.162748 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168058 INN loss: -1.168058 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136339 INN loss: -1.136339 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135081 INN loss: -1.135081 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124929 INN loss: -1.124929 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170016 INN loss: -1.170016 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151763 INN loss: -1.151763 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153295 INN loss: -1.153295 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121251 INN loss: -1.121251 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161781 INN loss: -1.161781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185897 INN loss: -1.185897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169527 INN loss: -1.169527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133312 INN loss: -1.133312 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177861 INN loss: -1.177861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123150 INN loss: -1.123150 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156541 INN loss: -1.156541 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159703 INN loss: -1.159703 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132048 INN loss: -1.132048 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091043 INN loss: -1.091043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138533 INN loss: -1.138533 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134826 INN loss: -1.134826 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139788 INN loss: -1.139788 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130905 INN loss: -1.130905 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134747 INN loss: -1.134747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163470 INN loss: -1.163470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142879 INN loss: -1.142879 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126624 INN loss: -1.126624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179108 INN loss: -1.179108 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103018 INN loss: -1.103018 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119088 INN loss: -1.119088 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159984 INN loss: -1.159984 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129913 INN loss: -1.129913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130695 INN loss: -1.130695 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140947 INN loss: -1.140947 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139072 INN loss: -1.139072 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111016 INN loss: -1.111016 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166934 INN loss: -1.166934 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178060 INN loss: -1.178060 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093302 INN loss: -1.093302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076470 INN loss: -1.076470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141675 INN loss: -1.141675 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109566 INN loss: -1.109566 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124461 INN loss: -1.124461 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155062 INN loss: -1.155062 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131797 INN loss: -1.131797 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180188 INN loss: -1.180188 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109091 INN loss: -1.109091 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111826 INN loss: -1.111826 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101361 INN loss: -1.101361 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156899 INN loss: -1.156899 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147111 INN loss: -1.147111 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121086 INN loss: -1.121086 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140837 INN loss: -1.140837 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134732 INN loss: -1.134732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134633 INN loss: -1.134633 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109760 INN loss: -1.109760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158203 INN loss: -1.158203 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136266 INN loss: -1.136266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151294 INN loss: -1.151294 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154583 INN loss: -1.154583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136913 INN loss: -1.136913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155009 INN loss: -1.155009 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113369 INN loss: -1.113369 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134417 INN loss: -1.134417 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126723 INN loss: -1.126723 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161853 INN loss: -1.161853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115200 INN loss: -1.115200 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129117 INN loss: -1.129117 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120151 INN loss: -1.120151 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119943 INN loss: -1.119943 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176152 INN loss: -1.176152 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124192 INN loss: -1.124192 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110960 INN loss: -1.110960 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152088 INN loss: -1.152088 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170618 INN loss: -1.170618 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.192490 INN loss: -1.192490 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149483 INN loss: -1.149483 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161126 INN loss: -1.161126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165414 INN loss: -1.165414 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123227 INN loss: -1.123227 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109209 INN loss: -1.109209 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108923 INN loss: -1.108923 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147443 INN loss: -1.147443 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149221 INN loss: -1.149221 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091313 INN loss: -1.091313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129475 INN loss: -1.129475 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186477 INN loss: -1.186477 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161310 INN loss: -1.161310 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153141 INN loss: -1.153141 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158740 INN loss: -1.158740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113771 INN loss: -1.113771 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141561 INN loss: -1.141561 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.227126 INN loss: -1.227126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151179 INN loss: -1.151179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134309 INN loss: -1.134309 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104960 INN loss: -1.104960 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136156 INN loss: -1.136156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122039 INN loss: -1.122039 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141868 INN loss: -1.141868 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098170 INN loss: -1.098170 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131758 INN loss: -1.131758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131699 INN loss: -1.131699 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157672 INN loss: -1.157672 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121387 INN loss: -1.121387 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080607 INN loss: -1.080607 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175811 INN loss: -1.175811 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176814 INN loss: -1.176814 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115693 INN loss: -1.115693 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158506 INN loss: -1.158506 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150239 INN loss: -1.150239 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155821 INN loss: -1.155821 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123338 INN loss: -1.123338 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████▍                           | 27/50 [4:05:48<39:42, 103.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.146230 INN loss: -1.146230 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135806 INN loss: -1.135806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133129 INN loss: -1.133129 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170144 INN loss: -1.170144 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122674 INN loss: -1.122674 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128694 INN loss: -1.128694 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157417 INN loss: -1.157417 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136403 INN loss: -1.136403 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134161 INN loss: -1.134161 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099383 INN loss: -1.099383 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104939 INN loss: -1.104939 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178558 INN loss: -1.178558 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146684 INN loss: -1.146684 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104893 INN loss: -1.104893 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100325 INN loss: -1.100325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095877 INN loss: -1.095877 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153467 INN loss: -1.153467 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145541 INN loss: -1.145541 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126692 INN loss: -1.126692 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107566 INN loss: -1.107566 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136931 INN loss: -1.136931 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137185 INN loss: -1.137185 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158060 INN loss: -1.158060 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135541 INN loss: -1.135541 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084982 INN loss: -1.084982 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150199 INN loss: -1.150199 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142492 INN loss: -1.142492 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130072 INN loss: -1.130072 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103837 INN loss: -1.103837 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135706 INN loss: -1.135706 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112985 INN loss: -1.112985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122562 INN loss: -1.122562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164352 INN loss: -1.164352 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159248 INN loss: -1.159248 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094128 INN loss: -1.094128 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105847 INN loss: -1.105847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119199 INN loss: -1.119199 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158460 INN loss: -1.158460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119736 INN loss: -1.119736 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136237 INN loss: -1.136237 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137592 INN loss: -1.137592 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140729 INN loss: -1.140729 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097787 INN loss: -1.097787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130741 INN loss: -1.130741 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149486 INN loss: -1.149486 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135535 INN loss: -1.135535 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122123 INN loss: -1.122123 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140312 INN loss: -1.140312 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147745 INN loss: -1.147745 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085992 INN loss: -1.085992 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128562 INN loss: -1.128562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128514 INN loss: -1.128514 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157639 INN loss: -1.157639 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138377 INN loss: -1.138377 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089914 INN loss: -1.089914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126055 INN loss: -1.126055 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150597 INN loss: -1.150597 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111942 INN loss: -1.111942 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164628 INN loss: -1.164628 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127327 INN loss: -1.127327 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153504 INN loss: -1.153504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131147 INN loss: -1.131147 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151668 INN loss: -1.151668 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166295 INN loss: -1.166295 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132999 INN loss: -1.132999 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126007 INN loss: -1.126007 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111048 INN loss: -1.111048 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154034 INN loss: -1.154034 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119564 INN loss: -1.119564 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163838 INN loss: -1.163838 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156698 INN loss: -1.156698 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150641 INN loss: -1.150641 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137821 INN loss: -1.137821 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128043 INN loss: -1.128043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173898 INN loss: -1.173898 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121580 INN loss: -1.121580 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145940 INN loss: -1.145940 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145932 INN loss: -1.145932 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147876 INN loss: -1.147876 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138173 INN loss: -1.138173 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112822 INN loss: -1.112822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145616 INN loss: -1.145616 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137476 INN loss: -1.137476 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187903 INN loss: -1.187903 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149148 INN loss: -1.149148 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177109 INN loss: -1.177109 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155619 INN loss: -1.155619 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170659 INN loss: -1.170659 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169050 INN loss: -1.169050 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147307 INN loss: -1.147307 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151745 INN loss: -1.151745 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114258 INN loss: -1.114258 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134958 INN loss: -1.134958 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164410 INN loss: -1.164410 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102499 INN loss: -1.102499 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141029 INN loss: -1.141029 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148702 INN loss: -1.148702 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160115 INN loss: -1.160115 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125341 INN loss: -1.125341 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133161 INN loss: -1.133161 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147706 INN loss: -1.147706 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110251 INN loss: -1.110251 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112883 INN loss: -1.112883 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165684 INN loss: -1.165684 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116733 INN loss: -1.116733 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112768 INN loss: -1.112768 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111776 INN loss: -1.111776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162691 INN loss: -1.162691 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167860 INN loss: -1.167860 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103400 INN loss: -1.103400 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124835 INN loss: -1.124835 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076869 INN loss: -1.076869 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155037 INN loss: -1.155037 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139720 INN loss: -1.139720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134158 INN loss: -1.134158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133785 INN loss: -1.133785 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149736 INN loss: -1.149736 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131584 INN loss: -1.131584 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163759 INN loss: -1.163759 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153513 INN loss: -1.153513 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122008 INN loss: -1.122008 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111338 INN loss: -1.111338 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144205 INN loss: -1.144205 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167351 INN loss: -1.167351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141513 INN loss: -1.141513 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158306 INN loss: -1.158306 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162018 INN loss: -1.162018 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147663 INN loss: -1.147663 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143048 INN loss: -1.143048 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172427 INN loss: -1.172427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135394 INN loss: -1.135394 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113662 INN loss: -1.113662 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091283 INN loss: -1.091283 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167481 INN loss: -1.167481 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123345 INN loss: -1.123345 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119070 INN loss: -1.119070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136491 INN loss: -1.136491 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173360 INN loss: -1.173360 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148110 INN loss: -1.148110 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110566 INN loss: -1.110566 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116742 INN loss: -1.116742 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122966 INN loss: -1.122966 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110479 INN loss: -1.110479 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111919 INN loss: -1.111919 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174351 INN loss: -1.174351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163911 INN loss: -1.163911 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145051 INN loss: -1.145051 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178956 INN loss: -1.178956 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154748 INN loss: -1.154748 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179521 INN loss: -1.179521 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168884 INN loss: -1.168884 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160281 INN loss: -1.160281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156320 INN loss: -1.156320 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170600 INN loss: -1.170600 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175233 INN loss: -1.175233 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154851 INN loss: -1.154851 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162770 INN loss: -1.162770 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.077839 INN loss: -1.077839 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127046 INN loss: -1.127046 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147248 INN loss: -1.147248 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████▌                          | 28/50 [4:07:30<37:44, 102.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.104132 INN loss: -1.104132 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117674 INN loss: -1.117674 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148209 INN loss: -1.148209 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108690 INN loss: -1.108690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.199799 INN loss: -1.199799 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185494 INN loss: -1.185494 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129285 INN loss: -1.129285 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125016 INN loss: -1.125016 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150819 INN loss: -1.150819 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122777 INN loss: -1.122777 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173247 INN loss: -1.173247 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135388 INN loss: -1.135388 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143309 INN loss: -1.143309 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164187 INN loss: -1.164187 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121378 INN loss: -1.121378 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129868 INN loss: -1.129868 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161443 INN loss: -1.161443 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156868 INN loss: -1.156868 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122309 INN loss: -1.122309 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186325 INN loss: -1.186325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170241 INN loss: -1.170241 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134213 INN loss: -1.134213 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156430 INN loss: -1.156430 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129138 INN loss: -1.129138 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128742 INN loss: -1.128742 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170961 INN loss: -1.170961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120739 INN loss: -1.120739 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.075539 INN loss: -1.075539 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125361 INN loss: -1.125361 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111557 INN loss: -1.111557 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155521 INN loss: -1.155521 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183720 INN loss: -1.183720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121609 INN loss: -1.121609 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164413 INN loss: -1.164413 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159393 INN loss: -1.159393 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116330 INN loss: -1.116330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125421 INN loss: -1.125421 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143014 INN loss: -1.143014 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130946 INN loss: -1.130946 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176871 INN loss: -1.176871 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108122 INN loss: -1.108122 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171181 INN loss: -1.171181 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130954 INN loss: -1.130954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121947 INN loss: -1.121947 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105143 INN loss: -1.105143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097670 INN loss: -1.097670 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091644 INN loss: -1.091644 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182362 INN loss: -1.182362 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138253 INN loss: -1.138253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147405 INN loss: -1.147405 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.196021 INN loss: -1.196021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127197 INN loss: -1.127197 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159627 INN loss: -1.159627 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188570 INN loss: -1.188570 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189824 INN loss: -1.189824 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137720 INN loss: -1.137720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.195246 INN loss: -1.195246 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.191313 INN loss: -1.191313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136754 INN loss: -1.136754 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106233 INN loss: -1.106233 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180303 INN loss: -1.180303 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147165 INN loss: -1.147165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161104 INN loss: -1.161104 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.056658 INN loss: -1.056658 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128673 INN loss: -1.128673 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120698 INN loss: -1.120698 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093539 INN loss: -1.093539 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143450 INN loss: -1.143450 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127480 INN loss: -1.127480 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181558 INN loss: -1.181558 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170847 INN loss: -1.170847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139485 INN loss: -1.139485 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104982 INN loss: -1.104982 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161461 INN loss: -1.161461 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135307 INN loss: -1.135307 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.079277 INN loss: -1.079277 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118694 INN loss: -1.118694 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132411 INN loss: -1.132411 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113835 INN loss: -1.113835 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125051 INN loss: -1.125051 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169457 INN loss: -1.169457 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165409 INN loss: -1.165409 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165718 INN loss: -1.165718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126021 INN loss: -1.126021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096900 INN loss: -1.096900 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090312 INN loss: -1.090312 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156427 INN loss: -1.156427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128207 INN loss: -1.128207 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158927 INN loss: -1.158927 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145051 INN loss: -1.145051 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110999 INN loss: -1.110999 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128143 INN loss: -1.128143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094252 INN loss: -1.094252 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174923 INN loss: -1.174923 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158906 INN loss: -1.158906 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115561 INN loss: -1.115561 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159664 INN loss: -1.159664 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150238 INN loss: -1.150238 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137061 INN loss: -1.137061 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172654 INN loss: -1.172654 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108450 INN loss: -1.108450 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130624 INN loss: -1.130624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128674 INN loss: -1.128674 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137626 INN loss: -1.137626 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146992 INN loss: -1.146992 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133954 INN loss: -1.133954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184496 INN loss: -1.184496 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103841 INN loss: -1.103841 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141260 INN loss: -1.141260 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168731 INN loss: -1.168731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128349 INN loss: -1.128349 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127315 INN loss: -1.127315 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141504 INN loss: -1.141504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108787 INN loss: -1.108787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148727 INN loss: -1.148727 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125148 INN loss: -1.125148 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128047 INN loss: -1.128047 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141435 INN loss: -1.141435 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186364 INN loss: -1.186364 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125810 INN loss: -1.125810 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143980 INN loss: -1.143980 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108279 INN loss: -1.108279 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121126 INN loss: -1.121126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118317 INN loss: -1.118317 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169968 INN loss: -1.169968 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132873 INN loss: -1.132873 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118238 INN loss: -1.118238 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172120 INN loss: -1.172120 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154429 INN loss: -1.154429 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130021 INN loss: -1.130021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105562 INN loss: -1.105562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129234 INN loss: -1.129234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.069782 INN loss: -1.069782 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105003 INN loss: -1.105003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131094 INN loss: -1.131094 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148418 INN loss: -1.148418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141241 INN loss: -1.141241 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159787 INN loss: -1.159787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152623 INN loss: -1.152623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112480 INN loss: -1.112480 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162526 INN loss: -1.162526 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182996 INN loss: -1.182996 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145884 INN loss: -1.145884 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153848 INN loss: -1.153848 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161566 INN loss: -1.161566 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107062 INN loss: -1.107062 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122913 INN loss: -1.122913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159857 INN loss: -1.159857 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122317 INN loss: -1.122317 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.079638 INN loss: -1.079638 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103403 INN loss: -1.103403 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146420 INN loss: -1.146420 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128979 INN loss: -1.128979 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179813 INN loss: -1.179813 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153402 INN loss: -1.153402 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126026 INN loss: -1.126026 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113137 INN loss: -1.113137 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144105 INN loss: -1.144105 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173170 INN loss: -1.173170 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182571 INN loss: -1.182571 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|██████████████████████████████████▊                         | 29/50 [4:09:17<36:31, 104.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.114997 INN loss: -1.114997 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111322 INN loss: -1.111322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107072 INN loss: -1.107072 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183406 INN loss: -1.183406 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095774 INN loss: -1.095774 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147285 INN loss: -1.147285 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127609 INN loss: -1.127609 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154515 INN loss: -1.154515 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152019 INN loss: -1.152019 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125222 INN loss: -1.125222 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176206 INN loss: -1.176206 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137732 INN loss: -1.137732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134871 INN loss: -1.134871 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157888 INN loss: -1.157888 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085507 INN loss: -1.085507 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152563 INN loss: -1.152563 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115750 INN loss: -1.115750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160822 INN loss: -1.160822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125677 INN loss: -1.125677 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161679 INN loss: -1.161679 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117830 INN loss: -1.117830 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108250 INN loss: -1.108250 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134620 INN loss: -1.134620 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111476 INN loss: -1.111476 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132079 INN loss: -1.132079 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114931 INN loss: -1.114931 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119527 INN loss: -1.119527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131336 INN loss: -1.131336 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117926 INN loss: -1.117926 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152368 INN loss: -1.152368 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107890 INN loss: -1.107890 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118066 INN loss: -1.118066 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102962 INN loss: -1.102962 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166975 INN loss: -1.166975 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157808 INN loss: -1.157808 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.195432 INN loss: -1.195432 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138487 INN loss: -1.138487 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076253 INN loss: -1.076253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120391 INN loss: -1.120391 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.205515 INN loss: -1.205515 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101555 INN loss: -1.101555 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135612 INN loss: -1.135612 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110253 INN loss: -1.110253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113014 INN loss: -1.113014 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164142 INN loss: -1.164142 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136158 INN loss: -1.136158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160300 INN loss: -1.160300 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130750 INN loss: -1.130750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109259 INN loss: -1.109259 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129200 INN loss: -1.129200 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103838 INN loss: -1.103838 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117536 INN loss: -1.117536 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.201736 INN loss: -1.201736 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112145 INN loss: -1.112145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115382 INN loss: -1.115382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148367 INN loss: -1.148367 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.207871 INN loss: -1.207871 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140997 INN loss: -1.140997 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105968 INN loss: -1.105968 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144594 INN loss: -1.144594 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143830 INN loss: -1.143830 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131502 INN loss: -1.131502 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150098 INN loss: -1.150098 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167872 INN loss: -1.167872 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121491 INN loss: -1.121491 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179908 INN loss: -1.179908 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136047 INN loss: -1.136047 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156258 INN loss: -1.156258 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159583 INN loss: -1.159583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157696 INN loss: -1.157696 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104411 INN loss: -1.104411 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188961 INN loss: -1.188961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172253 INN loss: -1.172253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158846 INN loss: -1.158846 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106238 INN loss: -1.106238 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182711 INN loss: -1.182711 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101017 INN loss: -1.101017 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171984 INN loss: -1.171984 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151850 INN loss: -1.151850 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143750 INN loss: -1.143750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145417 INN loss: -1.145417 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150275 INN loss: -1.150275 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121344 INN loss: -1.121344 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133822 INN loss: -1.133822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106755 INN loss: -1.106755 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152381 INN loss: -1.152381 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150877 INN loss: -1.150877 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128756 INN loss: -1.128756 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156762 INN loss: -1.156762 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162508 INN loss: -1.162508 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161801 INN loss: -1.161801 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136098 INN loss: -1.136098 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.199337 INN loss: -1.199337 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107743 INN loss: -1.107743 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.192793 INN loss: -1.192793 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157256 INN loss: -1.157256 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106732 INN loss: -1.106732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101265 INN loss: -1.101265 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163690 INN loss: -1.163690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134583 INN loss: -1.134583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141998 INN loss: -1.141998 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138645 INN loss: -1.138645 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129410 INN loss: -1.129410 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178229 INN loss: -1.178229 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127750 INN loss: -1.127750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125627 INN loss: -1.125627 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180860 INN loss: -1.180860 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131432 INN loss: -1.131432 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132084 INN loss: -1.132084 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146344 INN loss: -1.146344 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158009 INN loss: -1.158009 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153618 INN loss: -1.153618 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123701 INN loss: -1.123701 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115990 INN loss: -1.115990 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156834 INN loss: -1.156834 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132949 INN loss: -1.132949 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156630 INN loss: -1.156630 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147688 INN loss: -1.147688 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168732 INN loss: -1.168732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161125 INN loss: -1.161125 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125080 INN loss: -1.125080 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114485 INN loss: -1.114485 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136657 INN loss: -1.136657 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109324 INN loss: -1.109324 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180676 INN loss: -1.180676 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121612 INN loss: -1.121612 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104865 INN loss: -1.104865 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141554 INN loss: -1.141554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.195932 INN loss: -1.195932 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173304 INN loss: -1.173304 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169109 INN loss: -1.169109 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149689 INN loss: -1.149689 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134384 INN loss: -1.134384 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119120 INN loss: -1.119120 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151560 INN loss: -1.151560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135165 INN loss: -1.135165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126125 INN loss: -1.126125 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109776 INN loss: -1.109776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166446 INN loss: -1.166446 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142143 INN loss: -1.142143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176118 INN loss: -1.176118 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121704 INN loss: -1.121704 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115152 INN loss: -1.115152 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158435 INN loss: -1.158435 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137563 INN loss: -1.137563 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094448 INN loss: -1.094448 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150297 INN loss: -1.150297 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.077001 INN loss: -1.077001 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154147 INN loss: -1.154147 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175703 INN loss: -1.175703 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126766 INN loss: -1.126766 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135275 INN loss: -1.135275 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111902 INN loss: -1.111902 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152800 INN loss: -1.152800 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157562 INN loss: -1.157562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166590 INN loss: -1.166590 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135947 INN loss: -1.135947 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127798 INN loss: -1.127798 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135234 INN loss: -1.135234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163420 INN loss: -1.163420 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████                        | 30/50 [4:11:01<34:45, 104.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.113114 INN loss: -1.113114 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119843 INN loss: -1.119843 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155119 INN loss: -1.155119 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178128 INN loss: -1.178128 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154755 INN loss: -1.154755 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137601 INN loss: -1.137601 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101237 INN loss: -1.101237 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157340 INN loss: -1.157340 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170749 INN loss: -1.170749 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133820 INN loss: -1.133820 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134397 INN loss: -1.134397 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161222 INN loss: -1.161222 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141532 INN loss: -1.141532 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150620 INN loss: -1.150620 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171963 INN loss: -1.171963 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098439 INN loss: -1.098439 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179862 INN loss: -1.179862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137852 INN loss: -1.137852 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186910 INN loss: -1.186910 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109363 INN loss: -1.109363 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089126 INN loss: -1.089126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104746 INN loss: -1.104746 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126530 INN loss: -1.126530 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165315 INN loss: -1.165315 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162320 INN loss: -1.162320 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106492 INN loss: -1.106492 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153460 INN loss: -1.153460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144922 INN loss: -1.144922 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165493 INN loss: -1.165493 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.053398 INN loss: -1.053398 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.194443 INN loss: -1.194443 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156489 INN loss: -1.156489 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149681 INN loss: -1.149681 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150042 INN loss: -1.150042 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136053 INN loss: -1.136053 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107299 INN loss: -1.107299 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114283 INN loss: -1.114283 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147708 INN loss: -1.147708 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123011 INN loss: -1.123011 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114789 INN loss: -1.114789 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113401 INN loss: -1.113401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131918 INN loss: -1.131918 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125510 INN loss: -1.125510 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101883 INN loss: -1.101883 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126811 INN loss: -1.126811 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168661 INN loss: -1.168661 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146470 INN loss: -1.146470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142147 INN loss: -1.142147 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182723 INN loss: -1.182723 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144975 INN loss: -1.144975 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184499 INN loss: -1.184499 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.062038 INN loss: -1.062038 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183183 INN loss: -1.183183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126308 INN loss: -1.126308 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170929 INN loss: -1.170929 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131603 INN loss: -1.131603 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163121 INN loss: -1.163121 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190772 INN loss: -1.190772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148157 INN loss: -1.148157 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162696 INN loss: -1.162696 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135414 INN loss: -1.135414 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099628 INN loss: -1.099628 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131843 INN loss: -1.131843 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151470 INN loss: -1.151470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138283 INN loss: -1.138283 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182997 INN loss: -1.182997 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162159 INN loss: -1.162159 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112798 INN loss: -1.112798 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100441 INN loss: -1.100441 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141160 INN loss: -1.141160 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186862 INN loss: -1.186862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150061 INN loss: -1.150061 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125784 INN loss: -1.125784 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152267 INN loss: -1.152267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111253 INN loss: -1.111253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127362 INN loss: -1.127362 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162444 INN loss: -1.162444 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175351 INN loss: -1.175351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140787 INN loss: -1.140787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134887 INN loss: -1.134887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147084 INN loss: -1.147084 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159302 INN loss: -1.159302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162802 INN loss: -1.162802 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.193180 INN loss: -1.193180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153529 INN loss: -1.153529 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177325 INN loss: -1.177325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134464 INN loss: -1.134464 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160115 INN loss: -1.160115 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158445 INN loss: -1.158445 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143710 INN loss: -1.143710 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120445 INN loss: -1.120445 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129484 INN loss: -1.129484 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140070 INN loss: -1.140070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109916 INN loss: -1.109916 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157156 INN loss: -1.157156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141024 INN loss: -1.141024 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143618 INN loss: -1.143618 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150247 INN loss: -1.150247 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142865 INN loss: -1.142865 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153789 INN loss: -1.153789 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160781 INN loss: -1.160781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126088 INN loss: -1.126088 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101372 INN loss: -1.101372 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128865 INN loss: -1.128865 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159769 INN loss: -1.159769 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091497 INN loss: -1.091497 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168685 INN loss: -1.168685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139311 INN loss: -1.139311 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121536 INN loss: -1.121536 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133368 INN loss: -1.133368 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106358 INN loss: -1.106358 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118028 INN loss: -1.118028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145529 INN loss: -1.145529 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152545 INN loss: -1.152545 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165174 INN loss: -1.165174 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114596 INN loss: -1.114596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136415 INN loss: -1.136415 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145005 INN loss: -1.145005 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166433 INN loss: -1.166433 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175070 INN loss: -1.175070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119012 INN loss: -1.119012 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092857 INN loss: -1.092857 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119421 INN loss: -1.119421 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118350 INN loss: -1.118350 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185294 INN loss: -1.185294 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155746 INN loss: -1.155746 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139390 INN loss: -1.139390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144329 INN loss: -1.144329 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104509 INN loss: -1.104509 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142953 INN loss: -1.142953 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161842 INN loss: -1.161842 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141701 INN loss: -1.141701 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103258 INN loss: -1.103258 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137790 INN loss: -1.137790 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149971 INN loss: -1.149971 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150196 INN loss: -1.150196 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160510 INN loss: -1.160510 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154321 INN loss: -1.154321 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175166 INN loss: -1.175166 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096817 INN loss: -1.096817 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164214 INN loss: -1.164214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132251 INN loss: -1.132251 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096507 INN loss: -1.096507 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135749 INN loss: -1.135749 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157671 INN loss: -1.157671 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173398 INN loss: -1.173398 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142335 INN loss: -1.142335 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131005 INN loss: -1.131005 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174637 INN loss: -1.174637 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076578 INN loss: -1.076578 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112888 INN loss: -1.112888 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081255 INN loss: -1.081255 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157803 INN loss: -1.157803 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175711 INN loss: -1.175711 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144921 INN loss: -1.144921 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146229 INN loss: -1.146229 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107035 INN loss: -1.107035 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178488 INN loss: -1.178488 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174256 INN loss: -1.174256 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112275 INN loss: -1.112275 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████▏                      | 31/50 [4:12:46<33:04, 104.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.131897 INN loss: -1.131897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136564 INN loss: -1.136564 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144203 INN loss: -1.144203 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142324 INN loss: -1.142324 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140884 INN loss: -1.140884 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157985 INN loss: -1.157985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110533 INN loss: -1.110533 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185180 INN loss: -1.185180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162674 INN loss: -1.162674 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112321 INN loss: -1.112321 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140067 INN loss: -1.140067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170635 INN loss: -1.170635 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143718 INN loss: -1.143718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108497 INN loss: -1.108497 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122526 INN loss: -1.122526 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146922 INN loss: -1.146922 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089583 INN loss: -1.089583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142604 INN loss: -1.142604 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126603 INN loss: -1.126603 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095506 INN loss: -1.095506 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143783 INN loss: -1.143783 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133283 INN loss: -1.133283 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119313 INN loss: -1.119313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128606 INN loss: -1.128606 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134619 INN loss: -1.134619 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110855 INN loss: -1.110855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121604 INN loss: -1.121604 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148875 INN loss: -1.148875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140991 INN loss: -1.140991 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123153 INN loss: -1.123153 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136135 INN loss: -1.136135 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120003 INN loss: -1.120003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126793 INN loss: -1.126793 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166132 INN loss: -1.166132 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188642 INN loss: -1.188642 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093684 INN loss: -1.093684 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152191 INN loss: -1.152191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117002 INN loss: -1.117002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160484 INN loss: -1.160484 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110904 INN loss: -1.110904 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166306 INN loss: -1.166306 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108721 INN loss: -1.108721 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139273 INN loss: -1.139273 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139402 INN loss: -1.139402 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159769 INN loss: -1.159769 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121598 INN loss: -1.121598 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143819 INN loss: -1.143819 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148455 INN loss: -1.148455 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116923 INN loss: -1.116923 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094243 INN loss: -1.094243 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093995 INN loss: -1.093995 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096982 INN loss: -1.096982 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149262 INN loss: -1.149262 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159018 INN loss: -1.159018 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105257 INN loss: -1.105257 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113914 INN loss: -1.113914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175361 INN loss: -1.175361 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142932 INN loss: -1.142932 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155468 INN loss: -1.155468 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133385 INN loss: -1.133385 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139877 INN loss: -1.139877 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139176 INN loss: -1.139176 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150653 INN loss: -1.150653 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112707 INN loss: -1.112707 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137062 INN loss: -1.137062 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156424 INN loss: -1.156424 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139237 INN loss: -1.139237 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152772 INN loss: -1.152772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108012 INN loss: -1.108012 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161599 INN loss: -1.161599 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121755 INN loss: -1.121755 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135869 INN loss: -1.135869 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157307 INN loss: -1.157307 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155189 INN loss: -1.155189 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127440 INN loss: -1.127440 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109576 INN loss: -1.109576 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179492 INN loss: -1.179492 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151695 INN loss: -1.151695 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153794 INN loss: -1.153794 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164221 INN loss: -1.164221 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099982 INN loss: -1.099982 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129830 INN loss: -1.129830 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187760 INN loss: -1.187760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152090 INN loss: -1.152090 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132181 INN loss: -1.132181 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134855 INN loss: -1.134855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083964 INN loss: -1.083964 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171727 INN loss: -1.171727 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138830 INN loss: -1.138830 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118935 INN loss: -1.118935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099142 INN loss: -1.099142 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140934 INN loss: -1.140934 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182426 INN loss: -1.182426 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098057 INN loss: -1.098057 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174218 INN loss: -1.174218 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128266 INN loss: -1.128266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124920 INN loss: -1.124920 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133368 INN loss: -1.133368 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139847 INN loss: -1.139847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117481 INN loss: -1.117481 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160729 INN loss: -1.160729 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130967 INN loss: -1.130967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106303 INN loss: -1.106303 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102919 INN loss: -1.102919 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127560 INN loss: -1.127560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149388 INN loss: -1.149388 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129081 INN loss: -1.129081 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139823 INN loss: -1.139823 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170552 INN loss: -1.170552 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170973 INN loss: -1.170973 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117978 INN loss: -1.117978 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174766 INN loss: -1.174766 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133498 INN loss: -1.133498 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124909 INN loss: -1.124909 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125654 INN loss: -1.125654 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184402 INN loss: -1.184402 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120675 INN loss: -1.120675 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115597 INN loss: -1.115597 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182052 INN loss: -1.182052 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153229 INN loss: -1.153229 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143944 INN loss: -1.143944 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117780 INN loss: -1.117780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.077504 INN loss: -1.077504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166250 INN loss: -1.166250 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158050 INN loss: -1.158050 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111789 INN loss: -1.111789 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.077680 INN loss: -1.077680 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181415 INN loss: -1.181415 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110053 INN loss: -1.110053 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100977 INN loss: -1.100977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135425 INN loss: -1.135425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151875 INN loss: -1.151875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144428 INN loss: -1.144428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106625 INN loss: -1.106625 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122837 INN loss: -1.122837 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160508 INN loss: -1.160508 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166965 INN loss: -1.166965 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.192066 INN loss: -1.192066 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175389 INN loss: -1.175389 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147834 INN loss: -1.147834 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124344 INN loss: -1.124344 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133221 INN loss: -1.133221 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125993 INN loss: -1.125993 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120109 INN loss: -1.120109 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137173 INN loss: -1.137173 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141446 INN loss: -1.141446 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167916 INN loss: -1.167916 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105015 INN loss: -1.105015 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122595 INN loss: -1.122595 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179652 INN loss: -1.179652 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094365 INN loss: -1.094365 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122621 INN loss: -1.122621 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176158 INN loss: -1.176158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155782 INN loss: -1.155782 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149834 INN loss: -1.149834 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101093 INN loss: -1.101093 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167428 INN loss: -1.167428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156818 INN loss: -1.156818 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128538 INN loss: -1.128538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.073934 INN loss: -1.073934 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████████████████████████████████████▍                     | 32/50 [4:14:25<30:48, 102.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.154703 INN loss: -1.154703 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150155 INN loss: -1.150155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160579 INN loss: -1.160579 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113063 INN loss: -1.113063 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147252 INN loss: -1.147252 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130446 INN loss: -1.130446 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148861 INN loss: -1.148861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158383 INN loss: -1.158383 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123085 INN loss: -1.123085 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118817 INN loss: -1.118817 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094523 INN loss: -1.094523 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159731 INN loss: -1.159731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172396 INN loss: -1.172396 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139582 INN loss: -1.139582 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125875 INN loss: -1.125875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.086292 INN loss: -1.086292 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147224 INN loss: -1.147224 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134036 INN loss: -1.134036 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145247 INN loss: -1.145247 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159850 INN loss: -1.159850 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153337 INN loss: -1.153337 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142607 INN loss: -1.142607 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149032 INN loss: -1.149032 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119926 INN loss: -1.119926 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146353 INN loss: -1.146353 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136828 INN loss: -1.136828 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155648 INN loss: -1.155648 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117671 INN loss: -1.117671 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142905 INN loss: -1.142905 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119002 INN loss: -1.119002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170530 INN loss: -1.170530 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104574 INN loss: -1.104574 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162747 INN loss: -1.162747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143236 INN loss: -1.143236 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183887 INN loss: -1.183887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164633 INN loss: -1.164633 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169532 INN loss: -1.169532 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147816 INN loss: -1.147816 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146859 INN loss: -1.146859 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.068568 INN loss: -1.068568 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108960 INN loss: -1.108960 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104049 INN loss: -1.104049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119626 INN loss: -1.119626 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171900 INN loss: -1.171900 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158426 INN loss: -1.158426 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127693 INN loss: -1.127693 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138186 INN loss: -1.138186 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146054 INN loss: -1.146054 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133252 INN loss: -1.133252 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148463 INN loss: -1.148463 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115567 INN loss: -1.115567 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185768 INN loss: -1.185768 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162079 INN loss: -1.162079 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179313 INN loss: -1.179313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129196 INN loss: -1.129196 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110219 INN loss: -1.110219 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140769 INN loss: -1.140769 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128422 INN loss: -1.128422 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173731 INN loss: -1.173731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088177 INN loss: -1.088177 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112497 INN loss: -1.112497 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092148 INN loss: -1.092148 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132188 INN loss: -1.132188 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097538 INN loss: -1.097538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122269 INN loss: -1.122269 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.072106 INN loss: -1.072106 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.079427 INN loss: -1.079427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128976 INN loss: -1.128976 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141854 INN loss: -1.141854 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142826 INN loss: -1.142826 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162454 INN loss: -1.162454 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179201 INN loss: -1.179201 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171495 INN loss: -1.171495 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135835 INN loss: -1.135835 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136962 INN loss: -1.136962 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149769 INN loss: -1.149769 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.199785 INN loss: -1.199785 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100806 INN loss: -1.100806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123833 INN loss: -1.123833 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084027 INN loss: -1.084027 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148441 INN loss: -1.148441 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104765 INN loss: -1.104765 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134142 INN loss: -1.134142 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112153 INN loss: -1.112153 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166252 INN loss: -1.166252 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126776 INN loss: -1.126776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.075288 INN loss: -1.075288 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141545 INN loss: -1.141545 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126735 INN loss: -1.126735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162351 INN loss: -1.162351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187924 INN loss: -1.187924 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119277 INN loss: -1.119277 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121753 INN loss: -1.121753 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133571 INN loss: -1.133571 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137755 INN loss: -1.137755 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119954 INN loss: -1.119954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160510 INN loss: -1.160510 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100891 INN loss: -1.100891 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151872 INN loss: -1.151872 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103876 INN loss: -1.103876 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178736 INN loss: -1.178736 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090195 INN loss: -1.090195 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158309 INN loss: -1.158309 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149128 INN loss: -1.149128 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142795 INN loss: -1.142795 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170081 INN loss: -1.170081 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114080 INN loss: -1.114080 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153295 INN loss: -1.153295 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107538 INN loss: -1.107538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162425 INN loss: -1.162425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120332 INN loss: -1.120332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.065445 INN loss: -1.065445 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132342 INN loss: -1.132342 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125334 INN loss: -1.125334 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121587 INN loss: -1.121587 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160769 INN loss: -1.160769 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121670 INN loss: -1.121670 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101004 INN loss: -1.101004 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170204 INN loss: -1.170204 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149156 INN loss: -1.149156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103648 INN loss: -1.103648 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100332 INN loss: -1.100332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118352 INN loss: -1.118352 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163261 INN loss: -1.163261 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120285 INN loss: -1.120285 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126997 INN loss: -1.126997 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173550 INN loss: -1.173550 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125259 INN loss: -1.125259 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169126 INN loss: -1.169126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152761 INN loss: -1.152761 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153407 INN loss: -1.153407 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126092 INN loss: -1.126092 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157293 INN loss: -1.157293 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177616 INN loss: -1.177616 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190946 INN loss: -1.190946 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132482 INN loss: -1.132482 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128010 INN loss: -1.128010 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121938 INN loss: -1.121938 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102274 INN loss: -1.102274 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153334 INN loss: -1.153334 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112469 INN loss: -1.112469 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116081 INN loss: -1.116081 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159688 INN loss: -1.159688 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144148 INN loss: -1.144148 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112643 INN loss: -1.112643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098985 INN loss: -1.098985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129234 INN loss: -1.129234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114204 INN loss: -1.114204 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127607 INN loss: -1.127607 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.196464 INN loss: -1.196464 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131051 INN loss: -1.131051 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186455 INN loss: -1.186455 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122679 INN loss: -1.122679 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135028 INN loss: -1.135028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132490 INN loss: -1.132490 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137994 INN loss: -1.137994 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094551 INN loss: -1.094551 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134035 INN loss: -1.134035 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121779 INN loss: -1.121779 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142379 INN loss: -1.142379 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████████████████▌                    | 33/50 [4:16:08<29:09, 102.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.109674 INN loss: -1.109674 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111885 INN loss: -1.111885 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149112 INN loss: -1.149112 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.071481 INN loss: -1.071481 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110728 INN loss: -1.110728 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149418 INN loss: -1.149418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128965 INN loss: -1.128965 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128854 INN loss: -1.128854 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130466 INN loss: -1.130466 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130775 INN loss: -1.130775 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.086357 INN loss: -1.086357 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137946 INN loss: -1.137946 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150298 INN loss: -1.150298 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184159 INN loss: -1.184159 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139608 INN loss: -1.139608 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155464 INN loss: -1.155464 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140646 INN loss: -1.140646 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170579 INN loss: -1.170579 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116059 INN loss: -1.116059 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.194284 INN loss: -1.194284 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094847 INN loss: -1.094847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124516 INN loss: -1.124516 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156184 INN loss: -1.156184 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141224 INN loss: -1.141224 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130790 INN loss: -1.130790 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157453 INN loss: -1.157453 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154305 INN loss: -1.154305 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133953 INN loss: -1.133953 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164013 INN loss: -1.164013 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163112 INN loss: -1.163112 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164288 INN loss: -1.164288 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098216 INN loss: -1.098216 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134623 INN loss: -1.134623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160276 INN loss: -1.160276 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126185 INN loss: -1.126185 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084494 INN loss: -1.084494 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155746 INN loss: -1.155746 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154690 INN loss: -1.154690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.071783 INN loss: -1.071783 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152572 INN loss: -1.152572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115183 INN loss: -1.115183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153212 INN loss: -1.153212 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140741 INN loss: -1.140741 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124949 INN loss: -1.124949 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167820 INN loss: -1.167820 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126706 INN loss: -1.126706 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186044 INN loss: -1.186044 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172760 INN loss: -1.172760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137506 INN loss: -1.137506 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141732 INN loss: -1.141732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107787 INN loss: -1.107787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152303 INN loss: -1.152303 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154808 INN loss: -1.154808 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171232 INN loss: -1.171232 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117072 INN loss: -1.117072 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124764 INN loss: -1.124764 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095925 INN loss: -1.095925 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141040 INN loss: -1.141040 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157920 INN loss: -1.157920 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167459 INN loss: -1.167459 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132635 INN loss: -1.132635 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133717 INN loss: -1.133717 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130659 INN loss: -1.130659 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116831 INN loss: -1.116831 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160240 INN loss: -1.160240 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138014 INN loss: -1.138014 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116047 INN loss: -1.116047 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123073 INN loss: -1.123073 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124207 INN loss: -1.124207 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115704 INN loss: -1.115704 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106959 INN loss: -1.106959 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087941 INN loss: -1.087941 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166971 INN loss: -1.166971 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153914 INN loss: -1.153914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121918 INN loss: -1.121918 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136344 INN loss: -1.136344 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145395 INN loss: -1.145395 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126116 INN loss: -1.126116 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117080 INN loss: -1.117080 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168056 INN loss: -1.168056 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161437 INN loss: -1.161437 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135269 INN loss: -1.135269 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.079690 INN loss: -1.079690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.210622 INN loss: -1.210622 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105887 INN loss: -1.105887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149451 INN loss: -1.149451 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117806 INN loss: -1.117806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171875 INN loss: -1.171875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156926 INN loss: -1.156926 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136540 INN loss: -1.136540 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127519 INN loss: -1.127519 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120724 INN loss: -1.120724 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146471 INN loss: -1.146471 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185117 INN loss: -1.185117 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136222 INN loss: -1.136222 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140314 INN loss: -1.140314 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165236 INN loss: -1.165236 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151640 INN loss: -1.151640 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136884 INN loss: -1.136884 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082646 INN loss: -1.082646 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.199541 INN loss: -1.199541 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149529 INN loss: -1.149529 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163142 INN loss: -1.163142 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.202360 INN loss: -1.202360 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155812 INN loss: -1.155812 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173709 INN loss: -1.173709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138015 INN loss: -1.138015 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161814 INN loss: -1.161814 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111883 INN loss: -1.111883 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129952 INN loss: -1.129952 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.210393 INN loss: -1.210393 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169498 INN loss: -1.169498 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110709 INN loss: -1.110709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133139 INN loss: -1.133139 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130301 INN loss: -1.130301 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137491 INN loss: -1.137491 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137960 INN loss: -1.137960 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131213 INN loss: -1.131213 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090279 INN loss: -1.090279 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098814 INN loss: -1.098814 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155747 INN loss: -1.155747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152685 INN loss: -1.152685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.086969 INN loss: -1.086969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167037 INN loss: -1.167037 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182974 INN loss: -1.182974 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097817 INN loss: -1.097817 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125813 INN loss: -1.125813 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116781 INN loss: -1.116781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154611 INN loss: -1.154611 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099593 INN loss: -1.099593 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141914 INN loss: -1.141914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172352 INN loss: -1.172352 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138032 INN loss: -1.138032 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176247 INN loss: -1.176247 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160119 INN loss: -1.160119 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125351 INN loss: -1.125351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178130 INN loss: -1.178130 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090936 INN loss: -1.090936 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109021 INN loss: -1.109021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140408 INN loss: -1.140408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188942 INN loss: -1.188942 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154894 INN loss: -1.154894 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173231 INN loss: -1.173231 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.077290 INN loss: -1.077290 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166919 INN loss: -1.166919 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158179 INN loss: -1.158179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148143 INN loss: -1.148143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152808 INN loss: -1.152808 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154986 INN loss: -1.154986 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133555 INN loss: -1.133555 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113112 INN loss: -1.113112 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120141 INN loss: -1.120141 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115743 INN loss: -1.115743 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116553 INN loss: -1.116553 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136669 INN loss: -1.136669 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184324 INN loss: -1.184324 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128427 INN loss: -1.128427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153210 INN loss: -1.153210 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148391 INN loss: -1.148391 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177230 INN loss: -1.177230 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████▊                   | 34/50 [4:17:58<27:58, 104.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.189196 INN loss: -1.189196 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137422 INN loss: -1.137422 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139080 INN loss: -1.139080 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138056 INN loss: -1.138056 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154910 INN loss: -1.154910 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128306 INN loss: -1.128306 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149165 INN loss: -1.149165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100240 INN loss: -1.100240 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169431 INN loss: -1.169431 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159336 INN loss: -1.159336 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151293 INN loss: -1.151293 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178108 INN loss: -1.178108 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158049 INN loss: -1.158049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131845 INN loss: -1.131845 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102897 INN loss: -1.102897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145762 INN loss: -1.145762 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108813 INN loss: -1.108813 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178645 INN loss: -1.178645 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167343 INN loss: -1.167343 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127710 INN loss: -1.127710 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113152 INN loss: -1.113152 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145414 INN loss: -1.145414 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142562 INN loss: -1.142562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138901 INN loss: -1.138901 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.070231 INN loss: -1.070231 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149754 INN loss: -1.149754 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148719 INN loss: -1.148719 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143437 INN loss: -1.143437 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142719 INN loss: -1.142719 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121780 INN loss: -1.121780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158145 INN loss: -1.158145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142030 INN loss: -1.142030 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154784 INN loss: -1.154784 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106428 INN loss: -1.106428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144598 INN loss: -1.144598 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102647 INN loss: -1.102647 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124066 INN loss: -1.124066 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080974 INN loss: -1.080974 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155870 INN loss: -1.155870 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170416 INN loss: -1.170416 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097389 INN loss: -1.097389 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138667 INN loss: -1.138667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142485 INN loss: -1.142485 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163070 INN loss: -1.163070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115727 INN loss: -1.115727 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133116 INN loss: -1.133116 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125893 INN loss: -1.125893 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131956 INN loss: -1.131956 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128049 INN loss: -1.128049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175842 INN loss: -1.175842 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131191 INN loss: -1.131191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123034 INN loss: -1.123034 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150032 INN loss: -1.150032 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156237 INN loss: -1.156237 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148110 INN loss: -1.148110 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136417 INN loss: -1.136417 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144136 INN loss: -1.144136 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138422 INN loss: -1.138422 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146235 INN loss: -1.146235 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112701 INN loss: -1.112701 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181183 INN loss: -1.181183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132110 INN loss: -1.132110 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093008 INN loss: -1.093008 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144093 INN loss: -1.144093 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130048 INN loss: -1.130048 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127010 INN loss: -1.127010 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136886 INN loss: -1.136886 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148785 INN loss: -1.148785 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137961 INN loss: -1.137961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167098 INN loss: -1.167098 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140185 INN loss: -1.140185 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118326 INN loss: -1.118326 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135179 INN loss: -1.135179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117851 INN loss: -1.117851 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188877 INN loss: -1.188877 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170731 INN loss: -1.170731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139233 INN loss: -1.139233 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160184 INN loss: -1.160184 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158391 INN loss: -1.158391 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151735 INN loss: -1.151735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102587 INN loss: -1.102587 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141699 INN loss: -1.141699 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153323 INN loss: -1.153323 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127419 INN loss: -1.127419 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170787 INN loss: -1.170787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130208 INN loss: -1.130208 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106544 INN loss: -1.106544 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112299 INN loss: -1.112299 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144428 INN loss: -1.144428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148471 INN loss: -1.148471 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125502 INN loss: -1.125502 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170723 INN loss: -1.170723 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125752 INN loss: -1.125752 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113321 INN loss: -1.113321 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141446 INN loss: -1.141446 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106750 INN loss: -1.106750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161703 INN loss: -1.161703 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149685 INN loss: -1.149685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117323 INN loss: -1.117323 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126139 INN loss: -1.126139 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.212139 INN loss: -1.212139 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147142 INN loss: -1.147142 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118795 INN loss: -1.118795 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107718 INN loss: -1.107718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140854 INN loss: -1.140854 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110162 INN loss: -1.110162 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122422 INN loss: -1.122422 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167758 INN loss: -1.167758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132697 INN loss: -1.132697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161356 INN loss: -1.161356 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098246 INN loss: -1.098246 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076396 INN loss: -1.076396 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156538 INN loss: -1.156538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093872 INN loss: -1.093872 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085880 INN loss: -1.085880 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155987 INN loss: -1.155987 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125978 INN loss: -1.125978 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125967 INN loss: -1.125967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107324 INN loss: -1.107324 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156632 INN loss: -1.156632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152021 INN loss: -1.152021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151264 INN loss: -1.151264 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153367 INN loss: -1.153367 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141509 INN loss: -1.141509 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176090 INN loss: -1.176090 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137753 INN loss: -1.137753 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140169 INN loss: -1.140169 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160461 INN loss: -1.160461 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171566 INN loss: -1.171566 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120010 INN loss: -1.120010 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187064 INN loss: -1.187064 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162098 INN loss: -1.162098 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126701 INN loss: -1.126701 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122706 INN loss: -1.122706 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114715 INN loss: -1.114715 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146876 INN loss: -1.146876 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161462 INN loss: -1.161462 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129539 INN loss: -1.129539 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120241 INN loss: -1.120241 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130346 INN loss: -1.130346 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118787 INN loss: -1.118787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134469 INN loss: -1.134469 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132121 INN loss: -1.132121 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125760 INN loss: -1.125760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138198 INN loss: -1.138198 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124555 INN loss: -1.124555 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116046 INN loss: -1.116046 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108975 INN loss: -1.108975 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130635 INN loss: -1.130635 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141305 INN loss: -1.141305 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138294 INN loss: -1.138294 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136912 INN loss: -1.136912 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140688 INN loss: -1.140688 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141906 INN loss: -1.141906 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143654 INN loss: -1.143654 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173207 INN loss: -1.173207 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123853 INN loss: -1.123853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080047 INN loss: -1.080047 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120771 INN loss: -1.120771 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183314 INN loss: -1.183314 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████                  | 35/50 [4:19:28<25:04, 100.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.127967 INN loss: -1.127967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111628 INN loss: -1.111628 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154514 INN loss: -1.154514 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120788 INN loss: -1.120788 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178236 INN loss: -1.178236 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153434 INN loss: -1.153434 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156852 INN loss: -1.156852 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112332 INN loss: -1.112332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112982 INN loss: -1.112982 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146966 INN loss: -1.146966 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127847 INN loss: -1.127847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141355 INN loss: -1.141355 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119600 INN loss: -1.119600 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137676 INN loss: -1.137676 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095741 INN loss: -1.095741 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114151 INN loss: -1.114151 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152977 INN loss: -1.152977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120621 INN loss: -1.120621 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184583 INN loss: -1.184583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089855 INN loss: -1.089855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116799 INN loss: -1.116799 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154218 INN loss: -1.154218 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133996 INN loss: -1.133996 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126887 INN loss: -1.126887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128817 INN loss: -1.128817 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157286 INN loss: -1.157286 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151601 INN loss: -1.151601 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098118 INN loss: -1.098118 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131305 INN loss: -1.131305 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133623 INN loss: -1.133623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157996 INN loss: -1.157996 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116811 INN loss: -1.116811 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155562 INN loss: -1.155562 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130681 INN loss: -1.130681 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152606 INN loss: -1.152606 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120774 INN loss: -1.120774 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138082 INN loss: -1.138082 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.079721 INN loss: -1.079721 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172523 INN loss: -1.172523 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139050 INN loss: -1.139050 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178927 INN loss: -1.178927 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151653 INN loss: -1.151653 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174499 INN loss: -1.174499 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142624 INN loss: -1.142624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123322 INN loss: -1.123322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104617 INN loss: -1.104617 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125449 INN loss: -1.125449 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098008 INN loss: -1.098008 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137905 INN loss: -1.137905 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126306 INN loss: -1.126306 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085501 INN loss: -1.085501 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152993 INN loss: -1.152993 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181340 INN loss: -1.181340 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094707 INN loss: -1.094707 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126555 INN loss: -1.126555 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140346 INN loss: -1.140346 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112795 INN loss: -1.112795 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140423 INN loss: -1.140423 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129500 INN loss: -1.129500 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151788 INN loss: -1.151788 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166935 INN loss: -1.166935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117781 INN loss: -1.117781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098748 INN loss: -1.098748 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153014 INN loss: -1.153014 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137234 INN loss: -1.137234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145813 INN loss: -1.145813 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083474 INN loss: -1.083474 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150899 INN loss: -1.150899 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095506 INN loss: -1.095506 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115353 INN loss: -1.115353 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150667 INN loss: -1.150667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163112 INN loss: -1.163112 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152796 INN loss: -1.152796 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121191 INN loss: -1.121191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105914 INN loss: -1.105914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146332 INN loss: -1.146332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126886 INN loss: -1.126886 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122878 INN loss: -1.122878 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168865 INN loss: -1.168865 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128545 INN loss: -1.128545 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131691 INN loss: -1.131691 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119002 INN loss: -1.119002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141012 INN loss: -1.141012 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160379 INN loss: -1.160379 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127931 INN loss: -1.127931 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146866 INN loss: -1.146866 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120404 INN loss: -1.120404 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144896 INN loss: -1.144896 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132900 INN loss: -1.132900 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153926 INN loss: -1.153926 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083329 INN loss: -1.083329 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147708 INN loss: -1.147708 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114172 INN loss: -1.114172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083226 INN loss: -1.083226 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127361 INN loss: -1.127361 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103405 INN loss: -1.103405 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117960 INN loss: -1.117960 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142265 INN loss: -1.142265 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140953 INN loss: -1.140953 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120370 INN loss: -1.120370 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150828 INN loss: -1.150828 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148496 INN loss: -1.148496 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118833 INN loss: -1.118833 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126897 INN loss: -1.126897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125209 INN loss: -1.125209 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110786 INN loss: -1.110786 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161358 INN loss: -1.161358 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107390 INN loss: -1.107390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085317 INN loss: -1.085317 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089418 INN loss: -1.089418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132371 INN loss: -1.132371 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147479 INN loss: -1.147479 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123681 INN loss: -1.123681 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140732 INN loss: -1.140732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139175 INN loss: -1.139175 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117454 INN loss: -1.117454 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112833 INN loss: -1.112833 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132097 INN loss: -1.132097 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150058 INN loss: -1.150058 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162556 INN loss: -1.162556 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121964 INN loss: -1.121964 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110158 INN loss: -1.110158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133927 INN loss: -1.133927 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103009 INN loss: -1.103009 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134779 INN loss: -1.134779 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099515 INN loss: -1.099515 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114807 INN loss: -1.114807 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146610 INN loss: -1.146610 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148668 INN loss: -1.148668 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117626 INN loss: -1.117626 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151259 INN loss: -1.151259 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134785 INN loss: -1.134785 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117598 INN loss: -1.117598 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109892 INN loss: -1.109892 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125157 INN loss: -1.125157 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132873 INN loss: -1.132873 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120992 INN loss: -1.120992 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118432 INN loss: -1.118432 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104810 INN loss: -1.104810 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154438 INN loss: -1.154438 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143837 INN loss: -1.143837 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096760 INN loss: -1.096760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125305 INN loss: -1.125305 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141513 INN loss: -1.141513 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141503 INN loss: -1.141503 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121819 INN loss: -1.121819 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094220 INN loss: -1.094220 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139785 INN loss: -1.139785 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144789 INN loss: -1.144789 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144698 INN loss: -1.144698 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146731 INN loss: -1.146731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132454 INN loss: -1.132454 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138085 INN loss: -1.138085 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121661 INN loss: -1.121661 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157271 INN loss: -1.157271 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139344 INN loss: -1.139344 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148672 INN loss: -1.148672 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119855 INN loss: -1.119855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151447 INN loss: -1.151447 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152047 INN loss: -1.152047 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████▉                 | 36/50 [4:20:57<22:38, 97.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.165696 INN loss: -1.165696 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161230 INN loss: -1.161230 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139847 INN loss: -1.139847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.214666 INN loss: -1.214666 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154304 INN loss: -1.154304 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140878 INN loss: -1.140878 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133193 INN loss: -1.133193 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115119 INN loss: -1.115119 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138345 INN loss: -1.138345 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146230 INN loss: -1.146230 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159243 INN loss: -1.159243 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.205351 INN loss: -1.205351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105944 INN loss: -1.105944 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120733 INN loss: -1.120733 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158679 INN loss: -1.158679 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182659 INN loss: -1.182659 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154480 INN loss: -1.154480 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150124 INN loss: -1.150124 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150929 INN loss: -1.150929 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114912 INN loss: -1.114912 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131685 INN loss: -1.131685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164632 INN loss: -1.164632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114352 INN loss: -1.114352 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125526 INN loss: -1.125526 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147923 INN loss: -1.147923 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143170 INN loss: -1.143170 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127425 INN loss: -1.127425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136221 INN loss: -1.136221 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107389 INN loss: -1.107389 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100792 INN loss: -1.100792 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109874 INN loss: -1.109874 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112823 INN loss: -1.112823 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135434 INN loss: -1.135434 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097210 INN loss: -1.097210 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186140 INN loss: -1.186140 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110639 INN loss: -1.110639 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134983 INN loss: -1.134983 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119070 INN loss: -1.119070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116529 INN loss: -1.116529 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097374 INN loss: -1.097374 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173655 INN loss: -1.173655 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166895 INN loss: -1.166895 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125644 INN loss: -1.125644 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177339 INN loss: -1.177339 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107039 INN loss: -1.107039 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137105 INN loss: -1.137105 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118540 INN loss: -1.118540 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157966 INN loss: -1.157966 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140396 INN loss: -1.140396 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136006 INN loss: -1.136006 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104985 INN loss: -1.104985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138837 INN loss: -1.138837 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141625 INN loss: -1.141625 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129242 INN loss: -1.129242 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159347 INN loss: -1.159347 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103398 INN loss: -1.103398 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133867 INN loss: -1.133867 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114301 INN loss: -1.114301 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091462 INN loss: -1.091462 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099184 INN loss: -1.099184 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121018 INN loss: -1.121018 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156424 INN loss: -1.156424 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139626 INN loss: -1.139626 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156919 INN loss: -1.156919 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189800 INN loss: -1.189800 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152489 INN loss: -1.152489 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113266 INN loss: -1.113266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152029 INN loss: -1.152029 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140934 INN loss: -1.140934 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106841 INN loss: -1.106841 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146923 INN loss: -1.146923 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149467 INN loss: -1.149467 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112800 INN loss: -1.112800 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149238 INN loss: -1.149238 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148614 INN loss: -1.148614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180468 INN loss: -1.180468 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136191 INN loss: -1.136191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150235 INN loss: -1.150235 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103918 INN loss: -1.103918 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122294 INN loss: -1.122294 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171835 INN loss: -1.171835 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152131 INN loss: -1.152131 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168979 INN loss: -1.168979 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129154 INN loss: -1.129154 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115502 INN loss: -1.115502 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133018 INN loss: -1.133018 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175364 INN loss: -1.175364 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164237 INN loss: -1.164237 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115281 INN loss: -1.115281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144106 INN loss: -1.144106 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131456 INN loss: -1.131456 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167732 INN loss: -1.167732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158069 INN loss: -1.158069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138414 INN loss: -1.138414 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132280 INN loss: -1.132280 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117895 INN loss: -1.117895 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106257 INN loss: -1.106257 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179626 INN loss: -1.179626 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181261 INN loss: -1.181261 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154448 INN loss: -1.154448 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136969 INN loss: -1.136969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145084 INN loss: -1.145084 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160062 INN loss: -1.160062 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120570 INN loss: -1.120570 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145483 INN loss: -1.145483 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143435 INN loss: -1.143435 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131265 INN loss: -1.131265 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141475 INN loss: -1.141475 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154764 INN loss: -1.154764 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153643 INN loss: -1.153643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.210535 INN loss: -1.210535 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149920 INN loss: -1.149920 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100633 INN loss: -1.100633 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106833 INN loss: -1.106833 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125534 INN loss: -1.125534 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142181 INN loss: -1.142181 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143057 INN loss: -1.143057 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171050 INN loss: -1.171050 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161445 INN loss: -1.161445 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143342 INN loss: -1.143342 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108178 INN loss: -1.108178 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098225 INN loss: -1.098225 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134699 INN loss: -1.134699 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113062 INN loss: -1.113062 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119199 INN loss: -1.119199 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150931 INN loss: -1.150931 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149049 INN loss: -1.149049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128406 INN loss: -1.128406 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146412 INN loss: -1.146412 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149363 INN loss: -1.149363 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141796 INN loss: -1.141796 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132346 INN loss: -1.132346 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153319 INN loss: -1.153319 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134506 INN loss: -1.134506 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133327 INN loss: -1.133327 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111485 INN loss: -1.111485 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115429 INN loss: -1.115429 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131935 INN loss: -1.131935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139097 INN loss: -1.139097 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152292 INN loss: -1.152292 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133026 INN loss: -1.133026 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101458 INN loss: -1.101458 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158738 INN loss: -1.158738 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135977 INN loss: -1.135977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155221 INN loss: -1.155221 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118205 INN loss: -1.118205 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140621 INN loss: -1.140621 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113799 INN loss: -1.113799 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138405 INN loss: -1.138405 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082613 INN loss: -1.082613 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152191 INN loss: -1.152191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153866 INN loss: -1.153866 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138755 INN loss: -1.138755 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136379 INN loss: -1.136379 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128857 INN loss: -1.128857 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126591 INN loss: -1.126591 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103207 INN loss: -1.103207 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127215 INN loss: -1.127215 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.194676 INN loss: -1.194676 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141917 INN loss: -1.141917 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|█████████████████████████████████████████████▏               | 37/50 [4:22:27<20:33, 94.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.148411 INN loss: -1.148411 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171022 INN loss: -1.171022 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162747 INN loss: -1.162747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135570 INN loss: -1.135570 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135007 INN loss: -1.135007 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154341 INN loss: -1.154341 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117628 INN loss: -1.117628 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180093 INN loss: -1.180093 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149986 INN loss: -1.149986 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112517 INN loss: -1.112517 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108064 INN loss: -1.108064 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105435 INN loss: -1.105435 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142302 INN loss: -1.142302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154460 INN loss: -1.154460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115852 INN loss: -1.115852 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136312 INN loss: -1.136312 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146445 INN loss: -1.146445 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151969 INN loss: -1.151969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104530 INN loss: -1.104530 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150889 INN loss: -1.150889 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.073555 INN loss: -1.073555 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135738 INN loss: -1.135738 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162954 INN loss: -1.162954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168418 INN loss: -1.168418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131221 INN loss: -1.131221 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100010 INN loss: -1.100010 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165950 INN loss: -1.165950 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109338 INN loss: -1.109338 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150594 INN loss: -1.150594 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154556 INN loss: -1.154556 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128867 INN loss: -1.128867 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124670 INN loss: -1.124670 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150468 INN loss: -1.150468 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154363 INN loss: -1.154363 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131823 INN loss: -1.131823 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161214 INN loss: -1.161214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143543 INN loss: -1.143543 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139219 INN loss: -1.139219 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148281 INN loss: -1.148281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114915 INN loss: -1.114915 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094682 INN loss: -1.094682 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142457 INN loss: -1.142457 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171608 INN loss: -1.171608 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142088 INN loss: -1.142088 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153416 INN loss: -1.153416 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137529 INN loss: -1.137529 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117755 INN loss: -1.117755 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163768 INN loss: -1.163768 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139631 INN loss: -1.139631 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111216 INN loss: -1.111216 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132083 INN loss: -1.132083 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150484 INN loss: -1.150484 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142948 INN loss: -1.142948 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159961 INN loss: -1.159961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105533 INN loss: -1.105533 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143260 INN loss: -1.143260 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102304 INN loss: -1.102304 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137198 INN loss: -1.137198 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150788 INN loss: -1.150788 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167861 INN loss: -1.167861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093207 INN loss: -1.093207 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125536 INN loss: -1.125536 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103523 INN loss: -1.103523 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103849 INN loss: -1.103849 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189065 INN loss: -1.189065 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134677 INN loss: -1.134677 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147487 INN loss: -1.147487 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135737 INN loss: -1.135737 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145735 INN loss: -1.145735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150660 INN loss: -1.150660 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132977 INN loss: -1.132977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176480 INN loss: -1.176480 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084495 INN loss: -1.084495 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163586 INN loss: -1.163586 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178222 INN loss: -1.178222 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162556 INN loss: -1.162556 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126133 INN loss: -1.126133 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132266 INN loss: -1.132266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126365 INN loss: -1.126365 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173311 INN loss: -1.173311 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171805 INN loss: -1.171805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145740 INN loss: -1.145740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123723 INN loss: -1.123723 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126281 INN loss: -1.126281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139912 INN loss: -1.139912 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110608 INN loss: -1.110608 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183223 INN loss: -1.183223 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123330 INN loss: -1.123330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125420 INN loss: -1.125420 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175648 INN loss: -1.175648 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136564 INN loss: -1.136564 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173253 INN loss: -1.173253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129685 INN loss: -1.129685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.079582 INN loss: -1.079582 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160082 INN loss: -1.160082 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159102 INN loss: -1.159102 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118114 INN loss: -1.118114 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157712 INN loss: -1.157712 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124920 INN loss: -1.124920 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160511 INN loss: -1.160511 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114505 INN loss: -1.114505 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143091 INN loss: -1.143091 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136952 INN loss: -1.136952 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101869 INN loss: -1.101869 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137464 INN loss: -1.137464 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158110 INN loss: -1.158110 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089685 INN loss: -1.089685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102864 INN loss: -1.102864 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155480 INN loss: -1.155480 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128670 INN loss: -1.128670 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117091 INN loss: -1.117091 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159415 INN loss: -1.159415 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162325 INN loss: -1.162325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099270 INN loss: -1.099270 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139592 INN loss: -1.139592 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161378 INN loss: -1.161378 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156877 INN loss: -1.156877 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122487 INN loss: -1.122487 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166759 INN loss: -1.166759 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126877 INN loss: -1.126877 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149014 INN loss: -1.149014 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120602 INN loss: -1.120602 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132979 INN loss: -1.132979 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150087 INN loss: -1.150087 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122301 INN loss: -1.122301 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179245 INN loss: -1.179245 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136795 INN loss: -1.136795 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123536 INN loss: -1.123536 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129113 INN loss: -1.129113 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126408 INN loss: -1.126408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133327 INN loss: -1.133327 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159155 INN loss: -1.159155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105848 INN loss: -1.105848 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149785 INN loss: -1.149785 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133125 INN loss: -1.133125 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129267 INN loss: -1.129267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122778 INN loss: -1.122778 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099970 INN loss: -1.099970 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121289 INN loss: -1.121289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133901 INN loss: -1.133901 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147024 INN loss: -1.147024 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128495 INN loss: -1.128495 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132097 INN loss: -1.132097 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165496 INN loss: -1.165496 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135021 INN loss: -1.135021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113269 INN loss: -1.113269 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166385 INN loss: -1.166385 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119871 INN loss: -1.119871 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150646 INN loss: -1.150646 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115098 INN loss: -1.115098 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171843 INN loss: -1.171843 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120164 INN loss: -1.120164 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108896 INN loss: -1.108896 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120776 INN loss: -1.120776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.197156 INN loss: -1.197156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112031 INN loss: -1.112031 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145169 INN loss: -1.145169 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157643 INN loss: -1.157643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152393 INN loss: -1.152393 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101640 INN loss: -1.101640 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████▎              | 38/50 [4:23:57<18:41, 93.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.151992 INN loss: -1.151992 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138344 INN loss: -1.138344 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.220099 INN loss: -1.220099 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117787 INN loss: -1.117787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171794 INN loss: -1.171794 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123544 INN loss: -1.123544 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144628 INN loss: -1.144628 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158717 INN loss: -1.158717 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136713 INN loss: -1.136713 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131499 INN loss: -1.131499 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151897 INN loss: -1.151897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127596 INN loss: -1.127596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.054809 INN loss: -1.054809 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112264 INN loss: -1.112264 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109920 INN loss: -1.109920 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154072 INN loss: -1.154072 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111782 INN loss: -1.111782 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089144 INN loss: -1.089144 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108054 INN loss: -1.108054 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154213 INN loss: -1.154213 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122991 INN loss: -1.122991 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157507 INN loss: -1.157507 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124850 INN loss: -1.124850 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094847 INN loss: -1.094847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134271 INN loss: -1.134271 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166596 INN loss: -1.166596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170365 INN loss: -1.170365 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156251 INN loss: -1.156251 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183935 INN loss: -1.183935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174152 INN loss: -1.174152 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117227 INN loss: -1.117227 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133931 INN loss: -1.133931 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168508 INN loss: -1.168508 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131764 INN loss: -1.131764 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107368 INN loss: -1.107368 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150093 INN loss: -1.150093 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158752 INN loss: -1.158752 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165948 INN loss: -1.165948 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131474 INN loss: -1.131474 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118959 INN loss: -1.118959 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101088 INN loss: -1.101088 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140707 INN loss: -1.140707 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107076 INN loss: -1.107076 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146192 INN loss: -1.146192 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135669 INN loss: -1.135669 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173126 INN loss: -1.173126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146267 INN loss: -1.146267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154416 INN loss: -1.154416 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141700 INN loss: -1.141700 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131734 INN loss: -1.131734 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145840 INN loss: -1.145840 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.069335 INN loss: -1.069335 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149219 INN loss: -1.149219 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142311 INN loss: -1.142311 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145758 INN loss: -1.145758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157855 INN loss: -1.157855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.202189 INN loss: -1.202189 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113291 INN loss: -1.113291 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141004 INN loss: -1.141004 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111839 INN loss: -1.111839 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136718 INN loss: -1.136718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132590 INN loss: -1.132590 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147195 INN loss: -1.147195 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.192520 INN loss: -1.192520 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180159 INN loss: -1.180159 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117450 INN loss: -1.117450 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161328 INN loss: -1.161328 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104957 INN loss: -1.104957 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157380 INN loss: -1.157380 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096028 INN loss: -1.096028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132130 INN loss: -1.132130 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136479 INN loss: -1.136479 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115739 INN loss: -1.115739 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122196 INN loss: -1.122196 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174003 INN loss: -1.174003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152082 INN loss: -1.152082 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150442 INN loss: -1.150442 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115569 INN loss: -1.115569 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152706 INN loss: -1.152706 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140757 INN loss: -1.140757 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130955 INN loss: -1.130955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132186 INN loss: -1.132186 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129538 INN loss: -1.129538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145804 INN loss: -1.145804 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115627 INN loss: -1.115627 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145719 INN loss: -1.145719 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155265 INN loss: -1.155265 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142682 INN loss: -1.142682 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.196550 INN loss: -1.196550 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133052 INN loss: -1.133052 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137343 INN loss: -1.137343 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153878 INN loss: -1.153878 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145466 INN loss: -1.145466 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134643 INN loss: -1.134643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141649 INN loss: -1.141649 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175049 INN loss: -1.175049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170980 INN loss: -1.170980 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151551 INN loss: -1.151551 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101748 INN loss: -1.101748 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076423 INN loss: -1.076423 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136622 INN loss: -1.136622 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132011 INN loss: -1.132011 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123274 INN loss: -1.123274 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130712 INN loss: -1.130712 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142586 INN loss: -1.142586 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106897 INN loss: -1.106897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092060 INN loss: -1.092060 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149003 INN loss: -1.149003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103720 INN loss: -1.103720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152719 INN loss: -1.152719 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119942 INN loss: -1.119942 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164642 INN loss: -1.164642 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177456 INN loss: -1.177456 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164178 INN loss: -1.164178 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164065 INN loss: -1.164065 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108067 INN loss: -1.108067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136167 INN loss: -1.136167 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137231 INN loss: -1.137231 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147568 INN loss: -1.147568 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152700 INN loss: -1.152700 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150877 INN loss: -1.150877 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121291 INN loss: -1.121291 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135322 INN loss: -1.135322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135340 INN loss: -1.135340 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105341 INN loss: -1.105341 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161000 INN loss: -1.161000 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131363 INN loss: -1.131363 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158527 INN loss: -1.158527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141686 INN loss: -1.141686 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126236 INN loss: -1.126236 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120154 INN loss: -1.120154 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096823 INN loss: -1.096823 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125011 INN loss: -1.125011 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126381 INN loss: -1.126381 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128717 INN loss: -1.128717 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124241 INN loss: -1.124241 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112373 INN loss: -1.112373 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143492 INN loss: -1.143492 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130857 INN loss: -1.130857 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184711 INN loss: -1.184711 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163103 INN loss: -1.163103 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159337 INN loss: -1.159337 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160961 INN loss: -1.160961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145510 INN loss: -1.145510 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110795 INN loss: -1.110795 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149580 INN loss: -1.149580 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155479 INN loss: -1.155479 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140761 INN loss: -1.140761 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121247 INN loss: -1.121247 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108542 INN loss: -1.108542 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121069 INN loss: -1.121069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128619 INN loss: -1.128619 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134104 INN loss: -1.134104 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126019 INN loss: -1.126019 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114095 INN loss: -1.114095 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145328 INN loss: -1.145328 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168560 INN loss: -1.168560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122705 INN loss: -1.122705 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090148 INN loss: -1.090148 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100896 INN loss: -1.100896 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████▌             | 39/50 [4:25:27<16:56, 92.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.128596 INN loss: -1.128596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107367 INN loss: -1.107367 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110591 INN loss: -1.110591 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107827 INN loss: -1.107827 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140913 INN loss: -1.140913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129503 INN loss: -1.129503 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104232 INN loss: -1.104232 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106824 INN loss: -1.106824 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134822 INN loss: -1.134822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128707 INN loss: -1.128707 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188208 INN loss: -1.188208 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147050 INN loss: -1.147050 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146579 INN loss: -1.146579 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147650 INN loss: -1.147650 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149207 INN loss: -1.149207 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123115 INN loss: -1.123115 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149519 INN loss: -1.149519 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178778 INN loss: -1.178778 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151413 INN loss: -1.151413 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158047 INN loss: -1.158047 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135634 INN loss: -1.135634 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136863 INN loss: -1.136863 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166117 INN loss: -1.166117 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130988 INN loss: -1.130988 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141384 INN loss: -1.141384 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141342 INN loss: -1.141342 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.043518 INN loss: -1.043518 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145772 INN loss: -1.145772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106528 INN loss: -1.106528 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158145 INN loss: -1.158145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112035 INN loss: -1.112035 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123102 INN loss: -1.123102 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121013 INN loss: -1.121013 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.074179 INN loss: -1.074179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115158 INN loss: -1.115158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108690 INN loss: -1.108690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111923 INN loss: -1.111923 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169362 INN loss: -1.169362 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122442 INN loss: -1.122442 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190914 INN loss: -1.190914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152718 INN loss: -1.152718 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166630 INN loss: -1.166630 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156073 INN loss: -1.156073 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116105 INN loss: -1.116105 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159987 INN loss: -1.159987 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135413 INN loss: -1.135413 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148042 INN loss: -1.148042 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136309 INN loss: -1.136309 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.207945 INN loss: -1.207945 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145630 INN loss: -1.145630 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.073822 INN loss: -1.073822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087219 INN loss: -1.087219 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135333 INN loss: -1.135333 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135999 INN loss: -1.135999 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117094 INN loss: -1.117094 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163214 INN loss: -1.163214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101964 INN loss: -1.101964 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153560 INN loss: -1.153560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174387 INN loss: -1.174387 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139986 INN loss: -1.139986 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135633 INN loss: -1.135633 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165915 INN loss: -1.165915 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153318 INN loss: -1.153318 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.068213 INN loss: -1.068213 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108988 INN loss: -1.108988 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128960 INN loss: -1.128960 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153772 INN loss: -1.153772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106043 INN loss: -1.106043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135999 INN loss: -1.135999 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167585 INN loss: -1.167585 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127065 INN loss: -1.127065 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164165 INN loss: -1.164165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097317 INN loss: -1.097317 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152488 INN loss: -1.152488 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153950 INN loss: -1.153950 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113321 INN loss: -1.113321 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132703 INN loss: -1.132703 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138811 INN loss: -1.138811 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154460 INN loss: -1.154460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109728 INN loss: -1.109728 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114122 INN loss: -1.114122 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141784 INN loss: -1.141784 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144028 INN loss: -1.144028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150863 INN loss: -1.150863 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143545 INN loss: -1.143545 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122812 INN loss: -1.122812 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172685 INN loss: -1.172685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106294 INN loss: -1.106294 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131697 INN loss: -1.131697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109099 INN loss: -1.109099 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162496 INN loss: -1.162496 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152178 INN loss: -1.152178 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146369 INN loss: -1.146369 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113292 INN loss: -1.113292 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151107 INN loss: -1.151107 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171174 INN loss: -1.171174 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122745 INN loss: -1.122745 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114476 INN loss: -1.114476 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146306 INN loss: -1.146306 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129514 INN loss: -1.129514 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136561 INN loss: -1.136561 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136425 INN loss: -1.136425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156605 INN loss: -1.156605 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158542 INN loss: -1.158542 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177798 INN loss: -1.177798 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133359 INN loss: -1.133359 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155958 INN loss: -1.155958 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137015 INN loss: -1.137015 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139847 INN loss: -1.139847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167707 INN loss: -1.167707 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174836 INN loss: -1.174836 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147069 INN loss: -1.147069 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137284 INN loss: -1.137284 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.070085 INN loss: -1.070085 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141012 INN loss: -1.141012 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140822 INN loss: -1.140822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155711 INN loss: -1.155711 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142756 INN loss: -1.142756 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165112 INN loss: -1.165112 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143626 INN loss: -1.143626 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135888 INN loss: -1.135888 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091509 INN loss: -1.091509 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126272 INN loss: -1.126272 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112964 INN loss: -1.112964 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166036 INN loss: -1.166036 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156158 INN loss: -1.156158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151175 INN loss: -1.151175 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091183 INN loss: -1.091183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165338 INN loss: -1.165338 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136186 INN loss: -1.136186 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126644 INN loss: -1.126644 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128700 INN loss: -1.128700 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134931 INN loss: -1.134931 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103653 INN loss: -1.103653 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130333 INN loss: -1.130333 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175847 INN loss: -1.175847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153479 INN loss: -1.153479 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129512 INN loss: -1.129512 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145113 INN loss: -1.145113 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168158 INN loss: -1.168158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130451 INN loss: -1.130451 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112332 INN loss: -1.112332 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.086503 INN loss: -1.086503 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134272 INN loss: -1.134272 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127538 INN loss: -1.127538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115471 INN loss: -1.115471 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131509 INN loss: -1.131509 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127521 INN loss: -1.127521 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142003 INN loss: -1.142003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115748 INN loss: -1.115748 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146173 INN loss: -1.146173 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134126 INN loss: -1.134126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139362 INN loss: -1.139362 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112551 INN loss: -1.112551 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105478 INN loss: -1.105478 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156043 INN loss: -1.156043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.073142 INN loss: -1.073142 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130938 INN loss: -1.130938 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125893 INN loss: -1.125893 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109078 INN loss: -1.109078 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████▊            | 40/50 [4:27:06<15:45, 94.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.103812 INN loss: -1.103812 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135556 INN loss: -1.135556 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104325 INN loss: -1.104325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112818 INN loss: -1.112818 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140096 INN loss: -1.140096 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152550 INN loss: -1.152550 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155025 INN loss: -1.155025 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167783 INN loss: -1.167783 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128467 INN loss: -1.128467 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145683 INN loss: -1.145683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141223 INN loss: -1.141223 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099910 INN loss: -1.099910 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135440 INN loss: -1.135440 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128254 INN loss: -1.128254 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135567 INN loss: -1.135567 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090833 INN loss: -1.090833 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133596 INN loss: -1.133596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139983 INN loss: -1.139983 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096533 INN loss: -1.096533 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111198 INN loss: -1.111198 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116759 INN loss: -1.116759 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125064 INN loss: -1.125064 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162540 INN loss: -1.162540 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122988 INN loss: -1.122988 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127167 INN loss: -1.127167 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.197801 INN loss: -1.197801 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151724 INN loss: -1.151724 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152403 INN loss: -1.152403 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128940 INN loss: -1.128940 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146881 INN loss: -1.146881 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148261 INN loss: -1.148261 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139980 INN loss: -1.139980 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151985 INN loss: -1.151985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112979 INN loss: -1.112979 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138118 INN loss: -1.138118 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147697 INN loss: -1.147697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136759 INN loss: -1.136759 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.197787 INN loss: -1.197787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147753 INN loss: -1.147753 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154033 INN loss: -1.154033 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131440 INN loss: -1.131440 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080143 INN loss: -1.080143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108530 INN loss: -1.108530 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124050 INN loss: -1.124050 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154694 INN loss: -1.154694 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108788 INN loss: -1.108788 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161949 INN loss: -1.161949 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108155 INN loss: -1.108155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147853 INN loss: -1.147853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107172 INN loss: -1.107172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122898 INN loss: -1.122898 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133903 INN loss: -1.133903 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125306 INN loss: -1.125306 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121790 INN loss: -1.121790 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113626 INN loss: -1.113626 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128408 INN loss: -1.128408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153913 INN loss: -1.153913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168807 INN loss: -1.168807 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173900 INN loss: -1.173900 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124670 INN loss: -1.124670 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121787 INN loss: -1.121787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155502 INN loss: -1.155502 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143661 INN loss: -1.143661 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111118 INN loss: -1.111118 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141081 INN loss: -1.141081 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148827 INN loss: -1.148827 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169647 INN loss: -1.169647 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162772 INN loss: -1.162772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145254 INN loss: -1.145254 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157651 INN loss: -1.157651 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096652 INN loss: -1.096652 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130694 INN loss: -1.130694 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187492 INN loss: -1.187492 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170720 INN loss: -1.170720 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115602 INN loss: -1.115602 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152477 INN loss: -1.152477 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104382 INN loss: -1.104382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147767 INN loss: -1.147767 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157153 INN loss: -1.157153 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150293 INN loss: -1.150293 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157423 INN loss: -1.157423 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122929 INN loss: -1.122929 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154225 INN loss: -1.154225 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177288 INN loss: -1.177288 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129831 INN loss: -1.129831 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111990 INN loss: -1.111990 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127115 INN loss: -1.127115 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138408 INN loss: -1.138408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147927 INN loss: -1.147927 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106243 INN loss: -1.106243 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097321 INN loss: -1.097321 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126970 INN loss: -1.126970 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120446 INN loss: -1.120446 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087677 INN loss: -1.087677 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137560 INN loss: -1.137560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126608 INN loss: -1.126608 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136990 INN loss: -1.136990 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138888 INN loss: -1.138888 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148745 INN loss: -1.148745 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.191933 INN loss: -1.191933 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.071609 INN loss: -1.071609 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113964 INN loss: -1.113964 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098215 INN loss: -1.098215 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139788 INN loss: -1.139788 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119013 INN loss: -1.119013 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156850 INN loss: -1.156850 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125889 INN loss: -1.125889 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133991 INN loss: -1.133991 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134089 INN loss: -1.134089 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143887 INN loss: -1.143887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159961 INN loss: -1.159961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122697 INN loss: -1.122697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125864 INN loss: -1.125864 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145032 INN loss: -1.145032 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147342 INN loss: -1.147342 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129163 INN loss: -1.129163 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136168 INN loss: -1.136168 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131644 INN loss: -1.131644 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183583 INN loss: -1.183583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107945 INN loss: -1.107945 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155385 INN loss: -1.155385 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119158 INN loss: -1.119158 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181885 INN loss: -1.181885 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147585 INN loss: -1.147585 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150600 INN loss: -1.150600 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157066 INN loss: -1.157066 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081877 INN loss: -1.081877 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134240 INN loss: -1.134240 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122134 INN loss: -1.122134 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119503 INN loss: -1.119503 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116226 INN loss: -1.116226 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126978 INN loss: -1.126978 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142981 INN loss: -1.142981 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175910 INN loss: -1.175910 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164043 INN loss: -1.164043 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.196008 INN loss: -1.196008 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152310 INN loss: -1.152310 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141503 INN loss: -1.141503 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124807 INN loss: -1.124807 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189095 INN loss: -1.189095 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154313 INN loss: -1.154313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187969 INN loss: -1.187969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150131 INN loss: -1.150131 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128663 INN loss: -1.128663 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137981 INN loss: -1.137981 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131762 INN loss: -1.131762 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157256 INN loss: -1.157256 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149815 INN loss: -1.149815 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124159 INN loss: -1.124159 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.061423 INN loss: -1.061423 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181681 INN loss: -1.181681 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177917 INN loss: -1.177917 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142968 INN loss: -1.142968 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134203 INN loss: -1.134203 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131276 INN loss: -1.131276 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162234 INN loss: -1.162234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122129 INN loss: -1.122129 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134418 INN loss: -1.134418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164403 INN loss: -1.164403 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168521 INN loss: -1.168521 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████████████████████████████████████████████           | 41/50 [4:28:47<14:27, 96.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.128904 INN loss: -1.128904 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185374 INN loss: -1.185374 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167154 INN loss: -1.167154 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134742 INN loss: -1.134742 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123427 INN loss: -1.123427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116546 INN loss: -1.116546 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138602 INN loss: -1.138602 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119841 INN loss: -1.119841 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189108 INN loss: -1.189108 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147661 INN loss: -1.147661 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139193 INN loss: -1.139193 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174127 INN loss: -1.174127 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097969 INN loss: -1.097969 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126121 INN loss: -1.126121 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158862 INN loss: -1.158862 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114980 INN loss: -1.114980 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140812 INN loss: -1.140812 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161896 INN loss: -1.161896 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115480 INN loss: -1.115480 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134699 INN loss: -1.134699 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138861 INN loss: -1.138861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091340 INN loss: -1.091340 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142745 INN loss: -1.142745 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187132 INN loss: -1.187132 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110218 INN loss: -1.110218 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139440 INN loss: -1.139440 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188455 INN loss: -1.188455 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124013 INN loss: -1.124013 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.071316 INN loss: -1.071316 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116794 INN loss: -1.116794 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149290 INN loss: -1.149290 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152183 INN loss: -1.152183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174882 INN loss: -1.174882 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148829 INN loss: -1.148829 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106692 INN loss: -1.106692 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152728 INN loss: -1.152728 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160488 INN loss: -1.160488 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144268 INN loss: -1.144268 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140537 INN loss: -1.140537 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114998 INN loss: -1.114998 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130420 INN loss: -1.130420 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172610 INN loss: -1.172610 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158494 INN loss: -1.158494 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148260 INN loss: -1.148260 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119608 INN loss: -1.119608 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127998 INN loss: -1.127998 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168106 INN loss: -1.168106 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131156 INN loss: -1.131156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.197651 INN loss: -1.197651 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090674 INN loss: -1.090674 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138491 INN loss: -1.138491 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149929 INN loss: -1.149929 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.068368 INN loss: -1.068368 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099470 INN loss: -1.099470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175632 INN loss: -1.175632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110172 INN loss: -1.110172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.065782 INN loss: -1.065782 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106281 INN loss: -1.106281 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119393 INN loss: -1.119393 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122484 INN loss: -1.122484 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141243 INN loss: -1.141243 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153061 INN loss: -1.153061 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109692 INN loss: -1.109692 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126955 INN loss: -1.126955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108636 INN loss: -1.108636 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157374 INN loss: -1.157374 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140150 INN loss: -1.140150 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103064 INN loss: -1.103064 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179220 INN loss: -1.179220 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.061908 INN loss: -1.061908 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147689 INN loss: -1.147689 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169233 INN loss: -1.169233 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122723 INN loss: -1.122723 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154625 INN loss: -1.154625 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131232 INN loss: -1.131232 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148077 INN loss: -1.148077 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148859 INN loss: -1.148859 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144608 INN loss: -1.144608 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131248 INN loss: -1.131248 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151382 INN loss: -1.151382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111932 INN loss: -1.111932 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140962 INN loss: -1.140962 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155081 INN loss: -1.155081 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144101 INN loss: -1.144101 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145195 INN loss: -1.145195 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151003 INN loss: -1.151003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126857 INN loss: -1.126857 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142804 INN loss: -1.142804 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125408 INN loss: -1.125408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147735 INN loss: -1.147735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137967 INN loss: -1.137967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120470 INN loss: -1.120470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.204949 INN loss: -1.204949 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090882 INN loss: -1.090882 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190210 INN loss: -1.190210 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.065017 INN loss: -1.065017 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137082 INN loss: -1.137082 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173648 INN loss: -1.173648 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138686 INN loss: -1.138686 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121416 INN loss: -1.121416 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165673 INN loss: -1.165673 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152987 INN loss: -1.152987 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122528 INN loss: -1.122528 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170831 INN loss: -1.170831 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103124 INN loss: -1.103124 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176851 INN loss: -1.176851 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155164 INN loss: -1.155164 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152541 INN loss: -1.152541 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123450 INN loss: -1.123450 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154243 INN loss: -1.154243 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187563 INN loss: -1.187563 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111538 INN loss: -1.111538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154335 INN loss: -1.154335 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087094 INN loss: -1.087094 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120710 INN loss: -1.120710 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165236 INN loss: -1.165236 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146781 INN loss: -1.146781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151603 INN loss: -1.151603 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125874 INN loss: -1.125874 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124408 INN loss: -1.124408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150761 INN loss: -1.150761 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106155 INN loss: -1.106155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152480 INN loss: -1.152480 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080946 INN loss: -1.080946 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114440 INN loss: -1.114440 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092242 INN loss: -1.092242 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179511 INN loss: -1.179511 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173583 INN loss: -1.173583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.069390 INN loss: -1.069390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110024 INN loss: -1.110024 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138800 INN loss: -1.138800 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128404 INN loss: -1.128404 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150315 INN loss: -1.150315 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137282 INN loss: -1.137282 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105371 INN loss: -1.105371 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135678 INN loss: -1.135678 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.076220 INN loss: -1.076220 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130728 INN loss: -1.130728 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173715 INN loss: -1.173715 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096897 INN loss: -1.096897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155721 INN loss: -1.155721 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109176 INN loss: -1.109176 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105146 INN loss: -1.105146 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138119 INN loss: -1.138119 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102635 INN loss: -1.102635 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136221 INN loss: -1.136221 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132650 INN loss: -1.132650 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149472 INN loss: -1.149472 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149675 INN loss: -1.149675 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114737 INN loss: -1.114737 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118497 INN loss: -1.118497 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139725 INN loss: -1.139725 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166806 INN loss: -1.166806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127517 INN loss: -1.127517 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155578 INN loss: -1.155578 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154460 INN loss: -1.154460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128791 INN loss: -1.128791 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143793 INN loss: -1.143793 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154934 INN loss: -1.154934 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158996 INN loss: -1.158996 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|███████████████████████████████████████████████████▏         | 42/50 [4:30:23<12:50, 96.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.162784 INN loss: -1.162784 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129467 INN loss: -1.129467 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109548 INN loss: -1.109548 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082244 INN loss: -1.082244 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161282 INN loss: -1.161282 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129191 INN loss: -1.129191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126336 INN loss: -1.126336 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162713 INN loss: -1.162713 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126334 INN loss: -1.126334 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176723 INN loss: -1.176723 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140679 INN loss: -1.140679 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102236 INN loss: -1.102236 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178064 INN loss: -1.178064 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167558 INN loss: -1.167558 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180933 INN loss: -1.180933 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117635 INN loss: -1.117635 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164106 INN loss: -1.164106 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113857 INN loss: -1.113857 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131073 INN loss: -1.131073 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120002 INN loss: -1.120002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123841 INN loss: -1.123841 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107049 INN loss: -1.107049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163390 INN loss: -1.163390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153227 INN loss: -1.153227 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127325 INN loss: -1.127325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120632 INN loss: -1.120632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131848 INN loss: -1.131848 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163437 INN loss: -1.163437 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159124 INN loss: -1.159124 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175165 INN loss: -1.175165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148032 INN loss: -1.148032 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167341 INN loss: -1.167341 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130494 INN loss: -1.130494 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154466 INN loss: -1.154466 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101425 INN loss: -1.101425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131059 INN loss: -1.131059 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161190 INN loss: -1.161190 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157328 INN loss: -1.157328 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150962 INN loss: -1.150962 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125791 INN loss: -1.125791 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102586 INN loss: -1.102586 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117597 INN loss: -1.117597 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142758 INN loss: -1.142758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175253 INN loss: -1.175253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137834 INN loss: -1.137834 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175202 INN loss: -1.175202 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173648 INN loss: -1.173648 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178382 INN loss: -1.178382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172727 INN loss: -1.172727 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148020 INN loss: -1.148020 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144963 INN loss: -1.144963 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112034 INN loss: -1.112034 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.218073 INN loss: -1.218073 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142555 INN loss: -1.142555 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125616 INN loss: -1.125616 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128394 INN loss: -1.128394 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162175 INN loss: -1.162175 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.080968 INN loss: -1.080968 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133822 INN loss: -1.133822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152493 INN loss: -1.152493 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150966 INN loss: -1.150966 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132317 INN loss: -1.132317 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140028 INN loss: -1.140028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174500 INN loss: -1.174500 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161598 INN loss: -1.161598 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181545 INN loss: -1.181545 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143366 INN loss: -1.143366 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130418 INN loss: -1.130418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134234 INN loss: -1.134234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119096 INN loss: -1.119096 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118471 INN loss: -1.118471 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136505 INN loss: -1.136505 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121119 INN loss: -1.121119 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166617 INN loss: -1.166617 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114432 INN loss: -1.114432 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144061 INN loss: -1.144061 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144954 INN loss: -1.144954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148977 INN loss: -1.148977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128993 INN loss: -1.128993 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181763 INN loss: -1.181763 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099129 INN loss: -1.099129 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187887 INN loss: -1.187887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134640 INN loss: -1.134640 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135381 INN loss: -1.135381 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183880 INN loss: -1.183880 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162489 INN loss: -1.162489 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128624 INN loss: -1.128624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163572 INN loss: -1.163572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170366 INN loss: -1.170366 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168875 INN loss: -1.168875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123887 INN loss: -1.123887 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156640 INN loss: -1.156640 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141962 INN loss: -1.141962 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130172 INN loss: -1.130172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134391 INN loss: -1.134391 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112735 INN loss: -1.112735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172956 INN loss: -1.172956 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145811 INN loss: -1.145811 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160377 INN loss: -1.160377 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150396 INN loss: -1.150396 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106006 INN loss: -1.106006 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099003 INN loss: -1.099003 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094890 INN loss: -1.094890 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111790 INN loss: -1.111790 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160973 INN loss: -1.160973 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149031 INN loss: -1.149031 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133078 INN loss: -1.133078 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112278 INN loss: -1.112278 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140690 INN loss: -1.140690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173854 INN loss: -1.173854 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129844 INN loss: -1.129844 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087840 INN loss: -1.087840 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097662 INN loss: -1.097662 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120999 INN loss: -1.120999 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117018 INN loss: -1.117018 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100889 INN loss: -1.100889 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172597 INN loss: -1.172597 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155394 INN loss: -1.155394 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140100 INN loss: -1.140100 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149062 INN loss: -1.149062 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160635 INN loss: -1.160635 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152693 INN loss: -1.152693 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165378 INN loss: -1.165378 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.191309 INN loss: -1.191309 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152572 INN loss: -1.152572 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138840 INN loss: -1.138840 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151002 INN loss: -1.151002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146554 INN loss: -1.146554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113145 INN loss: -1.113145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140101 INN loss: -1.140101 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135322 INN loss: -1.135322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141261 INN loss: -1.141261 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132841 INN loss: -1.132841 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183833 INN loss: -1.183833 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102716 INN loss: -1.102716 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176863 INN loss: -1.176863 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149618 INN loss: -1.149618 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122712 INN loss: -1.122712 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145075 INN loss: -1.145075 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109953 INN loss: -1.109953 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135249 INN loss: -1.135249 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132072 INN loss: -1.132072 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178614 INN loss: -1.178614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135478 INN loss: -1.135478 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128170 INN loss: -1.128170 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114913 INN loss: -1.114913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.086212 INN loss: -1.086212 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146458 INN loss: -1.146458 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105418 INN loss: -1.105418 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173717 INN loss: -1.173717 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127154 INN loss: -1.127154 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143292 INN loss: -1.143292 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103241 INN loss: -1.103241 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184591 INN loss: -1.184591 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137815 INN loss: -1.137815 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124585 INN loss: -1.124585 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175648 INN loss: -1.175648 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170916 INN loss: -1.170916 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114052 INN loss: -1.114052 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157362 INN loss: -1.157362 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████████████████████▍        | 43/50 [4:32:06<11:26, 98.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.114180 INN loss: -1.114180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135659 INN loss: -1.135659 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147873 INN loss: -1.147873 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115108 INN loss: -1.115108 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112493 INN loss: -1.112493 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122141 INN loss: -1.122141 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134483 INN loss: -1.134483 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143873 INN loss: -1.143873 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125852 INN loss: -1.125852 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120719 INN loss: -1.120719 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136063 INN loss: -1.136063 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110512 INN loss: -1.110512 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177840 INN loss: -1.177840 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137486 INN loss: -1.137486 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180679 INN loss: -1.180679 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151030 INN loss: -1.151030 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139938 INN loss: -1.139938 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163351 INN loss: -1.163351 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126163 INN loss: -1.126163 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121770 INN loss: -1.121770 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150977 INN loss: -1.150977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148350 INN loss: -1.148350 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165694 INN loss: -1.165694 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111015 INN loss: -1.111015 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151940 INN loss: -1.151940 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129836 INN loss: -1.129836 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138664 INN loss: -1.138664 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134167 INN loss: -1.134167 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143677 INN loss: -1.143677 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164356 INN loss: -1.164356 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121750 INN loss: -1.121750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096135 INN loss: -1.096135 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124981 INN loss: -1.124981 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126527 INN loss: -1.126527 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186784 INN loss: -1.186784 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119053 INN loss: -1.119053 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170773 INN loss: -1.170773 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149979 INN loss: -1.149979 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146678 INN loss: -1.146678 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132874 INN loss: -1.132874 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130276 INN loss: -1.130276 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147035 INN loss: -1.147035 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119686 INN loss: -1.119686 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146828 INN loss: -1.146828 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176076 INN loss: -1.176076 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118433 INN loss: -1.118433 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128428 INN loss: -1.128428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133911 INN loss: -1.133911 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111018 INN loss: -1.111018 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120703 INN loss: -1.120703 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131165 INN loss: -1.131165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159942 INN loss: -1.159942 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131447 INN loss: -1.131447 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129763 INN loss: -1.129763 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134330 INN loss: -1.134330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157106 INN loss: -1.157106 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171905 INN loss: -1.171905 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153806 INN loss: -1.153806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.205851 INN loss: -1.205851 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152151 INN loss: -1.152151 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141547 INN loss: -1.141547 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166338 INN loss: -1.166338 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155348 INN loss: -1.155348 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117282 INN loss: -1.117282 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112240 INN loss: -1.112240 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132981 INN loss: -1.132981 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168546 INN loss: -1.168546 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150842 INN loss: -1.150842 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126819 INN loss: -1.126819 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101769 INN loss: -1.101769 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092067 INN loss: -1.092067 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111748 INN loss: -1.111748 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122373 INN loss: -1.122373 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115697 INN loss: -1.115697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161497 INN loss: -1.161497 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110274 INN loss: -1.110274 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135370 INN loss: -1.135370 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127624 INN loss: -1.127624 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117577 INN loss: -1.117577 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097240 INN loss: -1.097240 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110312 INN loss: -1.110312 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117631 INN loss: -1.117631 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.215859 INN loss: -1.215859 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107810 INN loss: -1.107810 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155090 INN loss: -1.155090 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143320 INN loss: -1.143320 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126878 INN loss: -1.126878 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143632 INN loss: -1.143632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155692 INN loss: -1.155692 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109780 INN loss: -1.109780 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147317 INN loss: -1.147317 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144803 INN loss: -1.144803 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152112 INN loss: -1.152112 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170760 INN loss: -1.170760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172727 INN loss: -1.172727 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169787 INN loss: -1.169787 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137248 INN loss: -1.137248 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128667 INN loss: -1.128667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133496 INN loss: -1.133496 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150167 INN loss: -1.150167 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096645 INN loss: -1.096645 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146630 INN loss: -1.146630 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151080 INN loss: -1.151080 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136459 INN loss: -1.136459 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115218 INN loss: -1.115218 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174945 INN loss: -1.174945 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140147 INN loss: -1.140147 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180629 INN loss: -1.180629 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124499 INN loss: -1.124499 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155603 INN loss: -1.155603 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140065 INN loss: -1.140065 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162284 INN loss: -1.162284 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150980 INN loss: -1.150980 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162596 INN loss: -1.162596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128558 INN loss: -1.128558 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152801 INN loss: -1.152801 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168342 INN loss: -1.168342 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176820 INN loss: -1.176820 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145134 INN loss: -1.145134 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081221 INN loss: -1.081221 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125682 INN loss: -1.125682 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149805 INN loss: -1.149805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127187 INN loss: -1.127187 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130609 INN loss: -1.130609 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144903 INN loss: -1.144903 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139446 INN loss: -1.139446 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128008 INN loss: -1.128008 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125772 INN loss: -1.125772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184955 INN loss: -1.184955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141269 INN loss: -1.141269 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141356 INN loss: -1.141356 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151183 INN loss: -1.151183 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114168 INN loss: -1.114168 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158430 INN loss: -1.158430 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.201365 INN loss: -1.201365 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111499 INN loss: -1.111499 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112019 INN loss: -1.112019 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108396 INN loss: -1.108396 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135145 INN loss: -1.135145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130695 INN loss: -1.130695 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122305 INN loss: -1.122305 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142743 INN loss: -1.142743 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137504 INN loss: -1.137504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159165 INN loss: -1.159165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166622 INN loss: -1.166622 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137026 INN loss: -1.137026 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102403 INN loss: -1.102403 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085451 INN loss: -1.085451 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154216 INN loss: -1.154216 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118073 INN loss: -1.118073 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151762 INN loss: -1.151762 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134625 INN loss: -1.134625 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130644 INN loss: -1.130644 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123725 INN loss: -1.123725 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.201353 INN loss: -1.201353 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131300 INN loss: -1.131300 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140021 INN loss: -1.140021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146623 INN loss: -1.146623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128072 INN loss: -1.128072 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126404 INN loss: -1.126404 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 44/50 [4:33:38<09:38, 96.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.132258 INN loss: -1.132258 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147540 INN loss: -1.147540 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120737 INN loss: -1.120737 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139676 INN loss: -1.139676 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145225 INN loss: -1.145225 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165822 INN loss: -1.165822 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137068 INN loss: -1.137068 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137967 INN loss: -1.137967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129002 INN loss: -1.129002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123963 INN loss: -1.123963 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144349 INN loss: -1.144349 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158329 INN loss: -1.158329 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171253 INN loss: -1.171253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159082 INN loss: -1.159082 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157622 INN loss: -1.157622 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123704 INN loss: -1.123704 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132770 INN loss: -1.132770 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136773 INN loss: -1.136773 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160014 INN loss: -1.160014 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168713 INN loss: -1.168713 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101148 INN loss: -1.101148 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110605 INN loss: -1.110605 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179211 INN loss: -1.179211 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146920 INN loss: -1.146920 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088619 INN loss: -1.088619 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164878 INN loss: -1.164878 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157245 INN loss: -1.157245 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141783 INN loss: -1.141783 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111447 INN loss: -1.111447 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167517 INN loss: -1.167517 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156743 INN loss: -1.156743 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158268 INN loss: -1.158268 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135387 INN loss: -1.135387 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091811 INN loss: -1.091811 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133770 INN loss: -1.133770 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121715 INN loss: -1.121715 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091061 INN loss: -1.091061 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186734 INN loss: -1.186734 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175241 INN loss: -1.175241 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144507 INN loss: -1.144507 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142616 INN loss: -1.142616 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159134 INN loss: -1.159134 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152544 INN loss: -1.152544 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125643 INN loss: -1.125643 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143438 INN loss: -1.143438 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145023 INN loss: -1.145023 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.202684 INN loss: -1.202684 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129409 INN loss: -1.129409 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145924 INN loss: -1.145924 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134435 INN loss: -1.134435 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124512 INN loss: -1.124512 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142412 INN loss: -1.142412 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112606 INN loss: -1.112606 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121506 INN loss: -1.121506 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154781 INN loss: -1.154781 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173097 INN loss: -1.173097 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109033 INN loss: -1.109033 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183123 INN loss: -1.183123 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113771 INN loss: -1.113771 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144719 INN loss: -1.144719 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135821 INN loss: -1.135821 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117539 INN loss: -1.117539 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164359 INN loss: -1.164359 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.177989 INN loss: -1.177989 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145079 INN loss: -1.145079 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169528 INN loss: -1.169528 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185026 INN loss: -1.185026 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148104 INN loss: -1.148104 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130640 INN loss: -1.130640 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146748 INN loss: -1.146748 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134793 INN loss: -1.134793 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151806 INN loss: -1.151806 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156462 INN loss: -1.156462 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158746 INN loss: -1.158746 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127789 INN loss: -1.127789 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138658 INN loss: -1.138658 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151625 INN loss: -1.151625 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147640 INN loss: -1.147640 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102908 INN loss: -1.102908 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122042 INN loss: -1.122042 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127262 INN loss: -1.127262 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136208 INN loss: -1.136208 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166934 INN loss: -1.166934 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111779 INN loss: -1.111779 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124690 INN loss: -1.124690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180368 INN loss: -1.180368 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089212 INN loss: -1.089212 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082474 INN loss: -1.082474 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143215 INN loss: -1.143215 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147326 INN loss: -1.147326 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162150 INN loss: -1.162150 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157440 INN loss: -1.157440 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128037 INN loss: -1.128037 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135714 INN loss: -1.135714 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116150 INN loss: -1.116150 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152955 INN loss: -1.152955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141570 INN loss: -1.141570 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168068 INN loss: -1.168068 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142797 INN loss: -1.142797 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146954 INN loss: -1.146954 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186701 INN loss: -1.186701 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115211 INN loss: -1.115211 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144792 INN loss: -1.144792 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163246 INN loss: -1.163246 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138027 INN loss: -1.138027 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108417 INN loss: -1.108417 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166942 INN loss: -1.166942 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127727 INN loss: -1.127727 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103576 INN loss: -1.103576 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121053 INN loss: -1.121053 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183244 INN loss: -1.183244 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153864 INN loss: -1.153864 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146390 INN loss: -1.146390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151194 INN loss: -1.151194 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145614 INN loss: -1.145614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151384 INN loss: -1.151384 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185019 INN loss: -1.185019 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146929 INN loss: -1.146929 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115263 INN loss: -1.115263 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151943 INN loss: -1.151943 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.203538 INN loss: -1.203538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085850 INN loss: -1.085850 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134838 INN loss: -1.134838 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134697 INN loss: -1.134697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138554 INN loss: -1.138554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113331 INN loss: -1.113331 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190864 INN loss: -1.190864 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151645 INN loss: -1.151645 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163861 INN loss: -1.163861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175188 INN loss: -1.175188 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146758 INN loss: -1.146758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151466 INN loss: -1.151466 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146320 INN loss: -1.146320 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131341 INN loss: -1.131341 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113555 INN loss: -1.113555 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173056 INN loss: -1.173056 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160792 INN loss: -1.160792 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125859 INN loss: -1.125859 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144974 INN loss: -1.144974 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154999 INN loss: -1.154999 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181722 INN loss: -1.181722 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128620 INN loss: -1.128620 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142579 INN loss: -1.142579 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105366 INN loss: -1.105366 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.099834 INN loss: -1.099834 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172565 INN loss: -1.172565 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172255 INN loss: -1.172255 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148076 INN loss: -1.148076 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096437 INN loss: -1.096437 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169683 INN loss: -1.169683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140169 INN loss: -1.140169 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146667 INN loss: -1.146667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101965 INN loss: -1.101965 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133955 INN loss: -1.133955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105722 INN loss: -1.105722 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140198 INN loss: -1.140198 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134353 INN loss: -1.134353 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144852 INN loss: -1.144852 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132654 INN loss: -1.132654 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126149 INN loss: -1.126149 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████▉      | 45/50 [4:35:07<07:50, 94.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.161651 INN loss: -1.161651 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130079 INN loss: -1.130079 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135899 INN loss: -1.135899 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158042 INN loss: -1.158042 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151126 INN loss: -1.151126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133129 INN loss: -1.133129 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161545 INN loss: -1.161545 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096531 INN loss: -1.096531 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158642 INN loss: -1.158642 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138819 INN loss: -1.138819 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135679 INN loss: -1.135679 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109265 INN loss: -1.109265 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136333 INN loss: -1.136333 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126056 INN loss: -1.126056 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.216431 INN loss: -1.216431 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143967 INN loss: -1.143967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179076 INN loss: -1.179076 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178049 INN loss: -1.178049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109044 INN loss: -1.109044 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130219 INN loss: -1.130219 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121667 INN loss: -1.121667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109930 INN loss: -1.109930 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109840 INN loss: -1.109840 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124936 INN loss: -1.124936 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146198 INN loss: -1.146198 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138948 INN loss: -1.138948 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182286 INN loss: -1.182286 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132049 INN loss: -1.132049 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157868 INN loss: -1.157868 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136663 INN loss: -1.136663 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165358 INN loss: -1.165358 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190977 INN loss: -1.190977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165481 INN loss: -1.165481 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109191 INN loss: -1.109191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136955 INN loss: -1.136955 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169311 INN loss: -1.169311 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147933 INN loss: -1.147933 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127199 INN loss: -1.127199 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141993 INN loss: -1.141993 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147750 INN loss: -1.147750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104298 INN loss: -1.104298 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143850 INN loss: -1.143850 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151415 INN loss: -1.151415 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110250 INN loss: -1.110250 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126754 INN loss: -1.126754 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148446 INN loss: -1.148446 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170883 INN loss: -1.170883 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121777 INN loss: -1.121777 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138942 INN loss: -1.138942 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133547 INN loss: -1.133547 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140033 INN loss: -1.140033 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125623 INN loss: -1.125623 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144288 INN loss: -1.144288 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150262 INN loss: -1.150262 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157094 INN loss: -1.157094 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121165 INN loss: -1.121165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156663 INN loss: -1.156663 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130315 INN loss: -1.130315 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126520 INN loss: -1.126520 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181144 INN loss: -1.181144 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137130 INN loss: -1.137130 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141657 INN loss: -1.141657 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157670 INN loss: -1.157670 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116179 INN loss: -1.116179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150769 INN loss: -1.150769 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133416 INN loss: -1.133416 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090713 INN loss: -1.090713 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112705 INN loss: -1.112705 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120182 INN loss: -1.120182 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.183645 INN loss: -1.183645 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157637 INN loss: -1.157637 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102583 INN loss: -1.102583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083837 INN loss: -1.083837 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114146 INN loss: -1.114146 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137034 INN loss: -1.137034 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116214 INN loss: -1.116214 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084985 INN loss: -1.084985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140115 INN loss: -1.140115 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127124 INN loss: -1.127124 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162845 INN loss: -1.162845 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160638 INN loss: -1.160638 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136907 INN loss: -1.136907 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136326 INN loss: -1.136326 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142986 INN loss: -1.142986 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140355 INN loss: -1.140355 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110688 INN loss: -1.110688 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167301 INN loss: -1.167301 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169370 INN loss: -1.169370 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151234 INN loss: -1.151234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124747 INN loss: -1.124747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124518 INN loss: -1.124518 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136277 INN loss: -1.136277 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147206 INN loss: -1.147206 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153068 INN loss: -1.153068 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180570 INN loss: -1.180570 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134750 INN loss: -1.134750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137896 INN loss: -1.137896 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118663 INN loss: -1.118663 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140115 INN loss: -1.140115 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172113 INN loss: -1.172113 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136073 INN loss: -1.136073 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158117 INN loss: -1.158117 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123746 INN loss: -1.123746 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185133 INN loss: -1.185133 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149470 INN loss: -1.149470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093833 INN loss: -1.093833 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153092 INN loss: -1.153092 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081376 INN loss: -1.081376 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132683 INN loss: -1.132683 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143361 INN loss: -1.143361 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107112 INN loss: -1.107112 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128875 INN loss: -1.128875 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156931 INN loss: -1.156931 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129286 INN loss: -1.129286 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089517 INN loss: -1.089517 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160785 INN loss: -1.160785 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157026 INN loss: -1.157026 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161225 INN loss: -1.161225 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174735 INN loss: -1.174735 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137772 INN loss: -1.137772 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139774 INN loss: -1.139774 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090908 INN loss: -1.090908 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113135 INN loss: -1.113135 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120736 INN loss: -1.120736 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160986 INN loss: -1.160986 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158960 INN loss: -1.158960 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178394 INN loss: -1.178394 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161095 INN loss: -1.161095 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118845 INN loss: -1.118845 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161811 INN loss: -1.161811 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141666 INN loss: -1.141666 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164249 INN loss: -1.164249 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152163 INN loss: -1.152163 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143150 INN loss: -1.143150 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131526 INN loss: -1.131526 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110734 INN loss: -1.110734 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141991 INN loss: -1.141991 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149143 INN loss: -1.149143 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121959 INN loss: -1.121959 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137295 INN loss: -1.137295 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189350 INN loss: -1.189350 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117967 INN loss: -1.117967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122550 INN loss: -1.122550 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170229 INN loss: -1.170229 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135783 INN loss: -1.135783 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136336 INN loss: -1.136336 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.193853 INN loss: -1.193853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142033 INN loss: -1.142033 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132032 INN loss: -1.132032 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115742 INN loss: -1.115742 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172628 INN loss: -1.172628 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176979 INN loss: -1.176979 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121689 INN loss: -1.121689 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123154 INN loss: -1.123154 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118126 INN loss: -1.118126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148274 INN loss: -1.148274 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111536 INN loss: -1.111536 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136156 INN loss: -1.136156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150422 INN loss: -1.150422 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151279 INN loss: -1.151279 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████▏    | 46/50 [4:40:57<11:23, 170.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.133807 INN loss: -1.133807 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100295 INN loss: -1.100295 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135667 INN loss: -1.135667 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161096 INN loss: -1.161096 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159081 INN loss: -1.159081 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.075628 INN loss: -1.075628 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175401 INN loss: -1.175401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147637 INN loss: -1.147637 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146485 INN loss: -1.146485 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118571 INN loss: -1.118571 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109091 INN loss: -1.109091 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170442 INN loss: -1.170442 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147861 INN loss: -1.147861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111035 INN loss: -1.111035 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108473 INN loss: -1.108473 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120838 INN loss: -1.120838 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149021 INN loss: -1.149021 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144929 INN loss: -1.144929 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164187 INN loss: -1.164187 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155652 INN loss: -1.155652 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150455 INN loss: -1.150455 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124178 INN loss: -1.124178 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109557 INN loss: -1.109557 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139266 INN loss: -1.139266 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.192376 INN loss: -1.192376 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170846 INN loss: -1.170846 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178030 INN loss: -1.178030 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110699 INN loss: -1.110699 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146948 INN loss: -1.146948 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179847 INN loss: -1.179847 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171185 INN loss: -1.171185 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172055 INN loss: -1.172055 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104521 INN loss: -1.104521 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157408 INN loss: -1.157408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157614 INN loss: -1.157614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116529 INN loss: -1.116529 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129206 INN loss: -1.129206 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103899 INN loss: -1.103899 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159821 INN loss: -1.159821 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.078538 INN loss: -1.078538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133628 INN loss: -1.133628 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147868 INN loss: -1.147868 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.072856 INN loss: -1.072856 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112854 INN loss: -1.112854 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139715 INN loss: -1.139715 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140339 INN loss: -1.140339 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108427 INN loss: -1.108427 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132189 INN loss: -1.132189 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135897 INN loss: -1.135897 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105448 INN loss: -1.105448 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135432 INN loss: -1.135432 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174629 INN loss: -1.174629 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116484 INN loss: -1.116484 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143242 INN loss: -1.143242 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175767 INN loss: -1.175767 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126596 INN loss: -1.126596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160910 INN loss: -1.160910 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164321 INN loss: -1.164321 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137253 INN loss: -1.137253 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093617 INN loss: -1.093617 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.195602 INN loss: -1.195602 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100530 INN loss: -1.100530 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153176 INN loss: -1.153176 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.090709 INN loss: -1.090709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176709 INN loss: -1.176709 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129505 INN loss: -1.129505 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131179 INN loss: -1.131179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.202125 INN loss: -1.202125 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132460 INN loss: -1.132460 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152613 INN loss: -1.152613 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118731 INN loss: -1.118731 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094225 INN loss: -1.094225 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138715 INN loss: -1.138715 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137234 INN loss: -1.137234 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098413 INN loss: -1.098413 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165692 INN loss: -1.165692 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167340 INN loss: -1.167340 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163336 INN loss: -1.163336 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135758 INN loss: -1.135758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188458 INN loss: -1.188458 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175415 INN loss: -1.175415 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124219 INN loss: -1.124219 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133310 INN loss: -1.133310 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144393 INN loss: -1.144393 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146640 INN loss: -1.146640 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163662 INN loss: -1.163662 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164304 INN loss: -1.164304 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159096 INN loss: -1.159096 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100504 INN loss: -1.100504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121229 INN loss: -1.121229 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159372 INN loss: -1.159372 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151677 INN loss: -1.151677 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130504 INN loss: -1.130504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128065 INN loss: -1.128065 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098081 INN loss: -1.098081 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156404 INN loss: -1.156404 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155441 INN loss: -1.155441 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181945 INN loss: -1.181945 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148257 INN loss: -1.148257 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.088555 INN loss: -1.088555 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103586 INN loss: -1.103586 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144668 INN loss: -1.144668 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148154 INN loss: -1.148154 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174284 INN loss: -1.174284 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172254 INN loss: -1.172254 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131935 INN loss: -1.131935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155390 INN loss: -1.155390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140836 INN loss: -1.140836 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153409 INN loss: -1.153409 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093353 INN loss: -1.093353 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179262 INN loss: -1.179262 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138515 INN loss: -1.138515 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120252 INN loss: -1.120252 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135466 INN loss: -1.135466 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114614 INN loss: -1.114614 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.195438 INN loss: -1.195438 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128227 INN loss: -1.128227 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182401 INN loss: -1.182401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153550 INN loss: -1.153550 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126795 INN loss: -1.126795 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162521 INN loss: -1.162521 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131820 INN loss: -1.131820 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147853 INN loss: -1.147853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151311 INN loss: -1.151311 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174918 INN loss: -1.174918 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139335 INN loss: -1.139335 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153714 INN loss: -1.153714 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139145 INN loss: -1.139145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149773 INN loss: -1.149773 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106367 INN loss: -1.106367 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159845 INN loss: -1.159845 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139695 INN loss: -1.139695 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136239 INN loss: -1.136239 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097977 INN loss: -1.097977 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155131 INN loss: -1.155131 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112101 INN loss: -1.112101 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167480 INN loss: -1.167480 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143956 INN loss: -1.143956 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138574 INN loss: -1.138574 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141762 INN loss: -1.141762 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142452 INN loss: -1.142452 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142475 INN loss: -1.142475 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136807 INN loss: -1.136807 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138804 INN loss: -1.138804 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157636 INN loss: -1.157636 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129640 INN loss: -1.129640 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124534 INN loss: -1.124534 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170258 INN loss: -1.170258 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100821 INN loss: -1.100821 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159549 INN loss: -1.159549 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133914 INN loss: -1.133914 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163548 INN loss: -1.163548 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111359 INN loss: -1.111359 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141594 INN loss: -1.141594 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.072746 INN loss: -1.072746 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.181938 INN loss: -1.181938 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168870 INN loss: -1.168870 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188929 INN loss: -1.188929 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139490 INN loss: -1.139490 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152018 INN loss: -1.152018 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|████████████████████████████████████████████████████████▍   | 47/50 [4:42:31<07:23, 147.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.171080 INN loss: -1.171080 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162345 INN loss: -1.162345 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146985 INN loss: -1.146985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121813 INN loss: -1.121813 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142608 INN loss: -1.142608 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119662 INN loss: -1.119662 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185697 INN loss: -1.185697 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120583 INN loss: -1.120583 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150151 INN loss: -1.150151 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123894 INN loss: -1.123894 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081574 INN loss: -1.081574 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165245 INN loss: -1.165245 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.078127 INN loss: -1.078127 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140882 INN loss: -1.140882 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145254 INN loss: -1.145254 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098808 INN loss: -1.098808 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100443 INN loss: -1.100443 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112358 INN loss: -1.112358 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110191 INN loss: -1.110191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118753 INN loss: -1.118753 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.103379 INN loss: -1.103379 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140355 INN loss: -1.140355 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108916 INN loss: -1.108916 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081204 INN loss: -1.081204 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175312 INN loss: -1.175312 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098828 INN loss: -1.098828 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159597 INN loss: -1.159597 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104145 INN loss: -1.104145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135034 INN loss: -1.135034 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135685 INN loss: -1.135685 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092972 INN loss: -1.092972 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143040 INN loss: -1.143040 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149473 INN loss: -1.149473 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138554 INN loss: -1.138554 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161666 INN loss: -1.161666 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133478 INN loss: -1.133478 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140014 INN loss: -1.140014 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128804 INN loss: -1.128804 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146773 INN loss: -1.146773 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149919 INN loss: -1.149919 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120561 INN loss: -1.120561 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176625 INN loss: -1.176625 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168326 INN loss: -1.168326 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120701 INN loss: -1.120701 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123537 INN loss: -1.123537 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176956 INN loss: -1.176956 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119712 INN loss: -1.119712 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136330 INN loss: -1.136330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134083 INN loss: -1.134083 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141203 INN loss: -1.141203 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190014 INN loss: -1.190014 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142618 INN loss: -1.142618 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159068 INN loss: -1.159068 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163155 INN loss: -1.163155 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155296 INN loss: -1.155296 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171373 INN loss: -1.171373 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138921 INN loss: -1.138921 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.198425 INN loss: -1.198425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143078 INN loss: -1.143078 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133226 INN loss: -1.133226 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143758 INN loss: -1.143758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117653 INN loss: -1.117653 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111688 INN loss: -1.111688 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119448 INN loss: -1.119448 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157294 INN loss: -1.157294 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135715 INN loss: -1.135715 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175470 INN loss: -1.175470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129210 INN loss: -1.129210 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131985 INN loss: -1.131985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140707 INN loss: -1.140707 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113394 INN loss: -1.113394 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138372 INN loss: -1.138372 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.188056 INN loss: -1.188056 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105834 INN loss: -1.105834 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162702 INN loss: -1.162702 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139676 INN loss: -1.139676 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182372 INN loss: -1.182372 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.205434 INN loss: -1.205434 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095073 INN loss: -1.095073 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133586 INN loss: -1.133586 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110950 INN loss: -1.110950 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119125 INN loss: -1.119125 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138178 INN loss: -1.138178 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150748 INN loss: -1.150748 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154231 INN loss: -1.154231 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148340 INN loss: -1.148340 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133549 INN loss: -1.133549 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102961 INN loss: -1.102961 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120434 INN loss: -1.120434 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083948 INN loss: -1.083948 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131992 INN loss: -1.131992 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174058 INN loss: -1.174058 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124777 INN loss: -1.124777 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128788 INN loss: -1.128788 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134529 INN loss: -1.134529 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178620 INN loss: -1.178620 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131616 INN loss: -1.131616 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093037 INN loss: -1.093037 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094952 INN loss: -1.094952 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135997 INN loss: -1.135997 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179339 INN loss: -1.179339 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136751 INN loss: -1.136751 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141763 INN loss: -1.141763 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155616 INN loss: -1.155616 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154935 INN loss: -1.154935 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187151 INN loss: -1.187151 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.097597 INN loss: -1.097597 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122006 INN loss: -1.122006 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166194 INN loss: -1.166194 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153457 INN loss: -1.153457 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149801 INN loss: -1.149801 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091966 INN loss: -1.091966 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113408 INN loss: -1.113408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140237 INN loss: -1.140237 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142728 INN loss: -1.142728 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137353 INN loss: -1.137353 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119342 INN loss: -1.119342 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170322 INN loss: -1.170322 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150950 INN loss: -1.150950 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178133 INN loss: -1.178133 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189805 INN loss: -1.189805 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174233 INN loss: -1.174233 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136271 INN loss: -1.136271 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128815 INN loss: -1.128815 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150035 INN loss: -1.150035 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101145 INN loss: -1.101145 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.058135 INN loss: -1.058135 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127975 INN loss: -1.127975 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120267 INN loss: -1.120267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147131 INN loss: -1.147131 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145053 INN loss: -1.145053 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.172039 INN loss: -1.172039 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133358 INN loss: -1.133358 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169846 INN loss: -1.169846 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127181 INN loss: -1.127181 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.100506 INN loss: -1.100506 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132505 INN loss: -1.132505 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.200711 INN loss: -1.200711 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.180564 INN loss: -1.180564 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161476 INN loss: -1.161476 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127945 INN loss: -1.127945 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116504 INN loss: -1.116504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104421 INN loss: -1.104421 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156419 INN loss: -1.156419 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093930 INN loss: -1.093930 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167089 INN loss: -1.167089 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141401 INN loss: -1.141401 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.063918 INN loss: -1.063918 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132861 INN loss: -1.132861 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112301 INN loss: -1.112301 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168314 INN loss: -1.168314 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147903 INN loss: -1.147903 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127504 INN loss: -1.127504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134529 INN loss: -1.134529 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089821 INN loss: -1.089821 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117057 INN loss: -1.117057 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118688 INN loss: -1.118688 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145455 INN loss: -1.145455 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160172 INN loss: -1.160172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095759 INN loss: -1.095759 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████████████████████████████████████████████████████▌  | 48/50 [4:44:01<04:21, 130.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.100598 INN loss: -1.100598 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133329 INN loss: -1.133329 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148042 INN loss: -1.148042 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132714 INN loss: -1.132714 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145275 INN loss: -1.145275 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150156 INN loss: -1.150156 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154898 INN loss: -1.154898 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155304 INN loss: -1.155304 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134989 INN loss: -1.134989 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151411 INN loss: -1.151411 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147409 INN loss: -1.147409 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151347 INN loss: -1.151347 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.185321 INN loss: -1.185321 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151526 INN loss: -1.151526 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136338 INN loss: -1.136338 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134618 INN loss: -1.134618 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089505 INN loss: -1.089505 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151985 INN loss: -1.151985 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124360 INN loss: -1.124360 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162690 INN loss: -1.162690 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094474 INN loss: -1.094474 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145339 INN loss: -1.145339 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166191 INN loss: -1.166191 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170995 INN loss: -1.170995 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133986 INN loss: -1.133986 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145495 INN loss: -1.145495 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.093687 INN loss: -1.093687 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146256 INN loss: -1.146256 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125300 INN loss: -1.125300 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154026 INN loss: -1.154026 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130129 INN loss: -1.130129 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122345 INN loss: -1.122345 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178169 INN loss: -1.178169 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112085 INN loss: -1.112085 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156323 INN loss: -1.156323 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152964 INN loss: -1.152964 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152737 INN loss: -1.152737 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124250 INN loss: -1.124250 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148699 INN loss: -1.148699 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.092333 INN loss: -1.092333 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151684 INN loss: -1.151684 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135302 INN loss: -1.135302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141412 INN loss: -1.141412 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132231 INN loss: -1.132231 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.095552 INN loss: -1.095552 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147333 INN loss: -1.147333 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155825 INN loss: -1.155825 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112165 INN loss: -1.112165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159167 INN loss: -1.159167 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114524 INN loss: -1.114524 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146598 INN loss: -1.146598 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101063 INN loss: -1.101063 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111747 INN loss: -1.111747 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108504 INN loss: -1.108504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107960 INN loss: -1.107960 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170386 INN loss: -1.170386 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143317 INN loss: -1.143317 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124732 INN loss: -1.124732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146890 INN loss: -1.146890 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.173580 INN loss: -1.173580 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137186 INN loss: -1.137186 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131726 INN loss: -1.131726 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161812 INN loss: -1.161812 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147547 INN loss: -1.147547 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157773 INN loss: -1.157773 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.082646 INN loss: -1.082646 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174851 INN loss: -1.174851 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150002 INN loss: -1.150002 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144268 INN loss: -1.144268 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167286 INN loss: -1.167286 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139325 INN loss: -1.139325 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126282 INN loss: -1.126282 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104131 INN loss: -1.104131 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138653 INN loss: -1.138653 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159560 INN loss: -1.159560 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121776 INN loss: -1.121776 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148659 INN loss: -1.148659 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126812 INN loss: -1.126812 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.087185 INN loss: -1.087185 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.170714 INN loss: -1.170714 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111165 INN loss: -1.111165 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124399 INN loss: -1.124399 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.193179 INN loss: -1.193179 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107160 INN loss: -1.107160 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150941 INN loss: -1.150941 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.194877 INN loss: -1.194877 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120807 INN loss: -1.120807 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143267 INN loss: -1.143267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119511 INN loss: -1.119511 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149070 INN loss: -1.149070 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.084867 INN loss: -1.084867 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137178 INN loss: -1.137178 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160794 INN loss: -1.160794 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.044882 INN loss: -1.044882 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089187 INN loss: -1.089187 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158566 INN loss: -1.158566 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153364 INN loss: -1.153364 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121210 INN loss: -1.121210 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102172 INN loss: -1.102172 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143990 INN loss: -1.143990 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.162501 INN loss: -1.162501 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.089100 INN loss: -1.089100 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161244 INN loss: -1.161244 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114278 INN loss: -1.114278 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189535 INN loss: -1.189535 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140632 INN loss: -1.140632 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158278 INN loss: -1.158278 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102425 INN loss: -1.102425 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117826 INN loss: -1.117826 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142494 INN loss: -1.142494 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174222 INN loss: -1.174222 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132766 INN loss: -1.132766 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144553 INN loss: -1.144553 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154263 INN loss: -1.154263 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.035230 INN loss: -1.035230 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115445 INN loss: -1.115445 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165368 INN loss: -1.165368 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121967 INN loss: -1.121967 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.189473 INN loss: -1.189473 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138465 INN loss: -1.138465 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120762 INN loss: -1.120762 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166050 INN loss: -1.166050 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130271 INN loss: -1.130271 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178588 INN loss: -1.178588 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119159 INN loss: -1.119159 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142057 INN loss: -1.142057 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140844 INN loss: -1.140844 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.102137 INN loss: -1.102137 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143440 INN loss: -1.143440 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.137700 INN loss: -1.137700 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.171662 INN loss: -1.171662 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190908 INN loss: -1.190908 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134874 INN loss: -1.134874 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144150 INN loss: -1.144150 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156631 INN loss: -1.156631 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138365 INN loss: -1.138365 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151261 INN loss: -1.151261 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126020 INN loss: -1.126020 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139190 INN loss: -1.139190 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151161 INN loss: -1.151161 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.186126 INN loss: -1.186126 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.083876 INN loss: -1.083876 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130428 INN loss: -1.130428 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168928 INN loss: -1.168928 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135963 INN loss: -1.135963 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.151202 INN loss: -1.151202 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106311 INN loss: -1.106311 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125354 INN loss: -1.125354 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105913 INN loss: -1.105913 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.067190 INN loss: -1.067190 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122576 INN loss: -1.122576 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150350 INN loss: -1.150350 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125621 INN loss: -1.125621 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126629 INN loss: -1.126629 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.085770 INN loss: -1.085770 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.176338 INN loss: -1.176338 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133127 INN loss: -1.133127 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145009 INN loss: -1.145009 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166495 INN loss: -1.166495 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114668 INN loss: -1.114668 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████▊ | 49/50 [4:45:29<01:57, 117.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: -1.162855 INN loss: -1.162855 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182390 INN loss: -1.182390 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.129116 INN loss: -1.129116 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117458 INN loss: -1.117458 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.167657 INN loss: -1.167657 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154289 INN loss: -1.154289 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150175 INN loss: -1.150175 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126904 INN loss: -1.126904 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165421 INN loss: -1.165421 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117023 INN loss: -1.117023 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096129 INN loss: -1.096129 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.192167 INN loss: -1.192167 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.148232 INN loss: -1.148232 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147559 INN loss: -1.147559 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.168551 INN loss: -1.168551 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139543 INN loss: -1.139543 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.155370 INN loss: -1.155370 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160000 INN loss: -1.160000 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.105313 INN loss: -1.105313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.133704 INN loss: -1.133704 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121079 INN loss: -1.121079 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146966 INN loss: -1.146966 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132983 INN loss: -1.132983 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094321 INN loss: -1.094321 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120902 INN loss: -1.120902 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145058 INN loss: -1.145058 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121760 INN loss: -1.121760 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.159663 INN loss: -1.159663 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153956 INN loss: -1.153956 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.101504 INN loss: -1.101504 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110501 INN loss: -1.110501 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126236 INN loss: -1.126236 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150911 INN loss: -1.150911 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163986 INN loss: -1.163986 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134530 INN loss: -1.134530 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.166414 INN loss: -1.166414 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109733 INN loss: -1.109733 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154193 INN loss: -1.154193 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153270 INN loss: -1.153270 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.144482 INN loss: -1.144482 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152732 INN loss: -1.152732 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.081846 INN loss: -1.081846 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.156841 INN loss: -1.156841 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.091823 INN loss: -1.091823 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.111055 INN loss: -1.111055 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.150206 INN loss: -1.150206 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112313 INN loss: -1.112313 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110928 INN loss: -1.110928 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153733 INN loss: -1.153733 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108170 INN loss: -1.108170 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.115729 INN loss: -1.115729 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121197 INN loss: -1.121197 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.060455 INN loss: -1.060455 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.121483 INN loss: -1.121483 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.132063 INN loss: -1.132063 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.078797 INN loss: -1.078797 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126945 INN loss: -1.126945 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157739 INN loss: -1.157739 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153993 INN loss: -1.153993 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.138799 INN loss: -1.138799 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.119330 INN loss: -1.119330 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.184470 INN loss: -1.184470 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.123666 INN loss: -1.123666 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.194028 INN loss: -1.194028 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.182547 INN loss: -1.182547 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152804 INN loss: -1.152804 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.106941 INN loss: -1.106941 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.134271 INN loss: -1.134271 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.096046 INN loss: -1.096046 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.153856 INN loss: -1.153856 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122164 INN loss: -1.122164 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169569 INN loss: -1.169569 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.190134 INN loss: -1.190134 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104853 INN loss: -1.104853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.127635 INN loss: -1.127635 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.164764 INN loss: -1.164764 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117918 INN loss: -1.117918 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169721 INN loss: -1.169721 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154346 INN loss: -1.154346 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.122088 INN loss: -1.122088 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.158361 INN loss: -1.158361 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.104903 INN loss: -1.104903 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107167 INN loss: -1.107167 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140538 INN loss: -1.140538 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143038 INN loss: -1.143038 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113708 INN loss: -1.113708 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125696 INN loss: -1.125696 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113933 INN loss: -1.113933 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.124206 INN loss: -1.124206 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.108245 INN loss: -1.108245 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163832 INN loss: -1.163832 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130774 INN loss: -1.130774 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147492 INN loss: -1.147492 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.126750 INN loss: -1.126750 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114993 INN loss: -1.114993 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136589 INN loss: -1.136589 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.094421 INN loss: -1.094421 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131054 INN loss: -1.131054 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145634 INN loss: -1.145634 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163131 INN loss: -1.163131 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113180 INN loss: -1.113180 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.059621 INN loss: -1.059621 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136079 INN loss: -1.136079 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.143798 INN loss: -1.143798 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.178197 INN loss: -1.178197 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.058553 INN loss: -1.058553 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160609 INN loss: -1.160609 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.128633 INN loss: -1.128633 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.110159 INN loss: -1.110159 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.169302 INN loss: -1.169302 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116740 INN loss: -1.116740 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139071 INN loss: -1.139071 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.165007 INN loss: -1.165007 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.152983 INN loss: -1.152983 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174592 INN loss: -1.174592 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113544 INN loss: -1.113544 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.113853 INN loss: -1.113853 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.140116 INN loss: -1.140116 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.145858 INN loss: -1.145858 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.118408 INN loss: -1.118408 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.120757 INN loss: -1.120757 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.179074 INN loss: -1.179074 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146097 INN loss: -1.146097 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.146758 INN loss: -1.146758 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142269 INN loss: -1.142269 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.142571 INN loss: -1.142571 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.174510 INN loss: -1.174510 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.112688 INN loss: -1.112688 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.065765 INN loss: -1.065765 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114375 INN loss: -1.114375 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.161204 INN loss: -1.161204 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.175246 INN loss: -1.175246 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.163059 INN loss: -1.163059 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.125814 INN loss: -1.125814 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114620 INN loss: -1.114620 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.141850 INN loss: -1.141850 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107596 INN loss: -1.107596 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.147934 INN loss: -1.147934 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.109699 INN loss: -1.109699 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.131588 INN loss: -1.131588 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.135264 INN loss: -1.135264 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.107546 INN loss: -1.107546 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.068277 INN loss: -1.068277 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.160074 INN loss: -1.160074 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.116636 INN loss: -1.116636 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.114137 INN loss: -1.114137 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.117297 INN loss: -1.117297 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098775 INN loss: -1.098775 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.187148 INN loss: -1.187148 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.154407 INN loss: -1.154407 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.149641 INN loss: -1.149641 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130995 INN loss: -1.130995 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.157382 INN loss: -1.157382 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.078979 INN loss: -1.078979 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.136268 INN loss: -1.136268 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130752 INN loss: -1.130752 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139041 INN loss: -1.139041 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.139267 INN loss: -1.139267 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.098753 INN loss: -1.098753 KL loss: 0.000000 learning_rate: 0.000000\n",
      "Total loss: -1.130019 INN loss: -1.130019 KL loss: 0.000000 learning_rate: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 50/50 [4:46:55<00:00, 344.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mp_thetap = np.load('D_Kspipi_SDP_1e7.npy')\n",
    "\n",
    "# Split data into train, validation\n",
    "mp_thetap_train, mp_thetap_val = train_test_split(mp_thetap, test_size=0.2)\n",
    "\n",
    "print(\"Training set size: \", mp_thetap_train.shape, \"Validation set size: \", mp_thetap_val.shape)\n",
    "\n",
    "# Convert into torch objects\n",
    "mp_thetap_train = torch.Tensor(mp_thetap_train)\n",
    "mp_thetap_val   = torch.Tensor(mp_thetap_val)\n",
    "\n",
    "trainset = dalitz_dataset(mp_thetap_train)\n",
    "valset   = dalitz_dataset(mp_thetap_val)\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size = batch_size, shuffle = True)\n",
    "val_dataloader = DataLoader(valset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_dim = mp_thetap.shape[1]\n",
    "input_dim = 1           # One value of mp or thetap\n",
    "hidden_dim = 50         # Subnetwork hidden layers\n",
    "output_dim = 1          # One value out for generated mp or thetap\n",
    "training_size = len(mp_thetap)\n",
    "resample = False\n",
    "\n",
    "# Define the model\n",
    "BINN = Bayes_INN(n_dim = n_dim, training_size = training_size, input_dim = input_dim, hidden_dim = hidden_dim, output_dim = output_dim, resample = resample)\n",
    "model = BINN.inn\n",
    "\n",
    "print(\"Model Architecture: \")\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode= 'min', factor = 0.9, patience = 50)\n",
    "#scheduler = None\n",
    "\n",
    "for t in tqdm(range(epochs), ncols = 100):\n",
    "    train(train_dataloader, model, optimizer, scheduler, n_dim)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Latent distribution\n",
    "n_samples = 100000\n",
    "z_gen = torch.randn((n_samples, n_dim))\n",
    "\n",
    "print(z_gen.shape)\n",
    "\n",
    "# Visualize the latent distribution\n",
    "fig, axs = plt.subplots(figsize=(7,5))\n",
    "\n",
    "axs.hist(z_gen.detach().numpy().flatten(), alpha=0.5, density=True, bins=100, label=r'$z$')\n",
    "\n",
    "axs.set_xlabel('$z$ $(\\mathrm{GeV})$')\n",
    "axs.set_ylabel( r'$\\mathrm{Events}$' )\n",
    "axs.legend(loc='best', frameon=False)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n",
      "[0.15261209 0.7676098 ]\n"
     ]
    }
   ],
   "source": [
    "# Generate m2 samples from latent distribution\n",
    "mp_thetap_gen, _ = model(z_gen, rev=True)\n",
    "mp_thetap_gen = mp_thetap_gen.detach().numpy()[:]\n",
    "\n",
    "print(mp_thetap_gen.shape)\n",
    "print(mp_thetap_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize \n",
    "fig, ax = plt.subplots( figsize=(8,6) )\n",
    "\n",
    "counts, bins, _ = ax.hist(mp_thetap[:n_samples,0], 50, alpha = 0.5, label = r'$\\mathrm{Truth}$')\n",
    "ax.hist(mp_thetap_gen[:,0], bins, alpha=0.5, label = r'$\\mathrm{Generated}$')\n",
    "\n",
    "ax.set_xlabel( r\"$m'$\")\n",
    "ax.set_ylabel( r'$\\mathrm{Events}$' )\n",
    "\n",
    "\n",
    "ax.legend(loc='best', frameon=False)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize \n",
    "fig, ax = plt.subplots( figsize=(8,6) )\n",
    "\n",
    "counts, bins, _ = ax.hist(mp_thetap[:n_samples,1], 50, alpha = 0.5, label = r'$\\mathrm{Truth}$')\n",
    "ax.hist(mp_thetap_gen[:,1], bins, alpha=0.5, label = r'$\\mathrm{Generated}$')\n",
    "\n",
    "ax.set_xlabel( r\"$m'$\")\n",
    "ax.set_ylabel( r'$\\mathrm{Events}$' )\n",
    "\n",
    "\n",
    "ax.legend(loc='best', frameon=False)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
